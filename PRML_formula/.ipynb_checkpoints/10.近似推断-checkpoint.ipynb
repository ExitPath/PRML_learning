{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# preparation\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "from prml.rv import VariationalGaussianMixture\n",
    "from prml.preprocess import PolynomialFeature\n",
    "from prml.linear import (\n",
    "    VariationalLinearRegression,\n",
    "    VariationalLogisticRegression\n",
    ")\n",
    "\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在概率模型的应用中，一个中心任务是在给定观测（可见）数据变量$ X $的条件下，计算潜在变量Z的后验概率分布$ p(Z|X) $，以及计算关于这个概率分布的期望。模型可能也包含某些确定性参数，我们现在不考虑它。模型也可能是一个纯粹的贝叶斯模型，其中任何未知的参数都有一个先验概率分布，并且被整合到了潜在变量集合中，记作向量$ Z $。例如，在EM算法中，我们需要计算完整数据对数似然函数关于潜在变量后验概率分布的期望。对于实际应用中的许多模型来说，计算后验概率分布或者计算关于这个后验概率分布的期望是不可行的。这可能是由于潜在空间的维度太高，以至于无法直接计算，或者由于后验概率分布的形式特别复杂，从而期望无法解析地计算。在连续变量的情形中，需要求解的积分可能没有解析解，而空间的维度和被积函数的复杂度可能使得数值积分变得不可行。对于离散变量，求边缘概率的过程涉及到对隐含变量的所有可能的配置进行求和。这个过程虽然原则上总是可以计算的，但是我们在实际应用中经常发现，隐含状态的数量可能有指数多个，从而精确的计算所需的代价过高。\n",
    "在这种情况下，我们需要借助近似方法。根据近似方法依赖于随机近似还是确定近似，方法大体分为两大类。随机方法，例如第11章介绍的马尔科夫链蒙特卡罗方法，使得贝叶斯方法能够在许多领域中广泛使用。这些方法通常具有这样的性质：给定无限多的计算资源，它们可以生成精确的结果，近似的来源是使用了有限的处理时间。在实际应用中，取样方法需要的计算量会相当大，经常将这些方法的应用限制在了小规模的问题中。并且，判断一种取样方法是否生成了服从所需的概率分布的独立样本是很困难的。\n",
    "本章中，我们介绍了一系列的确定性近似方法，有些方法对于大规模的数据很适用。这些方法基于对后验概率分布的解析近似，例如通过假设后验概率分布可以通过一种特定的方式分解，或者假设后验概率分布有一个具体的参数形式，例如高斯分布。对于这种情况，这些方法永远无法生成精确的解，因此这些方法的优点和缺点与取样方法是互补的。\n",
    "在4.4节中，我们讨论了拉普拉斯近似，它基于对概率分布的峰值（即最大值）的局部高斯近似。这里，我们考虑一类近似方法，被称为变分推断（variational inference）或者变分贝叶斯（variational Bayes），它使用了更加全局的准则，并且被广泛应用于实际问题中。我们最后简要介绍另一种变分的框架，被称为期望传播（expectation propagation）。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 10.1 变分推断"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "变分的方法起源于18世纪的欧拉、拉格朗日，以及其他的关于变分法（calculus of variations）的研究。标准的微积分关注的是寻找函数的导数。我们可以将函数想象为一个映射。这个映射以一个变量的值作为输入，返回函数值作为输出。函数的导数描述了当输入变量有一个无限小的变化时，输出值如何变化。类似地，我们可以将泛函（functional）作为一个映射，它以一个函数作为输入，返回泛函的值作为输出。一个例子是熵$ H[p] $，它的输入是一个概率分布$ p(x) $，返回下面的量\n",
    "$ H[p] = - \\int p(x)\\ln p(x) dx \\tag{10.1} $\n",
    "作为输出。我们可以引入泛函的导数（functional derivative）的概念，它表达了输入函数产生无穷小的改变时，泛函的值的变化情况（Feynman et al., 1964）。变分法的规则与标准的微积分规则很相似，在附录D中讨论。许多问题可以表示为最优化问题，其中需要最优化的量是一个泛函。研究所有可能的输入函数，找到最大化或者最小化泛函的函数就是问题的解。变分方法有很广泛的适用性，包括有限元方法（Kapur, 1989）和最大熵方法（Schwarz, 1988）。\n",
    "虽然变分方法本质上没有任何近似的东西，但是它们通常会被用于寻找近似解。寻找近似解的过程可以这样完成：限制需要最优化算法搜索的函数的范围，例如只考虑二次函数，或考虑由固定的基函数线性组合而成的函数，其中只有线性组合的系数可以发生变化。在概率推断的应用中，限制条件的形式可以是可分解的假设（Jordan et al., 1999; Jaakkola, 2001）。\n",
    "现在，让我们详细讨论变分最优化的概念如何应用于推断问题。假设我们有一个纯粹的贝叶斯模型，其中每个参数都有一个先验概率分布。这个模型也可以有潜在变量以及参数，我们会把所有潜在变量和参数组成的集合记作$ Z $。类似的，我们会把所有观测变量的集合记作$ X $。例如，我们可能有$ N $个独立同分布的数据，其中$ X = {x_1,...,x_N} $且$ Z = {z_1,...,z_N} $。我们的概率模型确定了联合概率分布$ p(X, Z) $，我们的目标是找到对后验概率分布$ p(Z|X) $以及模型证据$ p(X) $的近似。与我们关于EM的讨论相同，我们可以将对数边缘概率分解，即\n",
    "$ \\ln p(x) = L(q) + KL(q \\Vert p) \\tag{10.2} $\n",
    "其中我们定义了\n",
    "$ \\begin{eqnarray} L(q) &=& \\int q(Z)\\ln\\frac{p(X,Z)}{q(Z)}dZ \\tag{10.3} \\ KL(q \\Vert p) &=& - \\int q(Z)\\ln\\frac{p(Z|X)}{q(Z)}dZ \\tag{10.4} \\end{eqnarray} $\n",
    "这与我们关于EM的讨论的唯一的区别是，因为参数现在是随机变量，被整合到了Z中，所以参数向量$ \\theta $不再出现。由于本章中我们主要感兴趣的是连续变量，因此我们在这个分解的公式中使用了积分而不是求和。但是，如果某些变量或全部的变量都是离散变量，那么分析过程不变，只需根据需要把积分替换为求和即可。同样的，我们可以通过关于概率分布$ q(Z) $的最优化来使下界$ L(q) $达到最大值，这等价于最小化KL散度。如果我们允许任意选择$ q(Z) $，那么下界的最大值出现在KL散度等于零的时刻，此时$ q(Z) $等于后验概率分布$ p(Z|X) $。然而，我们假定在需要处理的模型中，对真实的概率分布进行操作是不可行的。\n",
    "于是，我们转而考虑概率分布$ q(Z) $的一个受限制的类别，然后寻找这个类别中使得KL散度达到最小值的概率分布。我们的目标是充分限制$ q(Z) $可以取得的概率分布的类别范围，使得这个范围中的所有概率分布都是可以处理的概率分布。同时，我们还要使得这个范围充分大、充分灵活，从而它能够提供对真实后验概率分布的一个足够好的近似。需要强调的是，施加限制条件的唯一目的是为了计算方便，并且在这个限制条件下，我们应该使用尽可能丰富的近似概率分布。特别的，对于高度灵活的概率分布来说，没有“过拟合”现象。使用灵活的近似仅仅使得我们更好地近似真实的后验概率分布。\n",
    "限制近似概率分布的范围的一种方法是使用参数概率分布$ q(Z| \\omega) $，它由参数集合$ \\omega $控制。 这样，下界$ L(q) $变成了$ \\omega $的函数，我们可以利用标准的非线性最优化方法确定参数的最优值。图10.1给出了这种方法的一个例子，其中变分分布是一个高斯分布，并且我们已经关于均值和协方差进行了最优化。\n",
    " \n",
    "图 10.1 对于之前在图4.14中考虑过的例子进行变分近似的结果。左图给出了原始的概率分布（黄色）以及拉普拉斯近似（红色）和变分近似（绿色），右图给出了对应曲线的负对数。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 10.1.1 分解概率分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "这里，我们考虑另一种方法，这种方法里，我们限制概率分布$ q(Z) $的范围。假设我们将$ Z $的元素划分成若干个互不相交的组，记作$ Z_i $，其中$ i = 1,...,M $。然后，我们假定$ q $分布关于这些分组可以进行分解，即\n",
    "$ q(Z) = \\prod\\limits_{i=1}^Mq_i(Z_i) \\tag{10.5} $\n",
    "需要强调的是，我们关于概率分布没有做更多的假设。特别地，我们没有限制各个因子$ q_i(Z_i) $的函数形式。变分推断的这个分解的形式对应于物理学中的一个近似框架，叫做平均场理论（mean field theory）（Parisi, 1988）。\n",
    "在所有具有式（10.5）的形式的概率分布$ q(Z) $中，我们现在寻找下界$ L(q) $最大的概率分布。于是，我们希望对$ L(q) $关于所有的概率分布$ q_i(Z_i) $进行一个自由形式的（变分）最优化。 我们通过关于每个因子进行最优化来完成整体的最优化过程。为了完成这一点，我们首先将式（10.5）代入式（10.3），然后分离出依赖于一个因子$ q_j(Z_j) $的项。为了记号的简洁，我们简单的将$ q_j(Z_j) $记作$ q_j $，这样我们就有\n",
    "$ \\begin{eqnarray} L(q) &=& \\int\\prod\\limits_iq_i\\left{\\ln p(X,Z) - \\sum\\limits_i\\ln q_i\\right}dZ \\ &=& \\int q_j\\left{\\int \\ln p(X,Z)\\prod\\limits_{i \\neq j}q_i dZ_i\\right}dZ_j - \\int q_j\\ln q_jdZ_j + const \\ &=& \\int q_j\\ln\\tilde{p}(X,Z_j)dZ_j - \\int q_j\\ln q_j dZ_j + const \\tag{10.6} \\end{eqnarray} $\n",
    "其中，我们定义了一个新的概率分布$ \\tilde{p}(X,Z_j) $，形式为\n",
    "$ \\ln\\tilde{p}(X,Z_j) = \\mathbb{E}_{i \\neq j}[\\ln p(X,Z)] + const \\tag{10.7} $\n",
    "这里，记号$ E_{i \\neq j}[\\dots] $表示关于定义在所有$ z_i(i \\neq j) $上的$ q $概率分布的期望，即\n",
    "$ \\mathbb{E}{j \\neq j}[\\ln p(X,Z)] = \\int \\ln p(X,Z) \\prod\\limits{i \\neq j}q_idZ_i \\tag{10.8} $\n",
    "现在假设我们保持$ {q_{i \\neq j}} $固定，关于概率分布$ q_j(Z_j) $的所有可能的形式最大化式（10.6）中的$ L(q) $。这很容易做，因为我们看到式（10.6）是$ q_j(Z_j) $和$ \\tilde{p}(X,Z_j) $之间的Kullback-Leibler散度的负值。因此，最大化式（10.6）等价于最小化Kullback-Leibler散度，且最小值出现在$ q_j(Z_j) = \\tilde{p}(X,Z_j) $的位置。于是，我们得到了最优解$ q_j^*(Z_j) $的一般的表达式，形式为\n",
    "$ \\ln q_j^*(Z_j) = \\mathbb{E}_{i \\neq j}[\\ln p(X, Z)] + const \\tag{10.9} $\n",
    "很值得花一些时间研究一下解的形式，因为它是变分方法应用的基础。这个解表明，为了得到因子$ q_j $的最优解的对数，我们只需考虑所有隐含变量和可见变量上的联合概率分布的对数，然后关于所有其他的因子$ {q_i} $取期望即可，其中$ i \\neq j $。\n",
    "式（10.9）中的可加性常数通过对概率分布$ q_j^*(Z_j) $进行归一化的方式来设定。因此，如果我们取两边的指数，然后标准化，得到：\n",
    "$ q_j^*(Z_j) = \\frac{exp(\\mathbb{E}{i \\neq j}[\\ln p(X,Z)])}{\\int exp(\\mathbb{E}{i \\neq j}[\\ln p(X,Z)])dZ_j} $\n",
    "在实际应用中，我们会发现，更方便的做法是对式（10.9）进行操作，然后在必要的时候，通过观察的方式恢复出标准化系数。这一点通过下面的例子就会变得逐渐清晰起来。\n",
    "由式（10.9）给定的方程的集合（其中$ j = 1,...,M $）表示在概率能够进行分解这一限制条件下，下界的最大值满足的一组相容的条件。然而，这些方程并没有给出一个显式的解，因为最优化$ q_j^*(Z_j) $的式（10.9）的右手边表达式依赖于关于其它的因子$ q_i(Z_j)(i \\neq j) $计算的期望。 于是，我们会用下面的方式寻找出一个相容的解：首先，恰当地初始化所有的因子$ q_i(Z_i) $然后在各个因子上进行循环，每一轮用一个修正后的估计来替换当前因子。这个修正后的估计由式（10.9）的右侧给出，计算时使用了当前对于所有其他因子的估计。因为下界关于每个因子$ q_i(Z_i) $是一个凸函数（Boyd and Vandenberghe, 2004），所以算法保证收敛。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 10.1.2 分解近似的性质"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "我们的变分推断的方法基于的是真实后验概率分布的分解近似。让我们现在考虑一下使用分解概率分布的方式近似一个一般的概率分布的问题。首先，我们讨论使用分解的高斯分布近似一个高斯分布的问题，这会让我们认识到在使用分解近似时会引入的不准确性有哪些类型。考虑两个相关的变量$ z = (z_1, z_2) $上的高斯分布$ p(z) = \\mathcal{N}(z|\\mu, \\Lambda^{−1}) $，其中均值和精度的元素为\n",
    "$ \\mu = \\left( \\begin{array}{c} \\mu_1 \\ \\mu_2 \\end{array} \\right) , \\Lambda = \\left( \\begin{array}{cc} \\Lambda_{11} & \\Lambda_{12} \\ \\Lambda_{21} & \\Lambda_{22} \\end{array} \\right) \\tag{10.10} $\n",
    "并且由于精度矩阵的对称性，$ \\Lambda_{21} = \\Lambda_{12} $。现在，假设我们希望使用一个分解的高斯分布$ q(z) = q_1(z_1)q_2(z_2) $来近似这个分布。首先，我们使用一般的结果（10.9）来寻找最优因子$ q_1^*(z_1) $的表达式。在寻找表达式的过程中，我们注意到，在等式右侧，我们只需要保留哪些与$ z_1 $有函数依赖关系的项即可，因为所有其他的项都可以被整合到标准化常数中。因此我们有\n",
    "$ \\begin{eqnarray} \\ln q^*1(z_1) &=& \\mathbb{E}{z_2}[\\ln p(z)] + const \\ &=& \\mathbb{E}{z_2}\\left[-\\frac{1}{2}(z_1 - \\mu_1)^2\\Lambda{11} - (z_1 - \\mu_1)\\Lambda_{12}(z_2 - \\mu_2)\\right] + const \\ &=& -\\frac{1}{2}z_1^2\\Lambda_{11} + z_1\\mu_1\\Lambda_{11} - z_1\\Lambda_{12}(\\mathbb{E}[z_2] - \\mu_2) + const \\tag{10.11} \\end{eqnarray} $\n",
    "接下来，我们观察到这个表达式的右侧是$ z_1 $的一个二次函数，因此我们可以将$ q^*(z_1) $看成一个高斯分布。值得强调的是，我们不假设$ q(z_i) $是高斯分布，而是通过对所有可能的分布$ q(z_i) $上 的KL散度的变分最优化推导出了这个结果。还要注意，我们不需要显式地考虑式（10.9）中的可加性常数，因为它表示标准化常数。如果需要的话，这个常数可以在计算的最后阶段通过观察的方式得到。使用配平方的方法，我们可以得到这个高斯分布的均值和方差，有\n",
    "$ q_1^*(z_1) = \\mathcal{N}(z_1|m_1,\\Lambda_{11}^-1) \\tag{10.12} $\n",
    "其中\n",
    "$ m_1 = \\mu_1 - \\Lambda_{11}^{-1}\\Lambda_{12}(\\mathbb{E}[z_2] - \\mu_2) \\tag{10.13} $\n",
    "根据对称性，$ q_2^*(z_2) $也是一个高斯分布，可以写成\n",
    "$ q_2^*(z_2) = \\mathcal{N}(z_2|m_2,\\Lambda_{22}^{-1} \\tag{10.14} $\n",
    "其中\n",
    "$ m_2 = \\mu_2 - \\Lambda_{22}^{-1}\\Lambda_{21}(\\mathbb{E}[z_1] - \\mu_1) \\tag{10.15} $\n",
    "注意，这些解是相互偶合的，即$ q^(z_1) $依赖于关于$ q^(z_2) $计算的期望，反之亦然。通常，我们这样解决这个问题：将变分解看成重估计方程，然后在变量之间循环，更新这些解，直到满足某个收敛准则。我们稍后会给出一个例子。但是这里，我们注意到这个问题是相当简单的，因为可以找到一个解析解。特别地，由于$ \\mathbb{E}[z_1] = m_1 $且$ \\mathbb{E}[z_2] = m_2 $，因此我们看到，如果我们取$ \\mathbb{E}[z_1] = \\mu_1 $且$ \\mathbb{E}[z_2] = \\mu_2 $，那么这两个方程会得到满足。并且很容易证明，只要概率分布非奇异，那么这个解是唯一解。这个结果如图10.2(a)所示。\n",
    " \n",
    "图 10.2 两种形式的KL散度的对比。绿色轮廓线对应于两个变量$ z_1 $和$ z_2 $上的相关高斯分布$ p(z) $的1、2、3个标准差的位置，红色轮廓线表示相同变量上的近似分布$ q(z) $的同样位置。近似分布$ q(z) $由两个独立的一元高斯分布的乘积给出，(a)图中，参数通过最小化Kullback-Leibler散度$ KL(q \\Vert p) $的方式获得，(b)图中，参数通过最小化相反的Kullback-Leibler散度$ KL(p \\Vert q) $的方式获得。\n",
    "我们看到，均值被正确地描述了，但是$ q(z) $的方差由$ p(z) $的最小方差的方向所确定，沿着垂直方向的方差被强烈地低估了。这是一个一般的结果，即分解变分近似对后验概率分布的近似倾向于过于紧凑。\n",
    "作为比较，假设我们最小化相反的Kullback-Leibler散度$ KL(p \\Vert q) $。正如我们将看到的那样，这种形式的KL散度被用于另一种近似推断的框架中，这种框架被称为期望传播（expectation propagation）。于是，我们考虑一般的最小化$ KL(p \\Vert q) $的问题，其中$ q(Z) $是形式为（10.5）的分解近似。这样，KL散度可以写成\n",
    "$ KL(p \\Vert q) = -\\int p(Z)\\left[\\sum\\limits_{i=1}^M\\ln q_i(Z_i)\\right]dZ + const \\tag{10.16} $\n",
    "其中，常数项就是$ p(Z) $的熵，因此不依赖于$ q(Z) $。我们现在可以关于每个因子$ q_j(Z_j) $进行最优化。使用拉格朗日乘数法，很容易得到结果\n",
    "$ q_j^* = \\int p(Z)\\prod\\limits_{i \\neq j}dZ_i = p(Z_j) \\tag{10.17} $\n",
    "在这种情况下，我们看到$ q_j(Z_j) $的最优解等于对应的边缘概率分布$ p(Z) $。注意，这是一个解析解，不需要迭代。\n",
    "为了将这个结果应用到向量$ z $上的高斯分布$ p(z) $这个例子上，我们可以使用式（2.98），它给出了图10.2(b)的结果。我们再一次看到，对均值的近似是正确的，但是它把相当多的概率质量放到了实际上具有很低的概率的变量空间区域中。\n",
    "这两个结果的区别可以用下面的方式理解。我们注意到，$ Z $空间中$ p(Z) $接近等于零的区域对于Kullback-Leibler散度\n",
    "$ KL(q \\Vert p) = - \\int q(Z)\\ln\\left{\\frac{p{Z}}{q{Z}}\\right}dZ \\tag{10.18} $\n",
    "有一个大的正数的贡献，除非$ q(Z) $也接近等于零。因此最小化这种形式的KL散度会使得概率分布$ q(Z) $避开$ p(Z) $很小的区域。相反的，使得Kullback-Leibler散度$ KL(p \\Vert q) $的散度取得最小值的概率分布$ q(Z) $在$ p(Z) $非零的区域中也是非零的。\n",
    "如果我们考虑用一个单峰分布近似多峰分布的问题，我们会更深刻地认识两个KL散度的不同行为，如图10.3所示。\n",
    " \n",
    "图 10.3 两种形式的Kullback-Leibler散度的另一个对比。(a)蓝色轮廓线展示了由两个高斯分布混合而成的双峰概率分布$ p(Z) $，红色轮廓线对应于一个高斯分布$ q(Z) $，它最小化了Kullback-Leibler散度$ KL(p \\Vert q) $， 在这种意义上最好地近似了$ p(Z) $。(b)与(a)相同，但是此时红色轮廓线对应的高斯分布$ q(Z) $是通过使用数值方法最小化Kullback-Leibler散度$ KL(q \\Vert p) $的方式得到的。(c)与(b)相同，但是给出了Kullback-Leibler散度的另一个局部最小值。\n",
    "在实际应用中，真实的后验概率分布经常是多峰的，大部分后验概率质量集中在参数空间中的某几个相对较小的区域中。这些多个峰值可能是由于潜在空间的不可区分性所造成的，也可能是由于对参数的复杂的非线性依赖关系造成的。我们在第9章中讨论高斯混合模型的时候遇到过这两种类型的多峰性质，那里，这些峰值以似然函数的多个极大值的形式显现出来。基于最小化$ KL(q \\Vert p) $的变分方法倾向于找到这些峰值中的一个。相反，如果我们最小化$ KL(p \\Vert q) $，那么得到的近似会在所有的均值上取平均。在混合模型问题中，这种方法会给出较差的预测分布（因为两个较好的参数值的平均值通常不是一个较好的参数值）。可以使用$ KL(p \\Vert q) $定义一个有用的推断步骤，但是这需要一种与这里讨论的内容相当不同的方法。当我们讨论期望传播的时候，我们会仔细讨论这一点。\n",
    "两种形式的Kullback-Leibler散度都是散度的alpha家族（alpha family）的成员（Ali and Silvey, 1966; Amari, 1985; Minka, 20005）定义为\n",
    "$ D_{\\alpha}(p \\Vert q) = \\frac{4}{1 - \\alpha^2}\\left( 1 - \\int p(x)^{(1 + \\alpha) / 2}q(x)^{(1 + \\alpha) / 2}dx\\right) \\tag{10.19} $\n",
    "其中$ −\\infty < \\alpha < \\infty $是一个连续参数。Kullback-Leibler散度$ KL(p \\Vert q) $对应于极限$ \\alpha \\to 1 $，而$ KL(q \\Vert p) $对应于极限$ \\alpha \\to −1 $。对于所有$ \\alpha $的值，我们有$ D_\\alpha(p \\Vert q) \\geq 0 $，当且仅当$ p(x) = q(x) $时等号成立。假设$ p(x) $是一个固定的分布，我们关于某个概率分布$ q(x) $的集合最小化$ D_\\alpha(p \\Vert q) $。那么对于$ \\alpha \\leq −1 $的情况，散度是零强制的（zero forcing），即对于使得$ p(x) = 0 $成立的任意$ x $值，都有$ q(x) = 0 $，通常$ q(x) $会低估$ p(x) $的支持，因此倾向于寻找具有最大质量的峰值。相反，对于$ \\alpha \\geq 1 $的情况，散度是零避免的（zero avoiding），即对于使得$ p(x) > 0 $成立的任意$ x $值，都有$ q(x) > 0 $，通常$ q(x) $会进行拉伸来覆盖到所有的$ p(x) $值，从而高估了$ p(x) $的支持。当$ \\alpha = 0 $时，我们得到了一个对称的散度，它与Hellinger距离线性相关，定义为\n",
    "$ D_H(p \\Vert q) = \\int \\left(p(x)^{1/2} - q(x)^{1/2}\\right)^2dx \\tag{10.20} $\n",
    "Hellinger距离的平方根是一个合法的距离度量。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 10.1.3 例子：一元高斯分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "我们现在使用一元变量$ x $上的高斯分布来说明分解变分近似（MacKay, 2003）。我们的目标是在给定$ x $的观测值的数据集$ D = {x_1,...,x_N} $的情况下，推断均值$ \\mu $和精度$ \\tau $的后验概率分布。其中，我们假设数据是独立地从高斯分布中抽取的。似然函数为\n",
    "$ p(D|\\mu,\\tau) = \\left(\\frac{\\tau}{2\\pi}\\right)^{N/2}exp\\left{-\\frac{\\tau}{2}\\sum\\limits_{n=1}^N(x_n - \\mu)^2\\right} \\tag{10.21} $\n",
    "我们现在引入$ \\mu, \\tau $的共轭先验分布，形式为\n",
    "$ \\begin{eqnarray} p(\\mu|\\tau) &=& \\mathcal{N}(\\mu|\\mu_0,(\\lambda_0\\tau)^{-1}) \\tag{10.22} \\ p(\\tau) &=& Gam(tau|a_0,b_0) \\tag{10.23} \\end{eqnarray} $\n",
    "其中$ Gam(\\tau | a_0,b_0) $是式（2.146）定义的Gamma分布。这些分布共同给出了一个高斯-Gamma共轭先验分布。\n",
    "对于这个简单的问题，后验概率可以求出精确解，并且形式还是高斯-Gamma分布。然而，为了讲解的目的，我们会考虑对后验概率分布的一个分解变分近似，形式为\n",
    "$ q(\\mu,\\tau) = q_\\mu(\\mu)q_\\tau(\\tau) \\tag{10.24} $\n",
    "注意，真实的后验概率分布不可以按照这种形式进行分解。最优的因子$ q_\\mu(\\mu), q_\\tau(\\tau) $可以从一般的结果（10.9）中得到，如下所述。对于$ q_\\mu(\\mu) $，我们有\n",
    "$ \\begin{eqnarray} \\ln q_\\mu^* &=& \\mathbb{E}\\tau[\\ln p(D|\\mu,\\tau) + \\ln p(\\mu|\\tau)] + const \\ &=& -\\frac{\\mathbb{E}[\\tau]}{2}\\left{\\lambda_0(\\mu - \\mu_0)^2 + \\sum\\limits{n=1}^N(x_n - \\mu)^2\\right} + const \\tag{10.25} \\end{eqnarray} $\n",
    "对于$ \\mu $配平方，我们看到$ q_\\mu(\\mu) $是一个高斯分布$ \\mathcal{N}(\\mu|\\mu_N , \\lambda_N^{−1}) $，其中，均值和方差为\n",
    "$ \\begin{eqnarray} \\mu_N &=& \\frac{\\lambda_0\\mu_0 + N\\bar{x}}{\\lambda_0 + N} \\tag{10.26} \\ \\lambda_N &=& (\\lambda_0 + N)\\mathbb{E}[\\tau] \\tag{10.27} \\end{eqnarray} $\n",
    "注意，对于$ N \\to \\infty $，这给出了最大似然的结果，其中$ \\mu_N = \\bar{x} $，精度为无穷大。\n",
    "类似地，因子$ q_\\tau(\\tau) $的最优解为\n",
    "$ \\begin{eqnarray} \\ln q_\\tau^*(\\tau) &=& \\mathbb{E}\\mu[\\ln p(D|\\mu,\\tau) + \\ln p(\\mu|\\tau)] + \\ln p(\\tau) + const \\ &=& (a_0 - 1)\\ln\\tau - b_0\\tau + \\frac{N + 1}{2}\\ln\\tau \\ & & -\\frac{\\tau}{2}\\mathbb{E}\\mu\\left[\\sum\\limits_{n=1}^N(x_n - \\mu)^2 + \\lambda_0(\\mu - \\mu_0)^2\\right] + const \\tag{10.28} \\end{eqnarray} $\n",
    "因此$ q_\\tau(\\tau) $是一个Gamma分布$ Gam(\\tau|a_N,b_N) $，参数为\n",
    "$ \\begin{eqnarray} a_N &=& a_0 + \\frac{N + 1}{2} \\tag{10.29} \\ b_N &=& b_0 + \\frac{1}{2}\\mathbb{E}\\mu\\left[\\sum\\limits{n=1}^N(x_n - \\mu)^2 + \\lambda_0(\\mu - \\mu_0)^2\\right] \\tag{10.30} \\end{eqnarray} $\n",
    "同样的，当$ N \\to \\infty $时，它的行为与预期相符。\n",
    "应该强调的是，我们不假设最优概率分布$ q_\\mu(\\mu), q_\\tau(\\tau) $的具体的函数形式。它们的函数形式从似然函数和对应的共轭先验分布中自然地得到。\n",
    "因此,我们得到了最优概率分布$ q_\\mu(\\mu), q_\\tau(\\tau) $的表达式，每个表达式依赖于关于其他概率分布计算得到的矩。因此，一种寻找解的方法是对例如$ \\mathbb{E}[\\tau] $进行一个初始的猜测，然后使用这个猜测来重新计算概率分布$ q_\\mu(\\mu) $。给定这个修正的概率分布之后，我们接下来可以计算所需的矩$ \\mathbb{E}[\\mu], \\mathbb{E}[\\mu_2] $，并且使用这些矩来重新计算概率分布$ q_\\tau(\\tau) $，以此类推。由于这个例子中，隐含变量空间是二维的，因此我们可以用图形来说明后验概率分布的变分近似过程。我们画出了真实后验概率的轮廓线和分解近似的等高线，如图10.4所示。\n",
    " \n",
    "图 10.4 一元高斯分布的均值$ \\mu $和精度$ \\tau $的变分推断的例子。真实后验概率分布$ p(\\mu,\\tau | D) $用绿色曲线表示。(a)初始的分解近似$ q_\\mu(\\mu)q_\\tau(\\tau) $，用蓝色曲线表示。(b)重新估计了因子$ q_\\mu(\\mu) $之后的结果。(c)重新估计了因子$ q_\\tau(\\tau) $之后的结果。(d)最优分解近似的轮廓线，其中迭代方法收敛，用红色表示。\n",
    "通常，我们需要使用一种迭代的方法来得到最优分解后验概率分布的解。然而，对于我们这里讨论的非常简单的例子来说，我们可以通过求解最优因子$ q_\\mu(\\mu), q_\\tau(\\tau) $的方程，得到一个显式的解。在做这件事之前，我们可以通过考虑无信息先验来简化表达式。无信息先验分布中，$ \\mu_0 = a_0 = b_0 = \\lambda_0 = 0 $。虽然这些参数设置对应于一个反常先验，但是我们看到后验概率分布仍然具有良好的定义。使用Gamma分布的均值的标准结果$ \\mathbb{E}[\\tau] = a_N $，以及式（10.29）和式（10.30），我们有\n",
    "$ \\frac{1}{\\mathbb{E}[\\tau]} = \\mathbb{E}\\left[\\frac{1}{N + 1}\\sum\\limits_{n=1}^N(x_n - \\mu)^2\\right] + \\frac{N}{N + 1}(\\bar{x^2} - 2\\bar{x}\\mathbb{E}[\\mu]+\\mathbb{E}[\\mu^2]) \\tag{10.31} $\n",
    "之后，使用式（10.26）和式（10.27），我们得到了$ q_\\mu(\\mu) $的一阶矩和二阶矩，形式为\n",
    "$ \\mathbb{E}[\\mu] = \\bar{x}, \\mathbb{E}[\\mu^2] = \\bar{x}^2 + \\frac{1}{N\\mathbb{E}[\\tau]} \\tag{10.32} $\n",
    "现在，我们可以将这些矩代入式（10.31），然后解出$ \\mathbb{E}[\\tau] $，可得\n",
    "$ \\frac{1}{\\mathbb{E}[\\tau]} = (\\bar{x^2} - \\bar{x}^2) = \\frac{1}{N}\\sum\\limits_{n=1}^N(x_n - \\bar{x})^2 \\tag{10.33} $\n",
    "对于高斯分布的贝叶斯推断的可理解的介绍，包括与最大似然方法的相比的优势的讨论，可以参考Minka(1998)。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 10.1.4 模型比较"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "除了在隐含变量$ Z $上进行推断之外，我们可能还希望对比一组候选模型。索引为$ m $的模型的先验概率分布为$ p(m) $。这样，我们的目标是近似后验概率分布$ p(m|X) $，其中$ X $是观测数据。因为不同的模型可能具有不同的结构，并且隐含变量$ Z $的维度实际上可能不同，所以这比我们目前为止考虑的情况稍微复杂一些。因此我们不能简单地考虑考虑分解近似$ q(Z)q(m) $，而是必须意识到$ Z $的后验概率分布必须以$ m $为条件，所以我们必须考虑$ q(Z, m) = q(Z|m)q(m) $。我们已经可以验证下面的基于变分概率分布的分解方式\n",
    "$ \\ln p(X) = L - \\sum\\limits_m\\sum\\limits_Zq(Z|m)q(m)\\ln\\left{\\frac{p(Z,m|X)}{q(Z|m)q(m)}\\right} \\tag{10.34} $\n",
    "其中$ L $是$ \\ln p(X) $的下界，形式为\n",
    "$ L = \\sum\\limits_m\\sum\\limits_Zq(Z|m)q(m)\\ln\\left{\\frac{p(Z,X,m)}{q(Z|m)q(m)}\\right} \\tag{10.35} $\n",
    "这里，我们假定$ Z $是离散变量，但是同样的分析也适用于连续潜在变量，只要我们把求和替换为积分即可。我们可以使用拉格朗日乘数法关于概率分布$ q(m) $最大化$ L $，结果为\n",
    "$ q(m) \\propto p(m)exp{L_m} \\tag{10.36} $\n",
    "其中\n",
    "$ L_m = \\sum\\limits_Zq(Z|m)\\ln\\left{\\frac{p(Z,X|m)}{q(Z|m)}\\right} $\n",
    "然而，如果我们关于$ q(Z|m) $最大化L，那么我们发现对于不同的$ m $值，解是相互偶合的，这与我们预期相符，因为这些概率分布是以$ m $为条件的。我们接下来首先通过最优化（10.35），或等价的，最优化$ L_m $，来独立地最优化每个$ q(Z|m) $，然后使用式（10.36）来确定$ q(m) $。在对求得的$ q(m) $值进行标准化之后，它的值可以用于模型选择或者模型平均。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 10.2 例子：高斯的变分混合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "我们现在回到我们对于高斯混合模型的讨论，并且使用前一节讨论的变分推断的方法。这会很好地说明变分方法的应用，也会展示出贝叶斯方法是如何优雅地解决最大似然方法中的许多困难之处的（Attias, 1999b）。我们建议读者仔细研究这个例子，因为这个例子给出了变分方法在实际应用中的许多重要的思想。许多贝叶斯模型，对应于复杂得多的概率分布，可以通过对本节中的分析进行简单的扩展进行求解。\n",
    "我们的起始点是高斯混合模型的似然函数。高斯混合模型如图9.6给出的图模型所示。对于每个观测$ x_n $，我们有一个对应的潜在变量$ z_n $，它是一个“1-of-K”的二值向量，元素为$ z_{nk} $，其中$ k = 1,...,K $。与之前一样，我们将观测数据集记作$ X = {x_1,...,x_N} $，类似地，我们将潜在变量记作$ Z = {z_1,...,z_N} $。给定混合系数$ \\pi $，根据式（9.10），我们可以写出$ Z $的条件概率分布，形式为\n",
    "$ p(Z|\\pi) = \\prod\\limits_{n=1}^N\\prod\\limits_{k=1}^K\\pi_k^{z_{nk}} \\tag{10.37} $\n",
    "类似的，给定潜在变量和分量参数，根据式（9.11），我们可以写出观测数据向量的条件概率分布，形式为\n",
    "$ p(X|Z,\\mu,\\Lambda) = \\prod\\limits_{n=1}^N\\prod\\limits_{k=1}^K\\mathcal{N}(x_n|\\mu_k,\\Lambda_k^{-1})^{z_{nk}} \\tag{10.38} $\n",
    "其中$ \\mu = {\\mu_k} $且$ \\Lambda = {\\Lambda_k} $。注意，我们计算时使用的时精度矩阵而不是协方差矩阵，因为这在一定程度上简化了数学计算的复杂度。\n",
    "接下来，我们引入参数$ \\mu, \\Lambda, \\pi $上的先验概率分布。如果我们使用共轭先验分布，那么分析过程会得到极大的简化。于是，我们选择混合系数$ \\pi $上的狄利克雷分布。\n",
    "$ p(\\pi) = Dir(\\pi|\\alpha_0) = C(\\alpha_0)\\prod\\limits_{k=1}^K\\pi_k^{\\alpha_0 - 1} \\tag{10.39} $\n",
    "其中，根据对称性，我们为每个分量选择了同样的参数$ \\alpha_0, C(\\alpha_0) $是狄利克雷分布的标准化常数，由式（B.23）定义。正如我们已经看到的那样，参数$ \\alpha_0 $可以看成与混合分布的每个分量关联的观测的有效先验数量。如果$ \\alpha_0 $的值很小，那么后验概率分布会主要被数据集影响，而受到先验概率的影响很小。\n",
    "类似的，我们引入一个独立的高斯-Wishart先验分布，控制每个高斯分布的均值和精度，形式为\n",
    "$ \\begin{eqnarray} p(\\mu,\\Lambda) &=& p(\\mu|\\Lambda)p(\\Lambda) \\ &=& \\prod\\limits_{k=1}^K\\mathcal{N}(\\mu_k|m_0, (\\beta_0\\Lambda_k)^{-1})W(\\Lambda_k|W_0,v_0) \\tag{10.40} \\end{eqnarray} $\n",
    "这是由于当均值和精度均未知的时候，它表示共轭先验分布。通常根据对称性，我们选择$ m_0 = 0 $。\n",
    "生成的模型可以表示为图10.5所示的有向图。\n",
    " \n",
    "图 10.5 表示高斯模型的贝叶斯混合的有向图，其中，方框表示一组$ N $个独立同分布的观测。这里$ \\mu $表示$ {\\mu_k} $，$ \\Lambda $表示$ {\\Lambda_k} $。\n",
    "注意，从$ \\Lambda $到$ \\mu $之间存在一个链接，这是由于式（10.40）中的$ \\mu $上的概率分布的方差为$ \\Lambda $的函数。\n",
    "这个例子很好地说明了潜在变量和参数之间的区别。像$ z_n $这样出现在方框内部的变量被看做隐含变量，因为这种变量的数量随着数据集规模的增大而增大。相反，像$ \\mu $这样出现在方框外的变量的数量与数据集的规模无关，因此被当做参数。然而，从图模型的观点来看，它们之间没有本质的区别。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# data\n",
    "x1 = np.random.normal(size=(100, 2))\n",
    "x1 += np.array([-5, -5])\n",
    "x2 = np.random.normal(size=(100, 2))\n",
    "x2 += np.array([5, -5])\n",
    "x3 = np.random.normal(size=(100, 2))\n",
    "x3 += np.array([0, 5])\n",
    "x_train = np.vstack((x1, x2, x3))\n",
    "\n",
    "x0, x1 = np.meshgrid(np.linspace(-10, 10, 100), np.linspace(-10, 10, 100))\n",
    "x = np.array([x0, x1]).reshape(2, -1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot\n",
    "vgmm = VariationalGaussianMixture(n_components=6)\n",
    "vgmm.fit(x_train)\n",
    "\n",
    "plt.scatter(x_train[:, 0], x_train[:, 1], c=vgmm.classify(x_train))\n",
    "plt.contour(x0, x1, vgmm.pdf(x).reshape(100, 100))\n",
    "plt.xlim(-10, 10, 100)\n",
    "plt.ylim(-10, 10, 100)\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Requested MovieWriter (ffmpeg) not available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/envs/jiang/lib/python3.7/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/jiang/lib/python3.7/site-packages/matplotlib/animation.py\u001b[0m in \u001b[0;36m_repr_html_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1389\u001b[0m         \u001b[0mfmt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'animation.html'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfmt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'html5'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1391\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_html5_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1392\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mfmt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'jshtml'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1393\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_jshtml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/jiang/lib/python3.7/site-packages/matplotlib/animation.py\u001b[0m in \u001b[0;36mto_html5_video\u001b[0;34m(self, embed_limit)\u001b[0m\n\u001b[1;32m   1326\u001b[0m                 \u001b[0;31m# We create a writer manually so that we can get the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m                 \u001b[0;31m# appropriate size for the tag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1328\u001b[0;31m                 \u001b[0mWriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwriters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'animation.writer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1329\u001b[0m                 writer = Writer(codec='h264',\n\u001b[1;32m   1330\u001b[0m                                 \u001b[0mbitrate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'animation.bitrate'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/jiang/lib/python3.7/site-packages/matplotlib/animation.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_registered\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Requested MovieWriter ({name}) not available\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Requested MovieWriter (ffmpeg) not available"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.animation.ArtistAnimation at 0x7ff805a85850>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# video\n",
    "vgmm = VariationalGaussianMixture(n_components=6)\n",
    "vgmm._init_params(x_train)\n",
    "params = np.hstack([param.flatten() for param in vgmm.get_params()])\n",
    "fig = plt.figure()\n",
    "colors = np.array([\"r\", \"orange\", \"y\", \"g\", \"b\", \"purple\"])\n",
    "frames = []\n",
    "for _ in range(100):\n",
    "    plt.xlim(-10, 10)\n",
    "    plt.ylim(-10, 10)\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    r = vgmm._variational_expectation(x_train)\n",
    "    imgs = [plt.scatter(x_train[:, 0], x_train[:, 1], c=colors[np.argmax(r, -1)])]\n",
    "    for i in range(vgmm.n_components):\n",
    "        if vgmm.component_size[i] > 1:\n",
    "            imgs.append(plt.scatter(vgmm.mu[i, 0], vgmm.mu[i, 1], 100, colors[i], \"X\", lw=2, edgecolors=\"white\"))\n",
    "    frames.append(imgs)\n",
    "    vgmm._variational_maximization(x_train, r)\n",
    "    new_params = np.hstack([param.flatten() for param in vgmm.get_params()])\n",
    "    if np.allclose(new_params, params):\n",
    "        break\n",
    "    else:\n",
    "        params = np.copy(new_params)\n",
    "plt.close()\n",
    "plt.rcParams['animation.html'] = 'html5'\n",
    "anim = animation.ArtistAnimation(fig, frames)\n",
    "anim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 10.2.1 变分分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "为了形式化地描述这个模型的变分方法，我们接下来写出所有随机变量的联合概率分布，形式为\n",
    "$ p(X,Z,\\pi,\\mu,\\Lambda) = p(X|Z,\\Lambda)p(Z|\\pi)p(\\pi)p(\\mu|\\Lambda)p(\\Lambda) \\tag{10.41} $\n",
    "其中，各种因子已经在之前定义过。读者现在应该验证一下这种分解方式确实对应于图10.5给出的概率图模型。注意，只有变量$ X = {x_1,...,x_N} $是观测变量。\n",
    "我们现在考虑一个变分分布，它可以在潜在变量与参数之间进行分解，即\n",
    "$ q(Z,\\pi,\\mu,\\Lambda) = q(Z)q(\\pi,\\mu,\\Lambda) \\tag{10.42} $\n",
    "需要注意的是，为了让我们的贝叶斯混合模型能够有一个合理的可以计算的解，这是我们需要做出的唯一的假设。特别的，因子$ q(Z) $和$ q(\\pi, \\mu, \\Lambda) $的函数形式会在变分分布的最优化过程中自动确定。注意，我们省略了$ q $分布的下标，就像我们在式（10.41）中做的那样。我们依赖参数来区分不同的分布。\n",
    "通过使用一般的结果（10.9），这些因子的对应的顺序更新方程可以很容易地推导出来。让我们考虑因子$ q(Z) $的更新方程的推导。最优因子的对数为\n",
    "$ \\ln q^*(Z) = \\mathbb{E}_{\\pi,\\mu,\\Lambda}[\\ln p(X,Z,\\pi,\\mu,\\Lambda)] + const \\tag{10.43} $\n",
    "我们现在使用式（10.41）给出的分解方式。注意，我们只对等式右侧与变量$ Z $相关的函数关系感兴趣。因此，任何与变量$ Z $无关的项都可以被整合到可加的标准化系数中，从而有\n",
    "$ \\ln q^*(Z) = \\mathbb{E}\\pi[\\ln p(Z|\\pi)] + \\mathbb{E}{\\mu,\\Lambda}[\\ln p(X|Z, \\mu, \\Lambda)] + const \\tag{10.44} $\n",
    "替换右手边的两个条件分布，然后再次把与$ Z $无关的项整合到可加性常数中，得到\n",
    "$ \\ln q^*(Z) = \\sum\\limits_{n=1}^N\\sum\\limits_{k=1}^Kz_{nk}\\ln\\rho_{nk} + const \\tag{10.45} $\n",
    "其中我们定义了\n",
    "$ \\begin{eqnarray} \\ln\\rho_{nk} = &\\mathbb{E}&[\\ln\\pi_k] + \\frac{1}{2}\\mathbb{E}[\\ln|\\Lambda_k|] - \\frac{D}{2}\\ln(2\\pi) \\ &-&\\frac{1}{2}\\mathbb{E}_{\\mu_k,\\Lambda_k}[(x_n - \\mu_k)^T\\Lambda_k(x_n - \\mu_k)] \\tag{10.46} \\end{eqnarray} $\n",
    "其中$ D $是数据变量$ x $的维度。式（10.45）两边取指数，得到\n",
    "$ q^*(Z) \\propto \\prod\\limits_{n=1}^N\\prod\\limits_{k=1}^K\\rho_{nk}^{z_{nk}} \\tag{10.47} $\n",
    "这个分布需要是标准化的，并且，我们注意到每个$ n $的值对应的$ z_{nk} $都是二值的，且所有的$ k $值上的和为$ 1 $。所以得到：\n",
    "$ q^*(Z) = \\prod\\limits_{n=1}^N\\prod\\limits_{k=1}^Kr_{nk}^{z_{nk}} \\tag{10.48} $\n",
    "其中\n",
    "$ r_{nk} = \\frac{\\rho_{nk}}{\\sum\\limits_{j=1}^K\\rho_{nj}} \\tag{10.49} $\n",
    "我们看到，因子$ q(Z) $的最优解的函数形式与先验概率分布$ p(Z|\\pi) $的函数形式相同。注意，由于$ \\rho_{nk} $是一个实数值的指数，因此$ r_{nk} $是非负的，且和等于1，满足要求。\n",
    "对于离散分布$ q^*(Z) $，我们有标准的结果\n",
    "$ \\mathbb{E}[z_nk] = r_{nk} \\tag{10.50} $\n",
    "从中我们看到$ r_{nk} $扮演着“责任”的角色。注意，$ q^*(Z) $的最优解依赖于关于其他变量计算得到的矩，因此同样的，变分更新方程是偶合的，必须用迭代的方式求解。\n",
    "现在，我们会发现定义观测数据关于“责任”的下面三个统计量会比较方便，即\n",
    "$ \\begin{eqnarray} N_k &=& \\sum\\limits_{n=1}^Nr_{nk} \\tag{10.51} \\ \\bar{x}k &=& \\frac{1}{N_k}\\sum\\limits{n=1}^Nr_{nk}x_n \\tag{10.52} \\ S_k &=& \\frac{1}{N_k}\\sum\\limits_{n=1}^Nr_{nk}(x_n - \\bar{x}_k)(x_n -\\bar{x}_k)^T \\tag{10.53} \\end{eqnarray} $\n",
    "注意，这些类似于高斯混合模型的最大似然EM算法中计算的量。\n",
    "现在让我们考虑变分后验概率分布中的因子$ q(\\pi, \\mu, \\Lambda) $。与之前一样，使用式（10.9）给出的一般的结果，得到\n",
    "$ \\begin{eqnarray} \\ln q^*(\\pi,\\mu,\\Lambda) = & & \\ln p(\\pi) + \\sum\\limits_{k=1}^K\\ln p(\\mu_k,\\Lambda_k) + \\mathbb{E}Z[\\ln p(X|\\pi)] \\ &+& \\sum\\limits{k=1}^K\\sum\\limits_{n=1}^N\\mathbb{E}[z_{nk}]\\ln\\mathcal{N}(x_n|\\mu_k,\\Lambda_k^{-1}) + const \\tag{10.54} \\end{eqnarray} $\n",
    "我们观察到，这个表达式的右侧分解成了若干项的和，一些项只与$ \\pi $相关，一些项只与$ \\mu, \\Lambda $相关，这表明变分后验概率$ q(\\pi,\\mu,\\Lambda) $可以分解为$ q(\\pi)q(\\mu,\\Lambda) $。此外，与$ \\mu, \\Lambda $相关的项本身由$ k $个与$ \\mu_k $和$ \\Lambda_k $相关的项有关，因此可以进一步分解，即\n",
    "$ q(\\pi,\\mu,\\Lambda) = q(\\pi)\\prod\\limits_{k=1}^Kq(\\mu_k,\\Lambda_k) \\tag{10.55} $\n",
    "分离出式（10.54）右侧的与$ \\pi $相关的项，我们有\n",
    "$ \\ln q^*(\\pi) = (\\alpha_0 - 1)\\sum\\limits_{k=1}^K\\ln\\pi_k + \\sum\\limits_{k=1}^K\\sum\\limits_{n=1}^Nr_{nk}\\ln\\pi_k + const \\tag{10.56} $\n",
    "其中我们使用了式（10.50）。两边取指数，我们将$ q^*(\\pi) $看成狄利克雷分布\n",
    "$ q^*(\\pi) = Dir(\\pi|\\alpha) \\tag{10.57} $\n",
    "其中$ \\alpha $的元素为$ \\alpha_k $，形式为\n",
    "$ \\alpha_k = \\alpha_0 + N_k \\tag{10.58} $\n",
    "最后，变分后验概率分布$ q^(\\mu_k, \\Lambda_k) $无法分解成边缘概率分布的乘积，但是我们总可以使用概率的乘积规则，将其写成$ q^(\\mu_k, \\Lambda_k) = q^(\\mu_k | \\Lambda_k)q^(\\Lambda_k) $。两个因子可以通过观察式（10.54）得到，且可以读出$ \\mu_k, \\Lambda_k $。与预期相符，结果是一个高斯-Wishart分布，形式为\n",
    "$ q^*(\\mu_k,\\Lambda_k) = \\mathcal{N}(\\mu_k|m_k,(\\beta_k\\Lambda_k)^{-1})W(\\Lambda_k|W_k,v_k) \\tag{10.59} $\n",
    "其中我们已经定义了\n",
    "$ \\begin{eqnarray} \\beta_k &=& \\beta_0 + N_k \\tag{10.60} \\ m_k &=& \\frac{1}{\\beta_k}(beta_0m_0 + N_k\\bar{x}_k) \\tag{10.61} \\ W_k^{-1} &=& W_0^{-1} + N_kS_k + \\frac{\\beta_0N_k}{\\beta_0 + N_k}(\\bar{x}_k - m_0)(\\bar{x}_k - m_0)^T \\tag{10.62} \\ v_k &=& v_0 + N_k \\tag{10.63} \\end{eqnarray} $\n",
    "更新方程类似于混合高斯模型的最大似然解的EM算法的M步骤的方程。我们看到，为了更新模型参数上的变分后验概率分布，必须进行的计算涉及到的在数据集上的求和操作与最大似然方法中的求和操作相同。\n",
    "为了进行这个变分M步骤，我们需要得到表示“责任”的期望$ E[z_{nk}] = r_{nk} $。这些可以通过对式（10.46）给出的$ ρ\\rho_{nk} $进行标准化的方式得到。我们看到，这个表达式涉及到关于变分分布的参数求期望，这些期望很容易求出，从而可得\n",
    "$ \\begin{eqnarray} \\mathbb{E}_{\\mu_k,\\Lambda_k}[(x_n - \\mu_k)^T\\Lambda_k(x_n - \\mu_k)] &=& D\\beta_k^{-1} + v_k(x_n - m_k)^TW_k(x_n - m_k) \\tag{10.64} \\ \\ln\\tilde{\\Lambda}k \\equiv \\mathbb{E}[\\ln|\\Lambda_k|] &=& \\sum\\limits{i=1}^D\\psi\\left(\\frac{v_k + 1 -i}{2}\\right) + D\\ln 2 + \\ln| W_k | \\tag{10.65} \\ \\ln\\tilde{\\pi}_k \\equiv \\mathbb{E}[\\ln\\pi_k] &=& \\psi(\\alpha_k) - \\psi(\\hat{\\alpha}) \\tag{10.66} \\end{eqnarray} $\n",
    "其中我们引入了$ \\tilde{\\Lambda}_k $和$ \\tilde{\\pi}_k $的定义，$ \\psi(\\dot) $是式（B.25）定义的Digamma函数，$ \\alpha = \\sum_k\\alpha_k $。式（10.65）和式（10.66）是从Wishart分布和狄利克雷分布的标准性质中得到的。\n",
    "如果我们将式（10.64）、（10.65）和（10.66）代入式（10.46），然后使用式（10.49），我们得到了下面的“责任”的结果\n",
    "$ r_{nk} \\propto \\tilde{\\pi}_k\\tilde{\\Lambda}_k^{1/2}exp\\left{-\\frac{D}{2\\beta_k} - \\frac{v_k}{2}(x_n - m_k)^T(x_n - m_k)\\right} \\tag{10.67} $\n",
    "注意这个结果与最大似然EM算法得到的“责任”的对应结果的相似性，后者根据式（9.13）可以写成\n",
    "$ r_{nk} \\propto \\pi_k|\\Lambda_k|^{1/2}exp\\left{-\\frac{1}{2}(x_n - \\mu_k)^T(x_n - \\mu_k)\\right} \\tag{10.68} $\n",
    "其中我们使用精度代替了协方差，来强调它与式（10.67）之间的相似性。\n",
    "因此变分后验概率分布的最优化涉及到在两个阶段之间进行循环，这两个阶段类似于最大似然EM算法的E步骤和M步骤。在变分推断的与E步骤等价的步骤中，我们使用当前状态下模型参数上的概率分布来计算公式（10.64）、（10.65）和（10.66）中的各阶矩，从而计算$ \\mathbb{E}[z_{nk}] = r_{nk} $。然后，在接下来的与M步骤等价的步骤中，我们令这些“责任”保持不变，然后使用它们通过公式（10.57）和（10.59）重新计算参数上的变分分布。在任何一种情形下，我们看到变分后验概率的形式与联合概率分布（10.41）中对应因子的函数形式相同。这是一个一般的结果，是由于选择了共轭先验所造成的。\n",
    "图10.6给出了将这种方法应用于老忠实间歇喷泉数据集上的结果。使用的模型是高斯混合模型，有$ K = 6 $个分量。我们看到，在收敛之后，只有两个分量的混合系数的期望值可以与它们的先验值区分开。这种效果可以根据贝叶斯模型中数据拟合与模型复杂度之间的折中来定性的理解。这种模型中的复杂度惩罚的来源是参数被推离了它们的先验值。对于解释数据点没有作用的分量满足$ r_{nk} \\simeq 0 $，从而$ N_k \\simeq 0 $。根据公式（10.58），我们看到$ \\alpha_k \\simeq \\alpha_0 $。根据式（10.60）至（10.63），我们看到其他的参数回到了它们的先验值。原则上，这些分量会微小的适应于数据点，但是对于一大类先验分布来说，这种微小的调整的效果太小了，以至于无法在数值上看出来。对于高斯混合模型，后验概率分布中的混合系数的期望值为\n",
    "$ \\mathbb{E}[\\pi_k] = \\frac{\\alpha_0 + N_k}{K\\alpha_0 + N} \\tag{10.69} $\n",
    "考虑一个分量其中$ N_k \\simeq 0 $且$ \\alpha_k \\simeq \\alpha_0 $。如果先验概率分布很宽，从而$ \\alpha_0 \\to 0 $，那么$ \\mathbb{E}[\\pi_k] \\to 0 $，分量对模型不起作用。而如果先验概率与混合系数密切相关，即$ \\alpha_0 \\to \\infty $，那在图10.6中，混合系数上的先验概率分布是一个狄利克雷分布，形式为（10.39）。回忆一 下，根据图2.5，对于$ \\alpha_0 < 1 $，先验概率分布倾向于选择某些混合系数趋近于零的解。\n",
    " \n",
    "图 10.6 $ K = 6 $个高斯分布的变分贝叶斯混合，应用于老忠实间歇喷泉数据集，其中椭圆表示每个分量的概率密度的一个标准差位置的轮廓线，每个椭圆内部的红点对应于每个分量的混合系数的均值。每张图中左上角的数字表示变分推断迭代的次数。混合系数的期望在数值上与零无法区分的分量没有画出。\n",
    "图10.6是使用$ \\alpha_0 = 10^{−3} $得到的结果，产生了两个混合系数非零的分量。如果我们选择$ \\alpha_0 = 1 $，那么我们得到三个混合系数非零的分量，对于$ \\alpha = 10 $，所有六个分量的混合系数都不等于0。\n",
    "正如我们已经看到的那样，高斯分布的贝叶斯混合的变分解与最大似然的EM算法的解很 相似。事实上，如果我们考虑$ N \\to \\infty $的极限情况，那么贝叶斯方法就收敛于最大似然方法的EM解。对于不是特别小的数据集来说，高斯混合模型的变分算法的主要的计算代价来自于“责任”的计算，以及加权数据协方差矩阵的计算与求逆。这些计算与最大似然EM算法中产生的计算相对应，因此使用这种贝叶斯方法几乎没有更多的计算代价。然而，这种方法有一些重要的优点。首先，在最大似然方法中，当一个高斯分量“退化”到一个具体的数据点时，会产生奇异性，而这种奇异性在贝叶斯方法中不存在。实际上，如果我们简单地引入一个先验分布， 然后使用MAP估计而不是最大似然估计，这种奇异性就会被消除。此外，当我们在混合分布中将混合分量的数量$ K $选得较大时，不会出现过拟合问题，正如我们在图10.6中看到的那样。最后，变分方法使得我们可以在确定混合分布中分量的最优数量时不必借助于交叉验证的技术。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 10.2.2 变分下界"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "我们也可以很容易地计算这个模型的下界（10.3）。在实际应用中，能够在重新估计期间监视模型的下界是很有用的，这可以用来检测是否收敛。它也可以为解的数学表达式和它们的软件执行提供一个有价值的检查，因为在迭代重新估计的每个步骤中，这个下界的值应该不会减小。我们可以进一步地使用变分下界检查更新方程的数学推导和它们的软件执行的正确性，方法是使用有限差来检查每次更新确实给出了下界的一个（具有限制条件的）极大值（Svensen and Bishop, 2004）。\n",
    "对于高斯分布的变分混合，下界（10.3）为\n",
    "$ \\begin{eqnarray} L &=& \\sum\\limits_Z\\int\\int\\int q(Z,\\pi,\\mu,\\Lambda)\\ln\\left{\\frac{p(X,Z,\\pi,\\mu,\\Lambda)}{q(Z,\\pi,\\mu,\\Lambda)}\\right}d\\pi d\\mu d\\Lambda \\ &=& \\mathbb{E}[\\ln p(X,Z,\\pi,\\mu,\\Lambda)] - \\mathbb{E}[\\ln q(Z,\\pi,\\mu,\\Lambda)] \\ &=& \\mathbb{E}[\\ln p(X|Z,\\mu,\\Lambda)] - \\mathbb{E}[\\ln q(Z|X)] + \\mathbb{E}[\\ln p(\\pi)] + \\mathbb{E}[\\ln p(\\mu,\\Lambda)] \\ & & -\\mathbb{E}[\\ln q(Z)] - \\mathbb{E}[\\ln q(\\pi)] - \\mathbb{E}[\\ln q(\\mu,\\Lambda)] \\tag{10.70} \\end{eqnarray} $\n",
    "其中，为了保持记号简洁，我们省略了$ q $分布上的$ * $上标，以及期望算符的下标，因为每个期望是关于它的所有参数进行计算的。下界的各项很容易计算，结果为\n",
    "$ \\begin{eqnarray} \\mathbb{E}[\\ln p(X|Z,\\mu,\\Lambda)] &=& \\frac{1}{2}\\sum\\limits_{k=1}^KN_k\\Bigg{\\ln\\tilde{\\Lambda}k - D\\beta_k^{-1} - v_k Tr(S_kW_k) \\\n",
    "& & - v_k(\\bar{x}k - m_k)^TW_k(\\bar{x}k - m_k) - D\\ln(2\\pi)\\Bigg} \\tag{10.71} \\ \\mathbb{E}[\\ln p(Z|\\pi)] &=& \\sum\\limits{n=1}^N\\sum\\limits{k=1}^Kr{nk}\\ln\\bar{\\pi}k \\tag{10.72} \\ \\mathbb{E}[\\ln p(\\pi)] &=& \\ln C()\\alpha_0) + (\\alpha_0 - 1)\\sum\\limits{k=1}^K\\ln\\tilde{\\pi}k \\tag{10.73} \\ \\mathbb{E}[\\ln p(\\mu,\\Lambda) &=& \\frac{1}{2}\\sum\\limits{k=1}^K\\Bigg{D\\ln\\left(\\frac{\\beta_0}{2\\pi}\\right) + \\ln\\tilde{\\Lambda}k - \\frac{D\\beta_0}{\\beta_k} \\ & & -\\beta_0v_0(m_k - m_0)^TW_k(m_k - m_0)\\Bigg} + K\\ln B(W_0,v_0) \\ & & +\\frac{v_0 - D - 1}{2}\\sum\\limits{k=1}^K\\ln\\tilde{\\Lambda}k - \\frac{1}{2}\\sum\\limits{k=1}{K}v_k Tr(W_0^{-1}W_k) \\tag{10.74} \\ \\mathbb{E}[\\ln q(Z)] &=& \\sum\\limits_{n=1}^N\\sum\\limits_{k=1}^Kr_{nk}\\ln r_{nk} \\tag{10.75} \\ \\mathbb{E}[\\ln q(\\pi)] &=& \\sum\\limits_{k=1}^K(\\alpha_k - 1)\\ln\\tilde{\\pi}k + \\ln C(\\alpha) \\tag{10.76} \\ \\mathbb{E}[\\ln q(\\mu,\\Lambda)] &=& \\sum\\limits{k=1}^K\\left{\\frac{1}{2}\\ln\\tilde{\\lambda}_k + \\frac{D}{2}\\ln\\left(\\frac{\\beta_k}{2\\pi}\\right) - \\frac{D}{2} - H[q(\\Lambda_k)]\\right} \\tag{10.77} \\end{eqnarray} $\n",
    "其中$ D $是$ x $的维度，$ H[q(\\Lambda_k)] $是式（B.82）给出的Wishart分布的熵，系数$ C(\\alpha) $和$ B(W , \\nu) $分别由式（B.23）和式（B.79）定义。注意，涉及到$ q $分布的对数的期望的项仅仅表示这些分布的熵的负值。当这些表达式进行加和给出下界的表达式时，某些项可以组合到一起，使表达式得到简化。然而，我们将各个表达式分开写，为了让理解更容易。\n",
    "最后，值得注意的一点是，下界提供了另一种推导变分重估计方程的方法（变分重估计方程在10.2.1节已经得到）。为了说明这一点，我们使用下面的事实：由于模型有共轭先验，因此变分后验分布（即$ Z $的离散分布、$ \\pi $的狄利克雷分布以及$ (\\mu_k, \\Lambda_k) $的高斯-Wishart分布）的函数形式是已知的。通过使用这些分布的一般的参数形式，我们可以推导出下界的形式，将下界作为概率分布的参数的函数。关于这些参数最大化下界就会得到所需的重估计方程。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 10.2.3 预测概率密度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "在高斯模型的贝叶斯混合的应用中，我们通常对观测变量的新值$ x $的预测概率密度感兴趣。与这个观测相关联的有一个潜在变量$ \\hat{z} $，从而预测概率分布为\n",
    "$ p(\\hat{x}|X) = \\sum\\limits_{\\hat{z}}\\int\\int\\int p(\\hat{x}|\\hat{z},\\mu,\\Lambda)p(\\hat{z}|\\pi)p(\\pi,\\mu,\\Lambda|X)d\\pi d\\mu d\\Lambda \\tag{10.78} $\n",
    "其中$ p(\\pi, \\mu, \\Lambda | X) $是参数的（未知）真实后验概率分布。使用式（10.37）和式（10.38），我们可以首先完成在$ z $上的求和，得到\n",
    "$ p(\\hat{x}|X) = \\sum\\limits_{k=1}^K\\int\\int\\int\\pi_k\\mathcal{N}(\\hat{x}|\\mu_k,\\Lambda_k^{-1})p(\\pi,\\mu,\\Lambda|X)d\\pi d\\mu d\\Lambda \\tag{10.79} $\n",
    "由于剩下的积分是无法计算的，因此我们通过将真实后验概率分布$ p(\\pi, \\mu, \\Lambda | X) $用它的变分近似$ q(\\pi)q(\\mu, \\Lambda) $替换的方式来近似预测概率分布，结果为\n",
    "$ p(\\hat{x}|X) \\simeq \\sum\\limits_{k=1}^K\\int\\int\\int\\pi_k\\mathcal{N}(\\hat{x}|\\mu_k,\\Lambda_k)d\\pi d\\mu_kd\\Lambda_k \\tag{10.80} $\n",
    "其中我们使用了式（10.55）给出的分解方式，且在每一项中，我们已经隐式的将$ i \\neq j$的全部$ {\\mu_j,\\Lambda_j} $变量积分出去。剩余的积分现在可以解析地计算，得到一个学生t分布的混合，即\n",
    "$ p(\\hat{x}|X) \\simeq \\frac{1}{\\hat{\\alpha}}\\sum\\limits_{k=1}^K\\alpha_kSt(\\hat{x}|m_k,L_k,\\nu_k + 1 - D) \\tag{10.81} $\n",
    "其中第$ k $个分量的均值为$ m_k $，精度为\n",
    "$ L_k = \\frac{(\\nu_k + 1 - D)\\beta_k}{1 + \\beta_k}W_k \\tag{10.82} $\n",
    "其中$ \\nu_k $由式（10.63）给出。当数据集的大小$ N $很大时，预测分布（10.81）就变成了高斯混合。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 10.2.4 确定分量的数量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "我们已经看到，变分下界可以用来确定具有$ K $个分量的混合模型的后验概率分布。然而，这里有一个需要强调的比较微妙的地方。对于高斯混合模型的任意给定的参数设置（除了一些特殊的退化的设置之外），会存在一些其他的参数设置，对于这些参数设置，观测变量上的概率密度是完全相同的。这些参数值的差别仅仅是由于分量的重新标记产生的。例如，考虑两个高斯分布的混合以及一个单一的观测变量$ x $，其中参数值为$ \\pi_1 = a，\\pi_2 = b，\\mu_1 = c，\\mu_2 = d，\\sigma_1 = e，\\sigma_2 = f $。那么对于参数值$ \\pi_1 = b，\\pi_2 = a，\\mu_1 = d，\\mu_2 = c，\\sigma_1 = f，\\sigma_2 = e $，即两个分量被交换，此时根据对称性，会给出同样的$ p(x) $值。如果我们有一个由$ K $个分量组成的混合模型，那么每个参数设置都是$ K! $个等价设置中的一个。\n",
    "在最大似然方法中，这种冗余性是不相关的，因为参数最优化算法（例如EM算法）会依赖于参数的初始值，找到一个具体的解，其他的等价的解不起作用。然而，在贝叶斯方法中，我们对所有可能的参数进行积分或求和。我们已经在图10.3中看到了，如果真实的后验概率分布是多峰的，那么基于最小化$ KL(q \\Vert p) $的变分推断会倾向于在某一个峰值的邻域内近似这个分布，而忽视其他的峰值。由于等价的峰值具有等价的预测分布，因此只要我们考虑一个具有具体的 数量$ K $个分量组成的模型，那么这种等价性就无需担心。然而，如果我们项比较不同的$ K $值，那么我们需要考虑这种多峰性。一个简单的近似解法是当我们进行模型比较和平均时，在下界中增加一项$ \\ln K! $。 图10.7给出了包含多峰值因子的下界关于分量数量K的关系图像，数据集是忠老泉的数据。\n",
    " \n",
    "图 10.7 变分下界$ L $与高斯混合模型的分量的数量$ K $的关系图像,数据集是老忠实间歇喷泉的数据。图中展示了$ K = 2 $个分量时的不同的峰值。对于每个$ K $值，模型使用100个不同的起始点进行训练，结果用“+”符号表示。图像中，水平方向被施加了微小的扰动，从而它们可以被区分开。注意，某些解找到了次优的局部极大值，但是这个不经常发生。\n",
    "值得再次强调的是，最大似然方法会使得似然函数的值随着$ K $的值单调递增（假设奇异解已经被避开，并且不考虑局部极大值的效果），因此不能够用于确定一个合适的模型复杂度。相反，贝叶斯推断自动地进行了模型复杂度和数据拟合之间的折中。\n",
    "这种确定K的方法需要对一组具有不同$ K $值的模型进行训练和比较。另一种确定一个合适 的K值的方法是将混合系数$ \\pi $看成参数，通过关于π最大化下界的方式来对它们的值进行点估计（Corduneanu and Bishop， 2001），这种方法没有使用纯粹的贝叶斯方法为它们保留一个概率分布。这种方法会得到下面的重估计方程\n",
    "$ \\pi_k = \\frac{1}{N}\\sum\\limits_{n=1}^Nr_{nk} \\tag{10.83} $\n",
    "并且最大化过程与剩余参数上的分布$ q $的变分更新过程相互交织在一起。对于解释数据集的贡献比较小的分量会让它们的混合系数在最优化的过程中趋于0，因此它们通过自动相关性确定（automatic relevance determination）的方式从模型中移除。这使得我们可以进行一轮训练，这一轮训练开始时，我们选择一个相对较大的K的初始值，然后让多于的分量从模型中被剪枝出去。关于超参数进行最优化时的稀疏性的来源已经在相关向量机中详细讨论过。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 10.2.5 诱导分解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "在推导高斯混合模型的这些变分更新方程时，我们假定了对变分后验概率分布的一种特 定的分解方式，由式（10.42）给定。然而，不同因子的最优解给出了额外的分解。特别的，$ q^(\\mu,\\Lambda) $的最优解由每个混合分量$ k $上的独立分布$ q^(\\mu_k, \\Lambda_k) $的乘积给定，而式（10.48）给定的潜在变量上的变分后验概率分布$ q^(Z) $可以分解为每个观测$ n $的独立概率分布$ q^(z_n) $（注意它不能关于$ k $进行分解，因为对于每个$ n $值，$ z_{nk} $需要满足在$ k $上的和等于1的限制）。这些额外的分解的产生原因是假定的分解方式与真实分布的条件独立性质相互作用的结果，正如图10.5所示的有向图所描述的那样。\n",
    "我们会把这些额外的分解方式成为诱导分解（induced factorizations），因为它们产生于在变分后验分布中假定的分解方式与真实联合概率分布的条件独立性质之间的相互作用。在变分方法的数值实现中，考虑这些附加的分解方式很重要。例如，对于一组变量上的高斯分布来说，如果分布的最优形式的精度矩阵总是对角矩阵（对应于关于由那个高斯分布独立描述的变量的分解方式），那么在计算过程中始终保留一个完整的精度矩阵是一种很低效的做法。\n",
    "使用一种基于d-划分的简单的图检测方法，这种诱导的分解方式可以很容易地被检测到。我们将潜在变量划分为三个互斥的组$ A, B, C $，然后让我们假定我们可以在变量$ C $与剩余变量之间进行分解，即\n",
    "$ q(A,B,C) = q(A,B)q(C) \\tag{10.84} $\n",
    "使用一般的结果（10.9）以及概率的乘积规则，我们看到$ q(A, B) $的最优解为\n",
    "$ \\begin{eqnarray} \\ln q^*(A,B) &=& \\mathbb{E}_C[\\ln p(X,A,B,C)] + const \\ &=& \\mathbb{E}_C[\\ln p(A,B|X,C)] + const \\tag{10.85} \\end{eqnarray} $\n",
    "我们现在考察这个解能否在$ A $和$ B $之间进行分解，即是否有$ q^(A, B) = q^(A)q^*(B) $。当且仅当$ \\ln p(A, B|X, C) = \\ln p(A|X, C) + \\ln p(B|X, C) $时，这种情况成立，也就是说，应该满足条件独立关系：\n",
    "$ A \\perp B | X, C \\tag{10.86} $\n",
    "我们也可以使用d-划分准则来检测对于任意的$ A $和$ B $的选择，这个关系是否确实成立。\n",
    "为了说明这一点，再次考虑由图10.5中的有向图表示的高斯分布的贝叶斯混合，其中我们假定变分分解由式（10.42）给出。我们立刻就可以看到，参数上的变分后验概率分布一定可以在$ \\pi $和剩余的参数$ \\mu, \\Lambda $之间进行分解，因为所有将$ \\pi, \\mu $或$ \\Lambda $相连接的路径一定通过某个$ z_n $结点，所有这些$ z_n $结点都在我们的条件独立性检测的条件集合中，并且所有的$ z_n $结点关于这种路径都是头到尾的。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 10.3 变分线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "作为变分推断的第二个例子，我们回到3.3节的贝叶斯线性回归模型中。在模型证据框架中，我们通过使用最大化似然函数的方法进行点估计，从而近似了在$ \\alpha $和$ \\beta $上的积分。一个纯粹的贝叶斯方法会对所有的超参数和参数进行积分。虽然精确的积分是无法计算的，但是我们可以使用变分方法来找到一个可以处理的近似。为了简化讨论，我们会假设噪声精度参数$ \\beta $已知，并且固定于它的真实值，虽然这个框架很容易扩展来包含$ \\beta $上的概率分布。对于线性回归模型来说，可以证明变分方法等价于模型证据的框架。尽管这样，这个例子给我们提供了使用变分方法的一个很好的练习，也是我们在10.6节讨论贝叶斯逻辑回归的变分方法的基础。\n",
    "回忆一下，$ w $的似然函数和$ w $上的先验概率分布为\n",
    "$ \\begin{eqnarray} p(t|w) &=& \\prod\\limits_{n=1}^N\\mathcal{N}(t_n|w^T\\phi_n,\\beta^{-1}) \\tag{10.87} \\ p(w|\\alpha) &=& \\mathcal{N}(w|0,\\alpha^{-1}I) \\tag{10.88} \\end{eqnarray} $\n",
    "其中$ \\phi_n = \\phi(x_n) $。我们现在引入参数$ \\alpha $上的先验概率分布。根据我们在2.3.6节的讨论，我们知道高斯分布的精度的共轭先验为Gamma分布，因此我们选择\n",
    "$ p(\\alpha) = Gam(\\alpha|a_0,b_0) \\tag{10.89} $\n",
    "其中$ Gam(\\dot|\\dot,\\dot) $由式（B.26）定义。因此所有变量上的联合概率分布为\n",
    "$ p(t,w,\\alpha) = p(t|w)p(w|\\alpha)p(\\alpha) \\tag{10.90} $\n",
    "这可以表示为图10.8中所示的有向图模型。\n",
    " \n",
    "图 10.8 表示贝叶斯线性回归模型的联合概率分布（10.90）的图模型。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# preparation\n",
    "def create_toy_data(func, sample_size, std, domain=[0, 1]):\n",
    "    x = np.linspace(domain[0], domain[1], sample_size)\n",
    "    np.random.shuffle(x)\n",
    "    t = func(x) + np.random.normal(scale=std, size=x.shape)\n",
    "    return x, t\n",
    "\n",
    "def cubic(x):\n",
    "    return x * (x - 5) * (x + 5)\n",
    "\n",
    "x_train, y_train = create_toy_data(cubic, 10, 10., [-5, 5])\n",
    "x = np.linspace(-5, 5, 100)\n",
    "y = cubic(x)\n",
    "\n",
    "feature = PolynomialFeature(degree=3)\n",
    "X_train = feature.transform(x_train)\n",
    "X = feature.transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot\n",
    "vlr = VariationalLinearRegression(beta=0.01)\n",
    "vlr.fit(X_train, y_train)\n",
    "y_mean, y_std = vlr.predict(X, return_std=True)\n",
    "plt.scatter(x_train, y_train, s=100, facecolor=\"none\", edgecolor=\"b\")\n",
    "plt.plot(x, y, c=\"g\", label=\"$\\sin(2\\pi x)$\")\n",
    "plt.plot(x, y_mean, c=\"r\", label=\"prediction\") \n",
    "plt.fill_between(x, y_mean - y_std, y_mean + y_std, alpha=0.2, color=\"pink\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 10.3.1 变分分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "我们的第一个目标是寻找对后验概率分布$ p(w, \\alpha|t) $的一个近似。为了完成这件事，我们使用10.1节的变分框架，变分后验概率分布的分解表达式为\n",
    "$ q(w,\\alpha) = q(w)q(\\alpha) \\tag{10.91} $\n",
    "我们可以使用式（10.9）给出的一般结果来找到这个分布中的因子的重估计方程。回忆一下，对于每个因子，我们取所有变量上的联合概率分布的对数，然后关于不在这个因子中的变量求平均。首先考虑$ \\alpha $上的概率分布。只保留与$ \\alpha $有函数依赖关系的项，我们有\n",
    "$ \\begin{eqnarray} \\ln q^*(\\alpha) &=& \\ln p(\\alpha) + \\mathbb{E}_w[\\ln p(w|\\alpha)] + const \\ &=& (a_0 - 1)\\ln\\alpha - b_0\\alpha + \\frac{M}{2}\\ln\\alpha - \\frac{\\alpha}{2}\\mathbb{E}[w^Tw] + const \\tag{10.92} \\end{eqnarray} $\n",
    "我们看到，这是Gamma分布的对数，因此通过观察$ \\alpha $和$ \\ln \\alpha $的系数，我们有\n",
    "$ q^*(\\alpha) = Gam(\\alpha|a_N,b_N) \\tag{10.93} $\n",
    "其中\n",
    "$ \\begin{eqnarray} a_N = a_0 + \\frac{M}{2} \\tag{10.94} \\ b_N = b_0 + \\frac{1}{2}\\mathbb{E}[w^Tw] \\tag{10.95} \\end{eqnarray} $\n",
    "类似的，我们可以找到$ w $上的后验概率分布的变分重估计方程。同样的，使用一般的结果（10.9），只保留与$ w $有函数依赖关系的项，得到\n",
    "$ \\begin{eqnarray} \\ln q^*(w) &=& \\ln p(t|w) + \\mathbb{E}\\alpha[\\ln p(w|\\alpha)] + const \\tag{10.96} \\ &=& -\\frac{\\beta}{2}\\sum\\limits{n=1}^N{w^T\\phi_n - t_n}^2 - \\frac{1}{2}\\mathbb{E}[\\alpha]w^Tw + const \\tag{10.97} \\ &=& -\\frac{1}{2}w^T(\\mathbb{E}[\\alpha]I + \\beta\\Phi^T\\Phi)w + \\beta w^T\\Phi^Tt + const \\tag{10.98} \\end{eqnarray} $\n",
    "由于这是一个二次型，因此分布$ q^*(w) $是一个高斯分布，因此我们可以使用一般的配平方的方法，得到均值和协方差，结果为\n",
    "$ q^*(w) = \\mathcal{N}(w|m_N,S_N) \\tag{10.99} $\n",
    "其中\n",
    "$ \\begin{eqnarray} m_N &=& \\beta S_N\\Phi^Tt \\tag{10.100} \\ S_N &=& (\\mathbb{E}[\\alpha]I + \\beta\\Phi^T\\Phi)^{-1} \\tag{10.101} \\end{eqnarray} $\n",
    "注意这个结果与$ \\alpha $被当成固定参数时得到的后验概率分布（3.52）的相似性。区别在于，这里$ \\alpha $被替换为了它在变分分布下的期望$ \\mathbb{E}[\\alpha] $。实际上，在两种情形中，我们选择使用了同样的协方差矩阵$ S_N $的记号。\n",
    "使用标准结果（B.27）、（B.38）和（B.39），我们可以得到所需的矩，形式为\n",
    "$ \\begin{eqnarray} \\mathbb{E}[\\alpha] &=& \\frac{a_N}{b_N} \\tag{10.102} \\ \\mathbb{E}[ww^T] &=& m_Nm_N^T + S_N \\tag{10.103} \\end{eqnarray} $\n",
    "变分后验概率分布的计算在开始时，对$ q(w) $或$ q(\\alpha) $中的一个概率分布的参数进行初始化，然后交替地重新更新这些因子，直到满足一个合适的收敛准则（通常根据下界来确定，稍后讨论）。\n",
    "将变分方法得到的解与3.5节使用模型证据得到的解练习起来是很有意义的。考虑$ a_0 = b_0 = 0 $的情形，对应于$ \\alpha $上的一个无限宽的鲜艳概率分布。变分后验概率$ q(\\alpha) $的均值为\n",
    "$ \\mathbb{E}[\\alpha] = \\frac{a_N}{b_N} = \\frac{M / 2}{\\mathbb{E}[w^Tw] / 2} = \\frac{M}{m_N^Tm_N + Tr(S_N)} \\tag{10.104} $\n",
    "与式（9.63）进行对比，表明在这种特别简单的模型中，变分方法得到的解与使用EM算法最大化模型证据函数的方法得到的解完全相同，唯一的区别是$ \\alpha $的点估计被替换为了它的期望 值。由于分布$ q(w) $只通过期望$ \\mathbb{E}[\\alpha] $对$ q(\\alpha) $产生依赖，因此我们看到这两种方法对于无限宽的先验概率分布会给出相同的结果。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 10.3.2 预测分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "给定一个新的输入$ x $，使用参数的高斯变分后验概率很容易计算出$ t $上的预测分布，即\n",
    "$ \\begin{eqnarray} p(t|x,t) &=& \\int p(t|x,W)p(w|t)dw \\ &\\simeq& \\int p(t|x,w)q(w)dw \\ &=& \\int\\mathcal{N}(t|w^T\\phi(x),\\beta^{-1})\\mathcal{N}(w|m_N,S_N)dw \\ &=& \\mathcal{N}(t|m_N^T\\phi(x),\\sigma^2(x)) \\tag{10.105} \\end{eqnarray} $\n",
    "其中我们使用了式（2.115）给出的线性高斯模型的结果计算积分。这里，与输入相关的方差为\n",
    "$ \\sigma^2(x) = \\frac{1}{\\beta} + \\phi(x)^TS_N\\phi(x) \\tag{10.106} $\n",
    "注意，这与我们固定$ \\alpha $得到的结果（3.59）的形式相同，唯一的区别在于现在期望值$ \\mathbb{E}[\\alpha] $出现在$ S_N $的定义中。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 10.3.3 下界"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "另一个很重要的量是下界L，定义为\n",
    "$ \\begin{eqnarray} L(q) &=& \\mathbb{E}[\\ln p(w,\\alpha,t)] - \\mathbb{E}[\\ln q(w,\\alpha)] \\ &=& \\mathbb{E}w[\\ln p(t|w)] + \\mathbb{E}{w,\\alpha}[\\ln p(w|\\alpha)] + \\mathbb{E}\\alpha[\\ln p(\\alpha)] \\ & & - \\mathbb{E}\\alpha[\\ln q(w)]_w - \\mathbb{E}[\\ln q(\\alpha)] \\tag{10.107} \\end{eqnarray} $\n",
    "使用之前章节得到的结果，计算各项的值是很容易的，结果为\n",
    "$ \\begin{eqnarray} \\mathbb{E}[\\ln p(t|w)]w &=& \\frac{N}{2}\\ln\\left(\\frac{\\beta}{2\\pi}\\right) - \\frac{\\beta}{2}t^Tt + \\beta m_N^T\\Phi^Tt \\ & & -\\frac{\\beta}{2}Tr[\\Phi^T\\Phi(m_Nm_N^T + S_N)] \\tag{10.108} \\ \\mathbb{E}[\\ln p(w|\\alpha)]{w,\\alpha} &=& -\\frac{M}{2}\\ln(2\\pi) + \\frac{M}{2}(\\psi(a_N) - \\ln b_N) \\ & & -\\frac{a_N}{2b_N}[m_N^Tm_N + Tr(S_N)] \\tag{10.109} \\ \\mathbb{E}[\\ln p(\\alpha)]_\\alpha &=& a_0\\ln b_0 + (a_0 - 1)[\\psi(a_N) - \\ln b_N] \\ & & b_0\\frac{a_N}{b_N} - \\ln\\Gamma(a_0) \\tag{10.110} \\ -\\mathbb{E}[\\ln q(w)]w &=& \\frac{1}{2}\\ln | S_N | + \\frac{M}{2}[1 + \\ln(2\\pi)] \\tag{10.111} \\ -\\mathbb{E}[\\ln q(\\alpha)]\\alpha &=& \\ln\\Gamma(a_N) - (a_N - 1)\\Psi(a_N) - \\ln b_N + a_N \\tag{10.112} \\end{eqnarray} $\n",
    "图10.9给出了下界$ L(q) $与多项式模型的阶数的关系图像，数据集是从一个三阶多项式中人工生成的。这里，先验参数被设置为$ a_0 = b_0 = 0 $，对应于无信息先验$ p(\\alpha) \\propto 1 $。根据2.3.6节的讨论，它是$ \\ln \\alpha $上的均匀分布。正如我们在10.1节看到的那样，$ L $表示模型的对数边缘似然函数$ \\ln p(t|M) $的下界。因此，变分框架将最高的概率赋予了$ M = 3 $的模型。这与最大似然的结果相反。最大似然方法通过增加模型的复杂度尽可能地让误差变小，直到误差趋于0，这导致了最大似然方法倾向于选择具有严重过拟合现象的模型。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 10.4 指数族分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "在第2章中，我们讨论了指数族概率分布和它们的共轭先验的重要作用。对于本书中讨论的许多模型来说，完整数据是服从指数族分布的。然而，通常这对于观测数据的边缘似然函数来说是不成立的。例如，在混合高斯模型中，观测数据$ x_n $和对应的隐含变量$ z_n $的联合概率分布是指数族分布的成员，但是$ x_n $的边缘概率分布是高斯混合分布，因此不是指数族的成员。\n",
    "目前为止，我们将模型中的变量分为了观测变量和隐含变量两组。我们现在进一步地将潜在变量和参数区分开。潜在变量（记作$ Z $ ）是分散的（extensive），它的数量随着数据集规模的增大而增大。参数（记作$ \\theta $）是聚集的（intensive），它的数量固定，与数据集的规模无关。例如，在高斯混合模型中，指示变量$ z_{kn} $（表示哪个分量$ k $对生成数据点$ x_n $起作用）表示潜在变量，而均值$ \\mu_k $、精度$ \\Lambda_k $以及混合系数$ \\pi_k $表示参数。\n",
    "考虑独立同分布数据的情形。我们将数据的值记作$ X = {x_n} $其中$ n = 1,...,N $，对应的潜在变量为$ Z = {z_n} $。现在假设观测变量和隐含变量的联合概率分布为指数族分布的成员，参数为自然参数$ \\eta $，即\n",
    "$ p(X,Z|\\eta) = \\prod\\limits_{n=1}^Nh(x_n,z_n)g(\\eta)exp{\\eta^Tu(x_n,z_n)} \\tag{10.113} $\n",
    "使用$ \\eta $的共轭先验，可以把它写成\n",
    "$ p(\\eta|\\nu_0,\\chi_0) = f(\\nu_0,\\chi_0)g(\\eta)^{\\nu_0}exp{\\nu_0\\eta^T\\chi_0} \\tag{10.114} $\n",
    "回忆一下，共轭先验分布的意义为，对于$ u $向量来说，所有值为$ \\chi_0 $的观测的先验数量$ \\nu_0 $。现在考虑一个变分分布，它可以在潜在变量和参数之间进行分解，即$ q(Z, \\eta) = q(Z)q(\\eta) $。使用一般的结果（10.9），我们可以解出这两个因子，如下所述。\n",
    "$ \\begin{eqnarray} \\ln q^*(Z) &=& \\mathbb{E}\\eta[\\ln p(X,Z|\\eta)] + const \\ &=& \\sum\\limits{n=1}^N{\\ln h(x_n,z_n) + \\mathbb{E}[\\eta^T]u(x_n,z_n)} + const \\tag{10.115} \\end{eqnarray} $\n",
    "因此我们看到它可以分解为一组相互独立的项的和,每个$ n $都对应于一项，因此$ q^(Z) $的解可以在$ n $上进行分解，即$ q^(Z) = \\prod_n q^*(z_n) $。这是诱导分解的一个例子。两侧取指数，我们有\n",
    "$ q^*(z_n) = h(x_n,z_n)g(\\mathbb{E}[\\eta])exp{\\mathbb{E}[\\eta^T]u(x_n,z_n)} \\tag{10.116} $\n",
    "其中标准化系数已经通过与指数族分布的标准形式进行比较的方式得到。\n",
    "类似的，对于参数上的变分分布，我们有\n",
    "$ \\begin{eqnarray} \\ln q^*(\\eta) &=& \\ln p(\\eta|\\nu_0,\\chi_0) + \\mathbb{E}Z[\\ln p(X,Z|\\eta)] + const \\tag{10.117} \\ &=& \\nu_0\\ln g(\\eta) + \\nu_0\\eta^T\\chi_0+\\sum\\limits{n=1}^N{\\ln g(\\eta) + \\eta^T\\mathbb{E}_{z_n}[u(x_n,z_n)]} + const \\tag{10.118} \\end{eqnarray} $\n",
    "同样的，两侧取指数，然后通过观察法确定标准化系数，我们有\n",
    "$ q^*(\\eta) = f(\\nu_N,\\chi_N)g(\\eta)^{\\nu_N}exp{\\nu_N\\eta^T\\chi_N} \\tag{10.119} $\n",
    "其中我们已经定义了\n",
    "$ \\begin{eqnarray} \\nu_N &=& \\nu_0 + N \\tag{10.120} \\ \\nu_N\\chi_N &=& \\nu_0\\chi_0 + \\sum\\limits_{n=1}^N\\mathbb{E}_{z_n}[u(x_n,z_n)] \\tag{10.121} \\end{eqnarray} $\n",
    "注意，$ q^(z_n) $的解与$ q^(\\eta) $的解相互偶合，因此我们可以使用一个两阶段的迭代方法进行求解。在变分E步骤中，我们使用潜在变量上的当前后验概率分布$ q(z_n) $计算充分统计量的期望$ \\mathbb{E}[u(x_n, z_n)] $，并且使用这个结果计算参数上的修正的后验概率分布$ q(\\eta) $。然后，在接下来的变分M步骤中，我们使用修正后的参数后验概率分布寻找自然参数的期望$ E[\\eta^T] $，它给出了潜在变量上的修正后的变分分布。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 10.4.1 变分信息传递"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "我们通过详细讨论一个具体的模型来说明变分方法的应用，这个模型是高斯模型的贝叶斯混合。这个模型可以被表示为图10.5中的有向图。这里我们从更一般的角度来讨论由有向图描述的模型中对变分方法的使用，推导出一些具有广泛适用性的结果。\n",
    "对应于有向图的联合概率分布可以写成下面的分解形式\n",
    "$ p(x) = \\prod\\limits_i p(x_i|pa_i) \\tag{10.122} $\n",
    "其中$ x_i $表示与结点$ i $关联的变量，$ pa_i $表示与结点$ i $对应的父结点集合。注意，$ x_i $可能是一个潜在变量，也可能属于观测变量集合。现在，考虑一个变分近似，其中我们假定概率分布$ q(x) $可以关于$ x_i $进行分解，即\n",
    "$ q(x) = \\prod\\limits_i q_i(x_i) \\tag{10.123} $\n",
    "注意，对于观测结点，在变分分布中没有因子$ q(x_i) $。我们现在将公示（10.122）代入我们的一般结果（10.9）中，可得\n",
    "$ \\ln q_j^*(x_j) = \\mathbb{E}_{i \\neq j}\\left[\\sum\\limits_i\\ln p(x_i|pa_i)\\right] + const \\tag{10.124} $\n",
    "等式右手边的任何不依赖于$ x_j $的项都可以整合到可加性常数中。事实上，唯一依赖于$ x_j $的项是由$ p(x_j | pa_j) $给出的$ x_j $的条件概率分布以及任何在条件集合中具有$ x_j $的条件概率分布。根据定义，这些条件概率分布对应于结点$ j $的子结点，因此他们也依赖于子结点的同父结点（co-parents），即子结点的除了结点$ x_j $本身之外的其他父结点。我们看到，$ q_j^*(x_j) $所依赖的所有结点组成的集合对应于结点$ x_j $的马尔科夫毯，如图8.26所示。因此，在变分后验概率分布中的更新因子表示图上的一个局部计算。这使得构建用于变分推断的具有一般性的软件成为可能，在这种一般性的变分推断中，模型的形式不必事先指定(Bishop et al., 2003)。\n",
    "如果我们现在确定模型的形式，其中所有的条件概率分布都有一个共轭-指数族的结构，那么变分推断的过程可以被转化为局部信息传递算法（Winn and Bishop, 2005）。特别的，对于一个特定的结点来说，一旦它接收到了来自所有的父结点和所有的子结点的信息，那么与这个结点相关联的概率分布就可以被更新。这反过来需要子结点从它们的同父结点已经接收完毕信息。下界的计算也可以得到简化，因为许多必要的值已经作为信息传递框架的一部分计算完毕。分布的信息传递形式有很好的缩放性质，对于大的网络很合适。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 10.5 局部变分方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "10.1节和10.2节讨论的变分框架，由于它直接寻找所有随机变量上的完整的后验概率分布的近似，所以可以被看做“全局”方法。另一种“局部”的方法涉及到寻找模型中的单独的变量或者变量组上定义的函数的界限。如：我们可能寻找条件概率分布$ p(y|x) $的界限，这个条件概率本身仅仅是一个由有向图模型描述的更大的概率模型中的一个因子。引入界限的目的显然是简化最终得到的概率分布。这个局部近似可以应用于多个变量，直到得到一个可以处理的近似。在10.6.1节，我们会在logistic回归的问题中给出这种方法的一个实际例子。这里，我们关注求解界限本身。\n",
    "我们已经看到，在我们对Kullback-Leibler散度的讨论中，对数函数的凸函数性质在求解全局变分方法的下界时起着关键的作用。我们将一个（严格）凸函数定义为每条弦都位于函数上方的函数。凸函数的性质对于局部变分的框架也起着核心的作用。注意，我们的讨论同样适用于凹函数，只需交换“最大值”运算与“最小值”运算，将下界变为上界即可。\n",
    " \n",
    "图 10.10 在左图中，红色曲线表示函数$ exp(−x) $，蓝色直线表示式（10.125）定义的在$ x = \\xi $处的切线，其中$ \\xi = 1 $。这条直线的斜率为$ \\eta = f'(\\xi) = − exp −\\xi $。注意，任何其它的切线，例如绿色的切线，在$ x = \\xi $处都会有一个更小的$ y $值。右图给出了函数$ \\eta\\xi − g(\\eta) $关于$ \\eta $的图像，其中$ g(\\eta) $由式（10.131）给出，$ \\xi = 1 $，此时最大值对应于$ \\eta = − exp(−\\xi) = −1/e $。\n",
    "让我们首先考虑一个简单的例子，即函数$ f(x) = exp(−x) $，它是$ x $的一个凸函数，如图10.10的左图所示。我们的目标是使用一个简单的函数来近似$ f(x) $，特别的，使用$ x $的一个线性函数。根据图10.10，我们看到，如果这个线性函数对应于一条切线，那么它是函数$ f(x) $的下界。我们可以得到在一个具体的$ x $处的$ y(x) $的切线，例如$ x = \\xi $处，方法是使用一阶泰勒展开式\n",
    "$ y(x) = f(\\xi)+ f'(\\xi)(x - \\xi) \\tag{10.125} $\n",
    "从而$ y(x) \\leq f(x) $，且等号只在$ x = \\xi $时成立。对于我们的例子，函数$ f(x) = exp(−x) $，因此我们得到了切线的形式如下\n",
    "$ y(x) = exp(-\\xi) - exp(-\\xi)(x - \\xi) \\tag{10.126} $\n",
    "它是一个以$ \\xi $为参数的线性函数。为了与后续的讨论相容，让我们定义$ \\eta = −exp(−\\xi) $，即\n",
    "$ y(x,\\eta) = \\eta x - \\eta + \\eta\\ln(-\\eta) \\tag{10.127} $\n",
    "不同的$ \\eta $值对应于不同的切线，并且由于所有的切线都是函数的下界，因此我们有$ f(x) \\geq y(x,\\eta) $。因此我们可以将函数写成下面的形式\n",
    "$ f(x) = \\max_\\eta{\\eta x - \\eta + \\eta\\ln(-\\eta)} \\tag{10.128} $\n",
    "我们已经成功地用一个简单的线性函数$ y(x,\\eta) $来近似凸函数$ f(x) $。代价是我们引入了一个变分参数$ \\eta $，并且为了得到最紧致的界限，我们必须关于$ \\eta $进行最优化。\n",
    "我们可以使用凸对偶（convex duality）的框架更加一般地形式化描述这种方法（Rockafellar, 1972; Jordan et al., 1999）。考虑图10.11的左侧图给出的凸函数$ f(x) $。在这个例子中，函数$ \\eta x $是$ f(x) $的一个下界，但不是斜率为$ \\eta $的线性函数能够达到的的最好的下界，因为最紧致的下界由切线给出。让我们将斜率为$ \\eta $的切线的方程写成$ \\eta x − g(\\eta) $，其中截距（的负值）$ g(\\eta) $显然依赖于切线的斜率$ \\eta $。为了确定截距，我们注意到这条直线必须垂直移动一段距离，这段距离等于直线和函数之间最小的垂直距离，如图10.11所示。\n",
    " \n",
    "图 10.11 在左图中，红色曲线给出了一个凸函数$ f(x) $，蓝色曲线表示线性函数$ \\eta x $,它是$ f(x) $的一个下界，因为对于所有的$ x $都有$ f(x) > \\eta x $。对于给定的斜率$ \\eta $的值，具有相同斜率的切线的接触点可以通过关于$ x $最小化差距$ f(x) − \\eta x $的方式得到，差距用绿色虚线表示。这定义了对偶函数$ g(\\eta) $,它对应于具有斜率$ \\eta $的切线的截距（的负值）。\n",
    "因此\n",
    "$ \\begin{eqnarray} g(\\eta) &=& -\\min_x{f(x) - \\eta x} \\ &=& \\max_x{\\eta x - f(x) } \\tag{10.129} \\end{eqnarray} $\n",
    "现在，我们不去固定$ \\eta $改变$ x $，而是可以考虑一个特定的$ x $值，然后调节$ \\eta $，直到切平面在这个特定的$ x $处与函数$ f(x) $相切。由于在特定的$ x $处，当切线的$ y $值与它的连接点的$ y $值相等时，$ y $的值最大，因此我们有\n",
    "$ f(x) = \\max_\\eta{\\eta x - g(\\eta)} \\tag{10.130} $\n",
    "我们看到函数$ f(x) $和$ g(\\eta) $的角色是对偶的，二者通过式（10.129）和式（10.130）相互关联。\n",
    "让我们将这两个对偶关系应用到我们简单的例子$ f(x) = exp(−x) $中。根据式（10.129），我们看到$ x $的最大值为$ \\xi = − \\ln(−\\eta) $，代回式中，我们得到了共轭函数$ g(\\eta) $，形式为\n",
    "$ g(\\eta) = \\eta - \\eta\\ln(-\\eta) \\tag{10.131} $\n",
    "与之前得到的结果相同。对于$ \\xi = 1 $的情况函数$ \\eta\\xi − g(\\eta) $的图像如图10.10右侧所示。作为检查，我们可以将式（10.131）代入到式（10.130），这给出了最大值$ \\eta = − exp(−x) $，代回到式中就恢复出了原始的函数$ f(x) = exp(−x) $。\n",
    "对于凹函数，我们可以采用类似的推导方式，得到上界，其中“最大化”运算被替换为“最小化”运算，即\n",
    "$ \\begin{eqnarray} f(x) &=& \\min_\\eta{\\eta x - g(\\eta)} \\tag{10.132} \\ g(\\eta) &=& \\min_x{\\eta x - f(x)} \\tag{10.133} \\end{eqnarray} $\n",
    "如果感兴趣的函数不是凸函数（或凹函数），那么我们不能直接应用这种方法得到上述界限。然而，我们可以首先寻找函数或者参数的一个可逆变换，这个变换将函数或者参数变换为一个凸函数的形式。然后，我们计算共轭函数，之后变换回原始的变量。\n",
    "在模式识别中经常出现的一个重要的例子时logistic sigmoid函数，它的定义为\n",
    "$ \\sigma(x)＝ \\frac{1}{1 + e^{-x}} \\tag{10.134} $\n",
    "这不是凹函数也不是凸函数。然而，如果我们取对数，那么我们就得到了一个凹函数，这一点通过取二阶导数的方式很容易证明。根据式（10.133），对应的共轭函数的形式为\n",
    "$ g(\\eta) = \\min_x{\\eta x - f(x)} = -\\eta\\ln\\eta - (1-\\eta)\\ln(1-\\eta) \\tag{10.135} $\n",
    "我们看到，它是一个二值变量的熵，这个变量的取值为$ 1 $的概率为$ \\eta $。使用式（10.132）得到对数sigmoid函数的一个上界\n",
    "$ \\ln\\sigma(x) \\leq \\eta x - g(\\eta) \\tag{10.136} $\n",
    "然后取指数，得到logistic sigmoid函数的一个上界，形式为\n",
    "$ \\sigma(x) \\leq exp(\\eta x - g(\\eta)) \\tag{10.137} $\n",
    "对于两个不同的$ \\eta $值，图像如图10.12的左图所示。\n",
    " \n",
    "图 10.12 左图中，红色曲线给出了式（10.134）定义的logistic sigmoid函数$ \\sigma(x) $。同时给出的还有两个指数上界（10.137）的例子，用蓝色曲线表示。右图再次用红色曲线给出了logistic sigmoid函数。同时给出的还有高斯下界（10.144），用蓝色曲线表示。这里，参数$ \\xi = 2.5 $，界限在$ x = \\xi $和$ x = −\\xi $出事精确的，用绿色曲线标记。\n",
    "我们也可以得到sigmoid函数的下界，下界的函数形式是高斯形式。为了达到这个目的，我们采用Jaakkola and Jordan(2000)的方法，对输入变量和函数本身都进行变换。首先，我们取logistic函数的对数，然后将其分解，即\n",
    "$ \\begin{eqnarray} \\ln\\sigma(x) &=& -\\ln(1+e^{-e}) = -\\ln\\left{e^{-x/2}\\left(e^{x/2} + e^{-x/2}\\right)\\right} \\ &=& \\frac{x}{2} - \\ln\\left(e^{x/2} + e^{-x/2}\\right) \\tag{10.138} \\end{eqnarray} $\n",
    "我们现在注意到，函数$ f(x) = −\\ln(e^{x/2} + e^{-e/2}) $是变量$ x^2 $的一个凸函数，这一点可以通过取二阶导数的方式证明。这产生了$ f(x) $的下界，它是$ x^2 $的一个线性函数，它的共轭函数为\n",
    "$ g(\\eta) = \\max_{x^2}\\left{\\eta x^2 - f\\left(\\sqrt{x^2}\\right)\\right} \\tag{10.139} $\n",
    "根据驻点的条件可得\n",
    "$ 0 = \\eta - \\frac{dx}{dx^2}\\frac{d}{dx}f(x) = \\eta + \\frac{1}{4x}\\tanh\\left(\\frac{x}{2}\\right) \\tag{10.140} $\n",
    "如果我们将这个值记作$ x $，对应于在这个特定的$ \\eta $值下，函数与切线的接触点，记作$ \\eta $，那么我们有\n",
    "$ \\eta = -\\frac{1}{4\\xi}\\tanh\\left((\\frac{\\xi}{2}\\right) = - \\frac{1}{2\\xi}\\left([\\sigma(\\xi) - \\frac{1}{2}\\right] \\tag{10.141} $\n",
    "其中，我们定义了$ \\lambda = −\\eta $，保持与Jaakkola and Jordan(2000)的相容性。我们不把$ \\lambda $看成变分参数，相反，我们可以令$ \\xi $为变分参数，因为这会产生共轭函数的更简单的表达式，它的形式为\n",
    "$ g(\\lambda) = \\lambda(\\xi)\\xi^2 − f(\\xi) = \\lambda(\\xi)\\xi^2 + \\ln(e^{\\xi/2} + e^{−\\xi/2}) \\tag{10.142} $\n",
    "这里，$ f(x) $的界限可以写成\n",
    "$ f(x) \\geq -\\lambda(\\xi)x^2 - g(\\lambda(\\xi)) = -\\lambda(\\xi)x^2 - \\lambda(\\xi)\\xi^2 - \\ln(e^{\\xi/2} + e^{-\\xi/2}) \\tag{10.143} $\n",
    "sigmoid函数的界限就变成了\n",
    "$ \\sigma(x) \\geq \\sigma(\\xi)exp\\left{\\frac{x - \\xi}{2} - \\lambda(\\xi)(x^2 - \\xi^2)\\right} \\tag{10.144} $\n",
    "其中$ \\lambda(\\xi) $的定义为（10.141）。这个界限如图10.12的右图所示。我们看到，界限的函数形式是$ x $的二次函数的指数形式。当我们寻找通过logistic sigmoid函数定义的后验概率分布的高斯表示时，这个界限的形式很有用。\n",
    "logistic sigmoid函数在二值变量上的概率模型中经常出现，因为它是将log odds函数转换为后验概率分布的函数。对于多类分布，对应的变换由softmax函数给出。不幸的是，这里推导出logistic sigmoid函数的下界不能直接扩展到softmax函数。Gibbs(1997)提出了一种构建高斯分布的方法，这个高斯分布被猜想为是一个界限（虽然没有给出严格的证明），这可以用于将局部变分方法应用到多分类问题。\n",
    "我们会在10.6.1节看到局部变分界限的一个例子。然而，现阶段从一般的角度考虑这些界限如何被使用是很有意义的。假设我们想计算一个形式如下的积分\n",
    "$ I = \\int \\sigma(a)p(a)da\\tag{10.147} $\n",
    "其中$ \\sigma(a) $是一个logistic sigmoid函数，$ p(a) $是一个高斯概率密度。当我们项计算贝叶斯模型中的预测分布时，这种积分会经常出现，此时$ p(a) $表示一个后验参数分布。由于积分是无法直接计算的，因此我们使用变分界限（10.144），我们将它写成$ \\sigma(a) \\geq f(a,\\xi) $，其中$ \\xi $是一个变分参数。积分现在变成了两个指数-二次函数的乘积，因此可以解析地求出积分，给出$ I $的界限\n",
    "$ I \\geq \\int f(a,\\xi)p(a)da = F(\\xi) \\tag{10.146} $\n",
    "我们可以自由地选择变分参数$ \\xi $，这里我们选择最大化函数$ F(\\xi) $的值$ \\xi^* $。得到的值$ F(\\xi^) $表示在所有的界限中最紧致的界限，可以用来近似$ I $。然而，这个最优化的界通常不是精确的。虽然logistic sigmoid函数的界限$ \\sigma(a) \\geq f(a, \\xi) $可以被精确地最优化，但是$ \\xi $的最优选择依赖于$ a $的值，从而界限只对一个$ a $的值是精确的。由于$ F(\\xi) $可以通过对a的所有值上进行积分的方式得到，因此$ \\xi^ $的值表示一个折中，权值为概率分布$ p(a) $。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.6 变分logistic回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们现在回到4.5节研究的贝叶斯logistic回归模型，说明局部变分方法的应用。在4.5节，我们将注意力集中于拉普拉斯近似的使用，而这里，我们考虑一种贝叶斯的方法，本方法基 于Jaakkola and Jordan(2000)的方法。与拉普拉斯方法相似，这也会生成后验概率分布的高斯近似。然而，变分方法的极大的灵活性使得模型的准确率与拉普拉斯相比有所提升。此外，与拉普拉斯方法不同，变分方法最优化一个具有良好定义的目标函数，这个目标函数由模型证据的一个严格界限给定。Dybowski and Roberts(2005)也从贝叶斯的角度研究了logistic回归问题，使用了蒙特卡罗取样的技术。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# preparaion\n",
    "def create_toy_data(add_outliers=False, add_class=False):\n",
    "    x0 = np.random.normal(size=50).reshape(-1, 2) - 3.\n",
    "    x1 = np.random.normal(size=50).reshape(-1, 2) + 3.\n",
    "    return np.concatenate([x0, x1]), np.concatenate([np.zeros(25), np.ones(25)]).astype(np.int)\n",
    "x_train, y_train = create_toy_data()\n",
    "x0, x1 = np.meshgrid(np.linspace(-7, 7, 100), np.linspace(-7, 7, 100))\n",
    "x = np.array([x0, x1]).reshape(2, -1).T\n",
    "feature = PolynomialFeature(degree=1)\n",
    "X_train = feature.transform(x_train)\n",
    "X = feature.transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATYAAAD8CAYAAAD9uIjPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO2deZicVZX/P6e27nQnnZ0EspAQQyCBABHCvoQ1USSKDpMogoiGKMjyE5VFmXkcdRzUcWXEDOCIgBjZccI6KCgkQNgTEswGSQgh+57uquo6vz/etzrV1VXVVV3vWnU/z1NPd73vve+5nXR/65x77r1HVBWDwWCoJSJ+D8BgMBicxgibwWCoOYywGQyGmsMIm8FgqDmMsBkMhprDCJvBYKg5jLAZDAbXEZE7RGSDiCwqcl9E5BcislxE3hSRSTn3porIO/a968qxZ4TNYDB4wf8AU0vcnwaMtV+zgF8DiEgUuMW+Px6YKSLjuzNmhM1gMLiOqj4HbCnRZDpwp1osAPqJyP7AZGC5qq5U1SRwr922JDEnBl0p/QcO0ANGjvDDdFW0axsNkbjPo2glJgmfx1AF2oZIg9+jqHleefXtTao6uJpnTD3nJN20aWu59hYDrTmX5qjqnArMDQPW5Lxfa18rdP3Y7h7mi7AN3H9/5j7zmB+mq2J76l0ADupT1e9LVbSn32Fgw0jf7FdNegUN8bF+j6LmiSQOf6/aZ2zatJWXF/yxXHutqnp0FeakwDUtcb30eKoYSFWs+LCUVxpM+sZH+T0EADa3rfZ7CAaD06wFcsO44cC6EtdL4ouwNUQsRzGs4rZy50bf7Edj43yzbTC4yCPARXZ29Dhgu6p+ALwMjBWR0SKSAGbYbUvim8c2ptkK58IoboCv4mYwhA0R+QMwHxgnImtF5FIRmS0is+0m84CVwHLgv4GvAqhqGrgCeAJYAsxV1cXd2fNlji3LmObBrNi9kRUfbmHMkAF+DqUi+sZHdcy3+UE0No7NbSGfazPUFao6s5v7Clxe5N48LOErG9+Xe4TZczNeWw+IjaEttczvURhqHN+FDcIpbtlEgp/iZpIIBkNhAiFsEG5x8wOTRDAYihMYYYNwihsYr81gCBqBEjYIn7j5GZIar81gKEzghA3CK24GgyEYBFLYIHziBv6FpCYcNRg644iwiUg/EblPRJaKyBIROd6J54ZJ3Pzy2sIajpolHwY3ccpj+znwuKoeAhyBtULYEcImbsZrK4PYGL9HYKhxqhY2EWkBTgFuB1DVpKpuq/a5uYRJ3MD7kDSsXpvB4BZOeGwHARuB34rIayJym4g05zcSkVkislBEFm7dUrlAhUXcTCLBYPAfJ4QtBkwCfq2qRwG7gS7nkqvqHFU9WlWP7j+gZ/tCwyRufnhtoQpHDQYXcULY1gJrVfVF+/19WELnCmERNzB7SQ0Gv6ha2FR1PbBGRLITPWcAb1f73FKEQdz8CknD5LWZzKjBLZzKin4NuFtE3gSOBH7g0HOLEgZxA2+9tlAlEUxm1OAijgibqr5uz59NVNVPqmp5FSCqJOji5td2qzB5bQaDGwR250G5hEXcvCJUXpvB4BKhFzYIvriBSSQYDF5SE8IGwRY3P0LSsISjJoFQP4jIVBF5R0SWi0iXJWEi0l9EHhSRN0XkJRE5LOfeVSKySEQWi8jV3dmqGWGDcIibF4QmHDUJhLpBRKLALcA0YDwwU0TG5zW7AXhdVScCF2Ft1cQWuC9jVYU/AjhXREoWp60pYYNgixsYr81Qt0wGlqvqSlVNAvcC0/PajAf+D0BVlwKjRGQIcCiwQFX32FWrngU+VcpYzQkbBFfcvAxJQ+O1GQKLahttqWVlvcpgGLAm5/1a+1oubwDnA4jIZOBArALJi4BTRGSgiDQBH6NzEeUu+Fp+z02CWtrP69J9m9tWmzJ9hp4hDZVMFwwSkYU57+eo6pzcpxXoo3nvfwj8XEReB94CXgPSqrpERP4DeArYhSWA6VKDqUmPLUtQPTcwXlsuJoFQE2zK7gW3X3Py7q+ls5c1HFiX20BVd6jqJap6JNYc22BglX3vdlWdpKqnAFuAkr80NS1sEExxMyeA5GASCPXCy8BYERktIglgBvBIbgP7wNqE/fZLwHOqusO+t5/9dSRWuPqHUsZqXtgguOLmldcWhCSCZvagyRfQ5Hw0s8fv4Rg8xp70vwJ4Ausg2rmqulhEZovIbLvZocBiEVmKlT29KucR94vI28CjwOXd7W6q2Tm2fII657Zy50YO6jPY72G4irY9D7t+CWJ/jmoG7X010nCcvwMzeIqqzgPm5V27Nef7+UDBZRyqenIlturCY8sSNM/Ny5DUL69NM5tg1y+ANtC91os22PUzNLPvoGUzz2ZwkroSNgimuLkdkvqaRGh7nq7JL5vkfOurmWczOEzdCRsET9zAmyypL16btgHtBW602/e8R9u3kdl9O5mtXyWz/Xq0bb4v4zC4R10KGwRL3LwISX3z2hIfBeIFbkQh7tpBywBoZifavhbVVM617ej2a6D1cch8AOml6K6fk9nzR1fHYvCWuhU2CJ641aLXJrEx0HCatdjTugI0QOPZSKzzwmGn5tlUW8ns+DG69VJ02zfQLReT2fuYdW/vPNDddF7f2QZ7H0Azuxyxb/CfuhY2CJa4gbshqW9eW/Nl0PsGaDgDGk6Hlm9D0yWd2zg4z6a7fgWpl4EU0ArshT2/Q9sWQvp1+3o+cWh/17ExGPyl7oUNgiNuXmVJPffaRJDERKT3FdYrfhgihXbYVI9mdkHyJSCZd6cN3XsvZHYU6ZmGSH9XxmTwHiNsNkESt5r02sqk6nBUdwDRwvfaV0Km0L9tFGKjkGj+nmxDWDHClkNQxA3q9MRdJ8LRyH4UFTaUghna2CFIn+urt20IDEbY8giCuLkdkgZlm1U5qLaj6ZVo+xpUi6yHy0EkBs0XAQ3dtrVoQJq/iET6VjVOQ7AwwlaAoIhbXXpt7AtHNfkGuuVSdPu3rezmtsvR9jXd9IZI49lIn29AbDzQSOETc7IoiBG1WsMxYRORqIi8JiJ/duqZfhIEcQP3QtLAem12OKrtm9Cd/w5sB+xtWJn16LbvdFqXVgxJfBTp/Q2ssLSYpxeD2AQkOtCZsRsCg5Me21VYu/ZrBr/Fza+6pEFA254BMvlXgSQkX+22f2bPH9Fts4ASJ4nEJiB9vl7FKA1BxRFhE5HhwMeB25x4XpAIiri5QZC9tmRqJYXXm2VAtxW4vg9Nvg57H7T7F/LWohCbSKTvvyKR5urHawgcTnlsPwO+SdeP2A5EZJaILBSRhVu3+J91rAS/xQ3q0GuLfwRrfqzQvfziRp3R1seBEvtQI0OQPtf2eGiG4FO1sInIucAGVX2lVDtVnZM9Nri5d0u1Zj3HT3FzMyQNrtd2GEkZSOfsZgMkjkOiJet4gHZzkGV8AhLpU+0IDQHGCY/tROA8EXkXq6TW6SJyV3edVr+7idXvbnLAvHcEQdzqBYkfDL2/Ar1mQPQgiI1Del+G9L6y+86JEyl+hmoExISftU7Vwqaq16vqcFUdhXWO+TOqemGpPg2xGGMGW6fYGnGrjHry2kQSpOITiPT7CZG+P0QapiDS/a+sNE6B6IFF7saRxtOdHaghcPi6ji1X3MIkcH6JW91lSXu8EyFubbxPnIG13COKFdImoPnL3YeyhtDjqLCp6l9V9dxK+owZPCCU3pvf4uY0QfXaoLL9o9q+Bt32FdjxHfuE3gZoPB/pfQ3S/w4ijWe4N1BDSURkqoi8IyLLReS6Avf7i8iDIvKmiLwkIoeV2zefwOw8yApcmLw3P8NS47V1RTWNbr8JMhuwsqJ7rFfrIxAb7vnSDlVFU4vJ7Po1mV23oqnFntoPEiISBW7Bqj41HpgpIvnp7RuA11V1IlZd0Z9X0LcTgatSNWbwAFZs3NIhbiNHDfJ5RKXxo/pVtpq80xWuLK/tnUBWjm9LLaMhXrCA0T5Sb9rHjeevXUujrU8hzV9waXSF0T23Q+vTZI9Q0ra/oo3nEGm+pHTHgJDWpJNe/GRguaquBBCRe4HpwNs5bcYD/w6gqktFZJSIDAEOKqNvJwLjseUStvDUD8/NzSxp4ELScr023VnkRjtkSpahdBxNr7JFLSu0an3f+gSafs/TsfScRqKxcWW9gEHZdar2a1bew4YBuRt919rXcnkDqxgyIjIZOBCrYnw5fTsRSGHLEqbkgl/i5nRIGuTz2rqda4sdSuHCMY1Iwt36CmAdSa7JN9H0MjT5Cp2PH8+SLmtLWAjZlF2nar/m5N0vdBJBvmv9Q6C/iLwOfA14DesfsZy+nQhcKJpPVtyy4WmQQ1O/ijK7UXR5c9vqYIWksTGQXlGyiUT3QxvPgdYn2bfzoAGiwyBxgqvDy7Q+A7vnYGVgM/bXQn+PUZBCxW1qnrVAbjp6OLAut4Gq7gAuARDriOVV9qupu775BNpjyyUsyYUxzYMtgfPIc3NjCUhgvbbYmG69Nmm6BOl9NcSOgNg4aLoQ6ft9xEUx0fQqW9SyCYtWIL9gTA6JE10bS4B5GRgrIqNFJIG15vWR3AYi0s++B/Al4Dlb7Lrtm0/gPbZ8wpJcsMRtoyeeWzaZ4DSB89ps2lLLSEQGQ/oNlDjEJxGJWPtKRQQajkMajvNsPNr6JEULxKDsKz+Ygd5XItH6q62gqmkRuQJ4AsudvUNVF4vIbPv+rcChwJ0i0o6VGLi0VN9S9kInbBCe8NRLcQNnQ9JobBzt6XcceVZPUVVofQz2PmDVMoiNsqpbJRegbU9gzadZ5y5kYhOQ3tci0X5V2rTnwDIbIPYRaytXd4VnMtsofP5DHJovtb1FgfhRdX2aiKrOA+blXbs15/v5QMHUd6G+pQilsGUJg/fmlbi5tQTEV69t71z7+CF7viy9DHb8C6AkSZPI3V6VXoxu/xba90dIZjVE+iHR4RWZ0/aN6Pbr7U30aSBiiVvLTeyLkLoiDZPR1OtYIWguKSQ+qWqxNVROaObYihGGpSFezbk5vQTEz7k21TZozRG1DlJk566Smucl6WbY9kV0x7+j264ls+1atL302W2duu/6GehWrBN7U5bt9DJ0zwOlOyZOshIU+SeR9DrfiJpPhF7YsgR9aYiX4uZ0IsGXdW2ZrZSuVWDRWdza7dceoA3a30V3/bAsc5rZBel/0DWkTELb0yX7isSRvj+Api9AbALEj0X6XEekaUZZtg3OUzPCBsH33rzMljq9vs1zcZN+kO+RFaGL59ZBO6RXoe0flvGUUra6H4dIgkivqUT6fo9Iy3VI4sgybBrcoqaELUuQl4Z4IW5OLwHxIySVSCM0TqX8MnrFiJXYkZBrrwUKzsnFoMHdNXAG56lJYcsS1PDUS3FzEs+9tqaLoNcnQZqwwtLiE/hJBlI4F6YlzmbLo+FMOv9JNFjHiPcyIWXYqGlhg+CGp16FpU57bV6Km0gUaZoB/e+CAXNBehVp2QAtN5CkF/vET6zrHcstSpNp/T/Y83s6h53W2jTd8z9o+9qe/yAGz6l5YcsSRO/NbXGrhZAUrEW3IrEioSIgESsr2e+XJBMnk4wMh/hxSMu/lnX+mmo77P4dXTOwGci8D21/Rbddi6aWVv2zGLyhboQNgum9hU3cwMfTP5o+S9c5N/sgSYkjkT5I768iva8k1etTSPyQ8p6b2U7Jqla0A23o7vx93YagUlfCliVo3ptX4uYEfq5tk/h46HM9REdhbSYfCE0XQ69Pd25oH3NU9sm7kd6Us7SE9nct784QeOpS2CB43psXc25OhqR+eW2SOALp91Nk4H3IgNugcQqk30bTK60tWFkqEDeRBDSeSfcZ2AR1/CcTKur+fylI3pub4lZTIamNtv4Ftn4Bdv4AdtwI2y5H263TbDS9Cm19Ct39O1p3/4HW5JKSz5KmL9jilsDaZ51PAzSe0/2+UUMgqHthg2B5b2ERN7eypKoZNPU2mnwBzRT/v9D0Stj9G+socN0D2gqZ9bD9RnTPo7D9emh7CtJvWVuzdt1Ca7LoSdKIxIg0fwkZcCf0uwUSU7BO5Wi2viaORppKVpU0BIhQb4J3mo5TQ3zeVO/mxnknjzhy+gQQbf/Q2uSuOwABTaGN06DpC109pdbH6HpUkIJug7135F1uA90AyYW0Sbxk7QSRBiQ6BPpcibZfZGVFI0OR6EAnfkSDR1TtsYnICBH5i4gsEZHFInKVEwPzkyB4b1nPzQ3vzen9pI55bTv/HTIbQffaJ2ykoPUJSD7ftW3Ro4KK0WbtBbUPqyxr7i3aD4lPMKIWQpwIRdPA11X1UOA44PLuSmOFgSBsy3K7jkKQQlJt/wDa19NVrNps7yyP+DFUvN1KmqyvOYmFSmqWVoq2b0T3/hnd+79ou/8JqnqiamFT1Q9U9VX7+53AErqpIBMm/Pbe3BI3N+bbqkL3WgttC97b3fVa46kQHUr54tZg7z21iY1xVeAye/+MbrsC3fN767XtcjKtTzpqw1AcR5MHIjIKOAp4scC9WdnSXFu3bnbSrOv47b2FRdyq8tqiIyn865goWCNApAH6/hCaPgdS7FRasbdhxaHXpwqfuOGCwGn7B/b2rKT9arO+7r7deG4e4ZiwiUhv4H7garsAQydUdU62NFf//uGcs/DTe3Nb3Jyip+ImEoPmK7A8sOxyiwaIDIbGjxfp04j0+gS0/BtIY06/CJCAXhdB7yug/xyk6Z9LD6CAwPVY5NrmU7gMIJBc0LNn1gAiMlVE3hGR5SJyXYH73xCR1+3XIhFpF5EB9r13ReQt+97C7mw5khUVa5fx/cDdqtrNcaPhxs/MqZvl/Zw4UjybJe3pceLScBwa+xHsfcJKIsQnQeOpiDSW7hcbjfb9iVUbIb0cIsOh6XwkdlDlP0Ruceb0ig5x67YKfSeKJTWUooJX44hIFLgFOAurFN/LIvKIqnaswVHVHwE/stt/ArhGVXM/yaeoalleRdXCZtf/ux1Yoqr/WU6fVFuhij7hIrfeQtjFzcl6CVWLW3QE9P5SD/odYHlnTpIVuRyBgzJEruFY2PsnuoqYQOJYR4foJm2ZlJPZ88nAclVdCSAi9wLTsapRFWIm8IeeGnPCYzsR+Dzwll3BGeAGu6pMQRKJGGuWfdDxfsTY/R0Yhvf45b2FRdxqhiJeHBQWOYmOQHt92vIgO2qLRqHXTCQ61N2xOkhUGiqZqhiUFyLOyasGPwxYk/N+LVBQ5UWkCZgK5H5SKfCkiCjwmwKV5jtRtbCp6t8pawdxZ0aOtObZVq/eHHqR88N7C4O4bW57J5B1SasiV+Touhc1K3SRpgvQhuPR1vmAII3HV1w1K2RsUtWjS9wvpBFa4BrAJ4Dn88LQE1V1nYjsBzwlIktV9blixnzfeZAVOOgscmETOD+8NzfFzSmCWnTZMUp4cwANzRd4PKDAshYYkfN+OLCuSNsZ5IWhqrrO/rpBRB7ECm2LClug9oqOHDmwQ+jWLPug4xUmvM6cupEtdWpngh+n7vpKNrNaJMPq5mLgEPAyMFZERotVpHUG8Eh+IxHpC5wKPJxzrVlE+mS/B84GFpUy5rvHVoiwe3FeF3J2K1sahGRCqMkLW6H4MUqVZV3Dh6qmReQK4AmsdTl3qOpiEZlt389WhP8U8KRqp1XZQ4AH7f3CMeAeVX28lD3pdIaVRxw+YaI+cM+jFfVZvbrzot6wiNyKjZYn5UVoumK35WU5JW7ZkNSJyvLZZELdiVu5pFcUvdUT0YskDn+lmzmvbplw1BE695kC29kKcNiAYVXbc5JAemyFCKsX52ViwWnPLUjLQGqeAt4dUHDeLp9a9/Z6QmiELZewZVS9TCwYcasxigleDnU+d1eQQCUPKiWbbMhPOAQVrxILTicUwnBAZV2Tn7QoQwxrnVALWy5hETivNtQbcTPUMzUjbFlyvbggLxnxwnsz4maoV2pO2HIJuhcXZnFzAiNuBreoaWHLEuSFv16Epm6Im9PV5XsqbqqKH0uWDMGmLoQtS5CTDW57b7UmbprZi+66BbbMgC2fQbd/G21f031HQ11QV8KWSxAFrt7FLXsKb1kCt/PfoO1ZrBNqM5B+G7Zfj2a2OTIeQ7ipW2HLEjSBczs0dWNvqZMVr8rx3jS9AtIr6Vx+T0FT0PqUY2MxhJe6F7YshbKpfuKm9+akuLlRYb5bcWtfV6TwSxLSqxwbhyG8GGErQFCWixhxKyJu0eGghY7fTkBPjgM31BxG2EoQhDDVzdA0DOJWaN5NYqMhdjCQyGkdAUlA41mO2TeEFyNsZRAUgQPnvbegixsU8d5aboCGs+zyejGIHwl9b0YifR21bQgnRtgqwG+BM+JGh/cm0oj0/hIy4B5k4J+Qlu8g0WAegmDwHiNsPcBPgQuLuDm5FCRLNjQFs1vBUBojbFXgl8C5Ne8W5HVuueR7bwZDPkbYHMBPgQNnvTe3xM14bwYvcUTYuitdXy/4IXBhETdwft4NjPcWJrrTCRE5TUS2i8jr9uumcvvmU7Ww5ZSunwaMB2aKyPhqnxtm8gXObYy4Ge8t6FSgE39T1SPt13cr7NuBEx5bR+l6VU0C2dL1dY+XOxnqXdzAeG8BpxqdqLivEzUPyipdLyKzgFkAB+w/zAGz4WHkyIGeFKBxo7ZCkGsoFCIrbtnaCmAqY/WUtlS6kg+1QSKyMOf9HFWdk/O+LJ0AjheRN7CKKV+rqosr6NuBE8JWVul6+4ecA1b5PQfshopCBWjcFDgnK2OFTdygq8AZcauchkisw2svg03dlN8rRydeBQ5U1V0i8jHgIWBsmX074UQoWknp+rrHq/k3p0PTsIWlWSo+DsngFt3qhKruUNVd9vfzgLiIDCqnbz5OCFtZpesNnfFi/i0M4ubWcpB8zPyb73SrEyIyVOxy7yIyGUufNpfTN5+qhU1V00C2dP0SYK4dFxvKwO3lIUEXN/DeewMjcF5TTCdEZLaIzLabfQZYZM+x/QKYoRYVa4z4cV784RMm6gP3POq53aCzevVmwJ25txUbLSFyqljzit2WCDkx55Zle+pdANfm3fJpT7/T8X2tzcH1ajjilW7mvLplwsQjdO7/PllW28NGDq3anpOYnQcBwk3vzXhuXTEeXO1ihC1guJlcMOJWGCNwtYcRtoDiprhlN9A78jyXxM2rpEIuRuBqByNsAcbNzGnQxQ388d7ACFwtYIQtBLjpvRlxK44RuPBihC0kuJVYCJu4GYEzlIMRthDhVmIhLOLmp/cGRuDChBG2EFKv4gb+hqZZjMAFHyNsIcWIm7/iBvsELncvqhG5YGCELcTUu7j5sSSkGMaLCxZG2EKOW0tCwiBuEBzvLYsRuGBghK1GcNJ7C8MOhVz8zpoWwoSp/mKErYaod3ELmveWxXhx3mOErcaoZ3GD4IWmuRiB8w4jbDWIEbdgJRbyMWGq+/gibMm9SVYveb/gy+AM9S5uEGzvLYvx4tzBiWIuFRNvjDNi7NAu19csW9+tuI08tL4qXFVDbnWsag+vDHqBmGLkFo4B7w6xrJSsuAFsbqvdAzC9whdhK0YhscunlPB5JXqqsGbpWjat28rwg/dnvxHOnEpbiE3vb2H122vpN6Qvow8/EClUr6cERtz2eW5uV8ZyClNCsHoCJWzlUEz8Snl7Tgre7h17+NUVt7N+1QYkGqE9lebwU8bzxe/NJBJ1LrLPZJS7vvsnXnnqDaKxGKpK/6F9ufrXs2gZ2KeiZ7khbk7hlbiBN2X/nKTWvDgRmQr8HIgCt6nqD/Pufw74lv12F/AVVX3DvvcusBNoB9LdHUMeOmErRiWCV43Q3fXd+3h/2Qek0+0d1xb9bQlP//45zv7CaT1+bj5/f+BFXv2/t0gl06SSaQA2rt7E7TfcwzW/uaxgH1V47k8v8PRdz7F72x5GTxzJ+Vedy7CxQ50XN4e8NvBe3ABW7nwXCG5omk/YvTgRiQK3AGdhldN7WUQeUdW3c5qtAk5V1a0iMg2rDnFuYeQpqlrWRG/NZ0VHjB3a5dXTpEWyNcXi59/pJGoAybYUz943n0xGO0SoUN9tG3eQyWTKsvXXuS+QbE12utbenmHVW6vZuXVXwT4P/eoxHvrlY2z+YCute9tY8uIyfnLpf7FhtV0Z3k4oOIVTyQTwLqGQJQyJhUKEOKM6GViuqitVNQncC0zPbaCqL6jqVvvtAqz6oT2iZjy2Ssj37vK9umIeXTqZplgB6p1bd/H1U28i1ZZi4PCB/PM3pjP++INJJdPMvfkhXnrsNRChoVeCz3z9PCZPPbLkGJN7kwWvRyJCqrWreO7d1cpf7/17F2FNtqV4/LfPcNG/XGD9bCMHsjqA823grecG4fXeshTy4sA5Ty6ZTFfy4TVIRBbmvJ+jqnNy3g8D1uS8X0tnbyyfS4HHct4r8KSIKPCbvGd3oSphE5EfAZ8AksAK4BJV3VbNM/0gX+iKiVxTSy8GjRjI+lUbujwjk86Qztgh45pNzPnmnVx962U8+6f5vPr0mx2Ck2pLcc/376NlQG8OmfyRomM64rQJ/O2+BaTTnYWquV8z/Yf269J+49rNROOxLsKWyWR4d/HaLu2DmEwA78UNwjf3lk/uXFy+yFVDQyzWsdSnDDZ1M+9VKO1V0EsQkSlYwnZSzuUTVXWdiOwHPCUiS1X1uWLGqg1FnwIOU9WJwD+A66t8XiAoFrYCXPidf6KhVwOxWBSAeML6bMgPMZOtKR7+1eO88uQbpNpSXe49fsczJccw7dLTaRnUh0RjAoBYLEZDY4KL//WCgpnR/kP62h5lZwQYOqrzH2uQ17iB92EpBHO/aU/IXRcXMNYCI3LeDwfW5TcSkYnAbcB0Vd2cva6q6+yvG4AHsULbolQlbKr6pF2lGaqMiYNKvsjFYlEu/cFnmTLzZCaeMp6TP30cjb0aCvZd9upKorYA5rN5Xek/2t79mvn2H6/hk1+bxpGnHcZpM0/kxj9ew8FHjynYvk//3hx1xuHEG+Kdrscb45zzhSld2rshbk7il7iFde4tBLwMjBWR0SKSAGYAj+Q2EJGRwAPA51X1HznXm0WkT/Z74GxgUSljTs6xfRH4Y7GbIjILmAVwwNADHDTrHdmQdQ3w0UYfw4gAABcDSURBVLMmArDfgYN47r4FRXoo7XmJBoCICKMPP7Bbe41NDZx2wQmcdsEJZY3vwu98huY+vXj+kZdpT7XTd3ALn7vhfA4cX/jzJsiZUvAnLIXwLOoNE6qaFpErgCewlnvcoaqLRWS2ff9W4CZgIPBfYoUl2WUdQ4AH7Wsx4B5VfbyUvW6FTUSeBgqtpbhRVR+229wIpIG7S/xgc7DStxw2/vDCM/AhIXdObs2y9Yw8dDgr33y3S7tMRhkxbn8+WPVhR4ZTEBKNCT4+60zHxxVPxBg6ej9EhERjnJ1bdvHE7/7KgRNG0Ny3qWCfrLg5hZPzbeCvuEF4FvWGAVWdB8zLu3ZrzvdfAr5UoN9K4IhKbHUbiqrqmap6WIFXVtQuBs4FPqeqoRasnjBi7FA+fumUgiFnPBHjqNMP46Kb/onhY/enT//eTDx1PN/4n8sZcqDzfyhLX1rOA7+YR7I1SeueNtKpNKvefI///ubvu+0b1Pk28CcszVIrc2/1RrVZ0alYK4VPVdU9zgwpfIw7egwDBrewed0WMjkz+9FYjOPOO5qWAb2ZZIeubvL075/tsvYtnW5n1aLVbFm/jQEFsqkQ7G1XHc/1yXMD472FkWqzor8C+mClX18XkVu761CLSCTCNXMuY+ykg4gKRAUGDRvIjG99kpYBvT0bx7aNOwpej8Zj7NxSeFFvlqAnE8Bfzw2M9xYmqvLYVLX4Qqw6o++gFq78ry+zd3cr7al2evdr7rTw14sN+uOPP5gN723qsvZNM8r+Y4Z029/J+TY3kgngr+cGxnsLCzW/pcprejU30rtfM7BvqQiUPpXEKc78/Kn06tNILLbv86qhMcEnr5hKIm8ZSCmCWBQmF789NzDeW9CpW2HbtG4zz/1pPvMfXcjenXtdtZUrbsnWFM/OfYFffPU2br/hHla8/q5jdloG9ObGe6/h1H8+ngMOGsqhx45l1o8v4tQyl4tA8Bfvdjw7IOJm1r0Fk7rcK/q/c57iyd/9FUSIRIQ/3vwwX/6PC5lwgnsrtkeMHUqyNcn3Zv6UbRu2k06mEeCtv73N9MunMWXGiY7YaRnQm09ffS5c3fNnOB6SOnjMUadn+xyWZgn7ntNapO48tlVvvcdTv3+OdDJNui1Fcm+SVGuS2667i7Y9bSX7bly7mWWvrmD3jt09sj3/0YVsX7+1Y+uTYm2veuhXj7F3d2uPnukmToWkThZh7vLsAHhuWYz3FhzqTthenPdql72bACLC2/PfKdAD9u7ay89m/4bvz/gpt379Tm6Y9gMe+tVjVLps742/LCbVmoK83QjRWJRVbwbr6Bk3qszXi7gFuZBMvVB3wtaeardOZCxAOl34rLTf/ctcVr35Hqm2FK27Wkkn0zw79wVeeuzVimw392vad8ZBrrhllKaWXhU9ywucPL/Nzfk2CJa4gUku+E3dCdtHzz6CRK9El+vt6XbGHz+2y/U9O/awZP47pFN5h0vuTfL0XX+ryPYhk8d2PqjFFrdoIsaB40cU7hQAnAxJ3SSI4mbCU3+oO2Ebd8xHmHTG4Za4CURiEWINcf75W9Npbmnu0r51dxtSpJbBnm2VzbW9NO+1rhfT7UhEWL2k65lpQcDpU3fdnG+D4IkbGO/ND+ouKyoiXHjTP3HCJyfz5rOLSTQmmDztKAYXqTTVf0hfGpsarLmx3OdEIhxy3MEV2V69tLB47dmyk1RbmtVL3g9keUGnTt3NxektV7kEJVuai1nY6y11J2xgiduYI0Yx5ohR3beNRPjsDedzx7fvtZIOqsTiURqaGjj3srMqstunf2827+3qScQSMSJkeO7++UTjMQYPG8CSF5eRaIhz4qcmM+GEQyouu+cGTuwlBff2k3ayEUBxA3MkklfUpbBVysRTJ/D1/57N03c9x6a1mxn70YM4/bMn0TKwpaLnnHXxaTzwsz93qmcQjUdp6t3ITy/7jZXYyDslZOmLyzjp/GP59DXnOvKz9BSnjzdyc31bh40AixsY781NjLCVyYhDhnHJ92ZW9YyTPjWZnZt38uSdzyJirWHLpDOdN6+nO4tbW2uS5+6bz6kXnMCgYf7/cTrltWVx02uD4IobmIW9blJ3yQO/UFX+/sAC5j+6EFTt89u08Fq4vHVuEonwzkvLvRloCdxIJIB7S0A67AQwoZCLSS44jxE2j3jstv/jgZ/NY8sHW0m1paz9qWWu741EhF4BWec2cuRARxfteiluY5oHB1rczNIQ5zDC5gHJ1iRP3tn1EMiS5HhtEhEOO+kQF0bWc9wQNy8IsrhBbXtvIjJVRN4RkeUicl2B+4eIyHwRaRORayvpm48RNg/Y+uE2IhVmNeONcRoTMZr69OLKW75c0bFDbuN0SJrFba8tSxjErda8NxGJArcA04DxwEwRGZ/XbAtwJfDjHvTthEkeeEDLoBba2wtv18olEo0wZeZJTJlxIh++t5F4IkasqQEJwlqPAjiZSPBiCUgne82DWfHhxsAlFHKpsaUhk4HldmEWROReYDrwdraBXTN0g4h8vNK++RhhcwDNZNi8fiuNTfsOmcylV3Mjx593NM8/9BKZIvtRI7EI333wm/Qf2h+A/kP21SfInsQbpMW7Ti//AG+WgHSyFxJxA3+WhqTaUpVMOQwSkYU57+fYlemyDMOqXJllLXBsmc+uuK8RtipZ9PxS7v7efezd1Yq2Z/jIpIO45HszuwjcBdeex+K/L2XL+m0FnzPmiFEdopbPiLFDWbNsveNjdwKnl3+A+0tAcgmDuIE/S0MSiVgl0w6b7BqgxSgUdpR7PE7Ffc0cWxV8sPJDbrvuLnZs2kmqNUU61c6yV1Zwy5V3dGkbiUYZd2zXTfZZylmj5sXx4pXgxlybV1nSTjYDPueWS4iTC2uB3JMehgPr3OprhK0AmsnwwsMv8YPP/oybpv8H9//0z+wqsOH9mT/8nXSy85qz9nSG9as2sG55Vxf+jM+eTKG9URKJcML0yZ2ubdu4nZ1bdna8zy3SHDSczJCCt1nSDpshE7cQJhdeBsaKyGgRSQAzgEfc6mtC0QLc+x8P8dJjr3VsfXr2Ty/w2jNvceO919CrubGj3aa1m9FM1zmzSCzC1g07OOAjnUO0/Q8awmkzTuBvf5pPuz3XFotHOfLMiYw+fCQAa/+xjt9++w9sen8LqHLAR4byxe9/tmOTfj3MtYF7Va5K2gxJWJolTMkFVU2LyBXAE0AUuENVF4vIbPv+rSIyFFgItAAZEbkaGK+qOwr1LWXPEWGz15z8CBisqt7FEC6w9cNtLPjzKx3Hd4N1OOWubbtZ8OhCpsw4qeP6uGPGsPLN9zq1BUi3pRkxrvC80z/9v/M48tQJlo10hmOmHokI3PP9+4lEhZcfe522nL2kq5eu4ydfupXv/fm6gnNtmUyGSMR/x9uNuTbwdr4NwiluEI59p6o6D5iXd+3WnO/XY4WZZfUtRdXCJiIjgLOAYJ1t3UPeXbyWWDzaRaxSrSmWvrS8k7Cd/Jnj+Ovc+ezevrsj25noleCE844uuUF+7EfHMPajY1BVbr/+Hha/sNTyDoWuU6KqJFuTvPXc2xx1hlVNfvWS91m7bB2P/vpJtm/aQb/BLXziK+dw/CdKzd26h6tem4dZ0g67IRM36JpcqHec+Kj/KfBNys9wBJp++7WQyXT9USLRCIMO6Jy1bG5p5oa7r+TET06m35C+HDB2KBdcO53PfP28smwtmf+PfaIGRf8F06k0Wz/cDlhzbW8+9zZzf/QI2zdZm+e3bdzBH29+mBfnVXZUudM4PdcG7h9MWdRuiObccskKXL1TlccmIucB76vqG90tIhWRWcAsgAOGHlCNWVcZNWEEA4b2Y8PqTWRyFtVG49GC9TlbBrYw41ufYsa3PlWxrdf+sqjTEUbFiMaiHDhhX1Lob/cv6LI9K9ma5NFfP8mxH5tU8TicwC2vLYvXISmE03MDI25QhscmIk+LyKICr+nAjcBN5RhS1TmqerSqHt2/f3B/UUSEK//ry4yeeCCxeIxEY4KWQX2YdfPn2W+ks/MXiV7xbncVxBviHDhhOAdNPBCwTgnZuWl7wbZbPyy8Rs4rnN4gn8WPJSAdtkPqudU73XpsqnpmoesicjgwGsh6a8OBV0Vksj0JGFr6Dmrh/82ZzY7NO2jbk2TQsAGICxP0x537UZ5/4KUu5QAjsQh9B7UQjUU4/rxjOONzp3QIoIgwYGh/tuSe4WYz8IDgfmBUi1/zbRBez62e6XEoqqpvAftl34vIu8DRYc+K5tIysAXc2e8NwIiDhzH98nN46JePE41FEBFUlct+cjHjjvlI0X7nXTGVu77/QKcER6IxzievmObeYCvArQwp+BOSghG3sGHWsfnMlJknc/Q5R7JkwTLijXHGH38wDb0aSvY55uwjiYpw/y1PsHPTTgYeMIDpV0zlqNMP82jUxXFzrs3rjfJd7BtxCw2OCZuqjnLqWfVGnwF9mFzhpP+ks45g8KghgVqsm4tbXpufISnsEzfACFyA8X9lp6HHjBg7NHD7R8G989py8SORkCXoR40bjLB1y47NO7jt+ru56oQbuOqEG/ntjfewc+suv4cVCtzIkIK/WdKOMRhxCzRG2EqQTqW5+ZJbeOMvi0in2kmn0rz6zFv8+Iu3kGlv7/4BdYzbXpsfG+W7jMGIW2AxwlaCN/6ymD3b9nRaqJtJZ9i5ZRdv/W2pjyPbR1DD0SxueW1Z/PTawIhbUDHCVoL1727otCE9S7I1zfpVG3wYUbjwymsz4mbIxwhbCYaM2o+GXoku1xONMYaMCu4pCkHDTa8tCCEpGHELGkbYSnDEaRNo6ttEJLrvnykSjdCnX28mnnKojyPrTJDDUS8ypOC/1wZG3IKEEbYSxBMxvvHby5l4yniisQjReJQjphzGtb+9nEg06vfwQoUXXpsRN0MWs/OgG/oOauHLN38eVetMoaCWwgsybp/8Af4v3M1lTPNgVuzeyIoPt5hFvD5hPLYyEZHAi1pQw1EvCYLXBsZz8xsjbDVCkIu9gHtHGuUSpJAUjLjlIyJTReQdEVkuItcVuC8i8gv7/psiMinn3h0iskFEFpVjywiboaYISpY0ixE3CxGJArcA04DxwEwRGZ/XbBow1n7NAn6dc+9/gKnl2jPCZvAUt722LEHx2sCIm81kYLmqrlTVJHAvMD2vzXTgTrVYAPQTkf0BVPU5oOx/QJM8MHiGF0kE8P94o0KEMaGQ3Jt0ct52GLAm5/1a4Ngy2gwDKv40NMJWQ2TXswX1KKMsbh5EmSVIWdIsYRO3eGO8krnbQSKyMOf9HFWdk/O+UOYtv3xROW3KwoSiBk/xasFuliCFpFDTYemmbE0T+zUn7/5aYETO++HAuh60KQsjbAZf8GKuLWhZ0iw1LG6leBkYKyKjRSQBzAAeyWvzCHCRnR09Dtiuqj36RTHCZvAcL722oGVJs9SbuKlqGrgCeAJYAsxV1cUiMltEZtvN5gErgeXAfwNfzfYXkT8A84FxIrJWRC4tZc/MseWxZME/ePAX8/jwvY30G9KXcy87m2POOdLvYVVEGObZvCZIiYQsYZtzqxZVnYclXrnXbs35XoHLi/SdWYkt47HlsOTFf/Cba+/k/WUfkE6m2bRmM3d/735eePglv4dWNkFfqJvFiwW7WYLqtUH9eW5eYYQth4d++ViXGp+p1iQP3/JEx15RQzgZM3hA4Obashhxcx4jbDl8+F7hX/w9O/aQLHDgpKF6vPLashhxqw+qFjYR+Zq9/2uxiNzsxKD8YuD+/Qpeb2hqINEY93g0tY/XSz+CHJKCETcnqUrYRGQK1jaIiao6AfixI6PyiXNnn0M8T8ASvRKcc8kUJGKcW7fw0msLckgKRtycotq/1q8AP1TVNgBVDXUhgKNOP4zP3nA+fQe3ICI09enFxy87kzMvPMXvoVVEkE/Uzcdrry2LEbfaptrlHgcDJ4vI94FW4FpVfblQQxGZhbVjnwOGHlClWfeYPG0Sx0w9inQqTSweC/wZbIbKCeJ2q3zqbSmI03TrsYnI0yKyqMBrOpYw9geOA74BzJUiSqCqc7LbLfr3D/Z/lIgQT8SNqHmEl0s/sgQ9JAXjuVVDt8Kmqmeq6mEFXg9j7e16wD5m5CUgAwRrFaTBUAIjbrVJtXNsDwGnA4jIwUACCPZviiGw+OG1hQEjbpVTrbDdARxkH9d7L3CxmpWshh7gVxIBgu+1gRG3SqlK2FQ1qaoX2qHpJFV9xqmBGaojTJlRPwnqCSCFMOJWPmZxliEw+JFEgPCEpGDErVyMsBkMNmHw2sCIWzkYYTMEDj+9NiNutYERNkOg8DOJEKaQFIy4lcIIm8GQR1i8NjDiVgwjbIbA4VcSAcLntYERt0IYYatxzJKPygnDdqt8jLh1xghbDROWY8KL4ZfXlsWIm7OIyFT77MblInJdgfsiIr+w778pIpPK7ZuPETZDIPEziQDhDElhn7gFDRGJArcA04DxwEwRGZ/XbBow1n7NAn5dQd9OGGEzGEoQNq8NAituk4HlqrpSVZNYWzCn57WZDtxpH6qxAOgnIvuX2bcTvpTfW7xk0aZxR495r4fdB+HvRns/7Rvbxna5HFit8cVLFj0x7ugx5Z7W0ygiC3Pez8mrBj8MWJPzfi1wbN4zCrUZVmbfTvgibKra448UEVmoqkc7OZ6w2De2jW0vUdWpDj6u0OGG+QdmFGtTTt9OmILJBoPBC9YCI3LeDwfWldkmUUbfTpg5NoPB4AUvA2NFZLSIJIAZwCN5bR4BLrKzo8cB21X1gzL7diKMHtuc7pvUrH1j29gOJaqaFpErgCeAKHCHqi4Wkdn2/VuBecDHgOXAHuCSUn1L2RNzLqTBYKg1TChqMBhqDiNsBoOh5gitsInI1+wtFotF5GYf7F8rIioinlXlEpEfichSe7vJgyLSzwObFW1lcdDuCBH5i4gssf+Pr/LKds4YoiLymoj82WO7/UTkPvv/eomIHO+l/VoglMImIlOwVh5PVNUJwI89tj8COAtY7aVd4CngMFWdCPwDuN5NYz3ZyuIgaeDrqnooVt3ayz20neUqYInHNgF+DjyuqocAR/g0hlATSmEDvgL8UFXbAFR1g8f2fwp8k24WCTqNqj6pqmn77QKs9TxuUvFWFqdQ1Q9U9VX7+51Yf9zDvLANICLDgY8Dt3ll07bbApwC3A4dBZO2eTmGWiCswnYwcLKIvCgiz4rIMV4ZFpHzgPdV9Q2vbBbhi8BjLtsotsXFU0RkFHAU8KKHZn+G9eGV8dAmwEHARuC3dhh8m4g0ezyG0BPYdWwi8jRQ6NydG7HG3R8rRDkGmCsiBzlV07Qb2zcAZzthp1Lbqvqw3eZGrFDtbrfGkR1OgWueeqki0hu4H7haVXd4ZPNcYIOqviIip3lhM4cYMAn4mqq+KCI/B64DvuPxOEJNYIVNVc8sdk9EvgI8YAvZSyKSwdowvNFN2yJyODAaeENEwAoFXxWRyaq63k3bOWO4GDgXOMOD4tTlbINxDRGJY4na3ar6gFd2gROB80TkY0Aj0CIid6nqhR7YXgusVdWsd3oflrAZKiCsoehDwOkAInIw1l4y109gUNW3VHU/VR2lqqOwfgknOSVq3SEiU4FvAeep6h4PTFa8lcUpxPrkuB1Yoqr/6YXNLKp6vaoOt/+PZwDPeCRq2L9La0RknH3pDOBtL2zXEoH12LrhDuAOEVkEJIGLPfBegsCvgAbgKdtjXKCqs90y1pOtLA5yIvB54C0Red2+doOqzvPIvp98Dbjb/jBZib21yFA+ZkuVwWCoOcIaihoMBkNRjLAZDIaawwibwWCoOYywGQyGmsMIm8FgqDmMsBkMhprDCJvBYKg5/j82wl+74+JSTwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot\n",
    "vlr = VariationalLogisticRegression()\n",
    "vlr.fit(X_train, y_train)\n",
    "y = vlr.proba(X).reshape(100, 100)\n",
    "\n",
    "plt.scatter(x_train[:, 0], x_train[:, 1], c=y_train)\n",
    "plt.contourf(x0, x1, y, np.array([0., 0.01, 0.1, 0.25, 0.5, 0.75, 0.9, 0.99, 1.]), alpha=0.2)\n",
    "plt.colorbar()\n",
    "plt.xlim(-7, 7)\n",
    "plt.ylim(-7, 7)\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 10.6.1 变分后验概率分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "这里，我们会使用一种基于10.5节介绍的局部界限的变分方法。这使得logistic回归的似然函数(由logistic sigmoid函数控制)可以有指数的二次形式近似。因此，与之前一样，比较方便的做法是选择形式为(4.140)的共轭高斯先验。现阶段，我们会将超参数$ m_0 $和$ S_0 $看成固定的常数。在10.6.3节，我们会展示变分形式如何扩展到超参数未知的情形，这种情况下，超参数的值要从数据中进行推断。\n",
    "在变分的框架上，我们寻找边缘似然函数的下界的最大值。对于贝叶斯logistic回归模型，边缘似然函数的形式为\n",
    "$ p(t) = \\int p(t|w)p(w)dw = \\int\\left[\\prod\\limits_{n=1}^Np(t_n|w)\\right]p(w)dw \\tag{10.147} $\n",
    "首先，我们注意到$ t $的条件概率分布可以写成\n",
    "$ \\begin{eqnarray} p(t|w) &=& \\sigma(a)^t{1 - \\sigma(a)}^{1-t} \\ &=& \\left(\\frac{1}{1 + e^{-a}}\\right)^t\\left(1 - \\frac{1}{1 + e^{-a}}\\right)^{1-t} \\ &=& e^{at}\\frac{e^{-a}}{1+e^{-a}} = e^{at}\\sigma(-a) \\tag{10.148} \\end{eqnarray} $\n",
    "其中$ a = w^T\\phi $。为了得到$ p(t) $的下界，我们使用式（10.144）给出的logistic sigmoid函数的变分下界。为了方便，我们在这里重新写一下。\n",
    "$ \\sigma(z) \\geq \\sigma(\\xi)exp\\left{\\frac{z-\\xi}{2} - \\lambda(\\xi)(z^2 - \\xi^2)\\right} \\tag{10.149} $\n",
    "其中\n",
    "$ \\lambda(\\xi) = \\frac{1}{2\\xi}\\left[\\sigma(\\xi) - \\frac{1}{2}\\right] \\tag{10.150} $\n",
    "于是，得到\n",
    "$ p(t|w) = e^{at}\\sigma(-a) \\geq e^{at}\\sigma(\\xi)exp\\left{-\\frac{a+\\xi}{2} - \\lambda(\\xi)(a^2 - \\xi^2)\\right} \\tag{10.151} $\n",
    "注意，由于这个下界分别作用于似然函数的每一项，因此存在一个变分参数$ \\xi_n $，对应于训练集的每个观测$ (\\phi_n, t_n) $。使用$ a = w^T\\phi $，乘以先验概率分布，我们可以得到下面的$ t $和$ w $的联合概率分布。\n",
    "$ p(t,w) = p(t|w)p(w) \\geq h(w,\\xi)p(w) \\tag{10.152} $\n",
    "其中，$ \\xi $表示变分参数的集合$ {\\xi_n} $，并且\n",
    "$ \\begin{eqnarray} h(w,\\xi) &=& \\prod\\limits_{n=1}^N\\sigma(\\xi_n)exp{w^T\\phi_nt_n - (w^T\\phi_n + \\xi_n) / 2 \\ & & -\\lambda(\\xi_n)([w^T\\phi_n]^2 - \\xi_n^2)} \\tag{10.153} \\end{eqnarray} $\n",
    "精确计算这个后验概率分布需要对不等式的左侧进行标准化。由于这是无法计算的，因此我们反过来对右侧进行操作。注意，右侧的函数不能看成一个概率密度，因为它没有被标准化。但是，一旦它被标准化，来表示一个后验概率分布$ q(w) $，它就不再表示下界了。\n",
    "由于对数函数是单调递增的函数，因此不等式$ A \\geq B $表示$ \\ln A \\geq \\ln B $。这给出了$ t $和$ w $之间的联合概率分布的对数的下界，形式为\n",
    "$ \\begin{eqnarray} \\ln{p(t|w)p(w)} \\geq & & \\ln p(w) + \\sum\\limits_{n=1}^N{\\ln\\sigma(\\xi_n) + w^T\\phi_nt_n \\ & & -(w^T\\phi_n + \\xi_n)/2 - \\lambda(\\xi_n)([w^T\\phi_n]^2 - \\xi_n^2(} \\tag{10.154} \\end{eqnarray} $\n",
    "代入先验概率分布$ p(w) $，不等式的右侧变成了一个关于$ w $的函数，形式为\n",
    "$ \\begin{eqnarray} -\\frac{1}{2}(w - m_0)^TS_0^{-1}(w - m_0) \\\n",
    "•\t\\sum\\limits_{n=1}^N{w^T\\phi_n(t_n - 1/2) - \\lambda(\\xi_n)w^T(\\phi_n\\phi_n^T)w} + const \\tag{10.155} \\end{eqnarray} $\n",
    "这是$ w $的一个二次函数，因此我们可以通过分裂出$ w $的线性项和二次项，得到后验概率分布的对应的变分近似，这是一个高斯变分后验概率，形式为\n",
    "$ q(w) = \\mathcal{N}(w|m_N,S_N) \\tag{10.156} $\n",
    "其中\n",
    "$ \\begin{eqnarray} m_N = S_N\\left(S_0^{-1}m_0 + \\sum\\limits_{n=1}^N\\left(t_n + \\frac{1}{2}\\right)\\phi_n\\right) \\tag{10.157} \\ S_N^{-1} = S_0^{-1} + 2\\sum\\limits_{n=1}^N\\lambda(\\xi_n)\\phi_n\\phi_n^T \\tag{10.158} \\end{eqnarray} $\n",
    "与拉格朗日框架一样，我们又一次得到了对后验概率分布的一个高斯近似。然后，变分参数$ {\\xi_n} $提供的额外的灵活性使得这个近似的精度更高（Jaakkola and Jordan, 2000）。\n",
    "这里，我们考虑了一个批量学习的问题，其中所有的训练数据能够一次全部得到。然而，贝叶斯方法本质上相当适用于顺序学习的问题，其中数据点每次只处理一个，然后被丢弃。得到顺序学习情形下的变分方法的公式是很容易的。\n",
    "注意，式（10.149）给出的下界只适用于二分类问题，因此这个方法不能直接推广到$ K > 2 $个类别的多类问题。Gibbs(1997)研究了多分类问题的另一种下界的形式。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 10.6.2 最优化变分参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "我们现在得到了后验概率分布的一个归一化的高斯近似。我们稍后会使用这个近似得到对于新数据的预测分布。然而，首先我们需要通过最大化边缘似然函数的下界，确定变分参数$ {\\xi_n} $。\n",
    "为了完成这一点，我们首先将不等式（10.152）代回到边缘似然函数，可得\n",
    "$ \\ln p(t) = \\ln \\int p(t|w)p(w)dw \\geq \\ln\\int h(w,\\xi)p(w)dw = L(\\xi) \\tag{10.159} $\n",
    "与3.5节的线性回归模型的超参数$ \\alpha $的最优化一样，有两种方法确定$ \\xi_n $。在第一种方法中，我们看到函数$ L(\\xi) $由$ w $上的积分定义，因此我们可以将$ w $看成一个潜在变量，然后使用EM算法。在第二种方法中，我们解析的对$ w $积分，然后直接关于$ \\xi $进行最大化。让我们首先考虑EM方法。\n",
    "在EM算法中，首先选择参数$ {\\xi_n} $的某个初始值，我们将这些初始值聚集在一起，记作$ {\\xi}^{old} $。然后在EM算法的E步骤中，我们使用这些参数值找到$ w $上的后验概率分布，它由式（10.156）给出。之后在M步骤中，我们最大化完整数据似然函数的期望，形式为\n",
    "$ Q(\\xi,\\xi^{old}) = \\mathbb{E}[\\ln{h(w,\\xi)p(w)}] \\tag{10.160} $\n",
    "其中期望是关于使用$ \\xi^{old} $得到的后验概率分布$ q(w) $进行计算的。注意，$ p(w) $不依赖于$ \\xi $，代入$ h(w, \\xi) $，我们有\n",
    "$ Q(\\xi,\\xi^{old}) = \\sum\\limits_{n=1}^N\\left{\\ln\\sigma(\\xi_n) - \\frac{\\xi_n}{2} - \\lambda(\\xi_n)(\\phi^T\\mathbb{E}[ww^T]\\phi_n - \\xi_n^2)\\right} + const \\tag{10.161} $\n",
    "其中，“常数”表示与$ \\xi $无关的项。我们现在令关于$ \\xi_n $的导数等于0。经过简单的代数推导，使用$ \\sigma(\\xi) $和$ \\lambda(\\xi) $，有\n",
    "$ 0 = \\lambda'(\\xi_n)(\\phi_n^T\\mathbb{E}[ww^T]\\phi_n - \\xi_n^2) \\tag{10.162} $\n",
    "现在，我们注意到，对于$ \\xi \\geq 0,\\lambda'(\\xi) $是$ \\xi $的一个单调函数，并且由于界限在$ \\xi = 0 $两侧的对称性，我们可以将我们的注意力限制在$ \\xi $的非负部分而不失一般性。因此，$ \\lambda'(\\xi) \\neq 0 $，从而我们得到了下面的重估计方程\n",
    "$ (\\xi^{new})^2 = \\phi_n^T\\mathbb{E}[ww^T]\\phi_n = \\phi_n^T(S_N + m_Nm_N^T)\\phi_n \\tag{10.163} $\n",
    "推导过程中我们使用了式（10.156）。\n",
    "让我们总结一下寻找变分后验概率分布的EM算法。首先，我们初始化变分参数$ \\xi^{old} $。在E步骤中，我们计算由式（10.156）给出的$ w $上的后验概率分布，其中均值和协方差分别由式（10.157）和式（10.158）定义。在M步骤中，我们使用这个变分后验概率，计算由式（10.163）给出的一个新的$ \\xi $值。不断重复E步骤和M步骤，直到满足一个适当的收敛准则，这在实际应用中通常只需要几步迭代。\n",
    "我们介绍另一种得到$ \\xi $的重估计方程的方法。我们注意到，在下界$ L(\\xi) $的定义（10.159）中的关于$ w $的积分中，被积函数的形式类似于高斯分布，因此积分可以解析地计算。计算出这个积分之后，我们可以关于$ \\xi_n $进行求导。可以证明，这种方法得到的重估计方程与之前用EM方法得到的方程（10.163）完全相同。\n",
    "正如我们已经强调过的那样，在变分方法的应用中，能够计算出由式（10.159）给出的下界$ L(\\xi) $是很有用的。我们注意到$ p(w) $是一个高斯分布，$ h(w, \\xi) $是$ w $的二次函数的指数形式，从而我们可以解析地计算$ w $上的积分。因此，通过配平方的方法，然后使用高斯分布的标准化系数的标准结果，我们可以得到解的精确形式如下\n",
    "$ \\begin{eqnarray} L(\\xi) &=& \\frac{1}{2}\\ln\\frac{|S_N|}{|S_0|} + \\frac{1}{2}m_N^TS_N^{-1}m_N - \\frac{1}{2}m_0^TS_0^{-1}m_0 \\ & & +\\sum\\limits_{n=1}^N\\left{\\ln\\sigma(\\xi_n) - \\frac{1}{2}\\xi_n + \\lambda(\\xi_n)\\xi_n^2\\right} \\tag{10.164} \\end{eqnarray} $\n",
    "变分框架也可以应用于数据顺序到达的情形（Jaakkla and Jordan, 2000）。在这种情况下，我们保持$ w $上的一个高斯后验概率分布，它使用先验概率分布$ p(w) $进行初始化。随着每个数据点的到达，使用界限（10.151），然后标准化，我们就可以对后验概率进行更新，得到一个更新后的后验概率分布。\n",
    "通过对后验概率分布进行积分，我们可以得到预测分布，它的形式与4.5.2节讨论的拉普拉斯近似的形式相同。图10.13给出了人工生成数据集的变分预测分布。\n",
    " \n",
    "图 10.13 logistic回归的贝叶斯方法的例子。数据集是一个简单的线性可分的数据集。左图给出了使用变分推断的方法得到的预测分布。我们看到决策边界大致位于数据点的聚类的中间位置，并且预测分布的轮廓线在远离数据点的位置发生分叉，这反映出了在这些区域进行分类的不确定性。右图给出了对应于从后验概率分布$ p(w|t) $中抽取的参数$ w $的五个样本点的决策边界。\n",
    "这个例子为7.1节讨论的“大边缘”的概念提供了一些有趣的认识。“大边缘”的概念与贝叶斯的解有着定性的相似的行为。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 10.6.3 超参数的推断"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "目前为止，我们将先验概率分布的超参数$ \\alpha $看成一个已知参数。我们现在将贝叶斯logistic回归模型进行推广，使得这个参数的值可以从数据集中推断出来。这可以通过将全局变分近似和局部变分近似结合到一个框架中的方式完成，从而在每个阶段都保留边缘似然函数的下界。Bishop and Svenen(2003)在研究专家模型的层次混合的贝叶斯方法中，采用了这样一种组合的方法。\n",
    "特别的，我们再次考虑一个简单的各向同性的高斯先验概率分布，形式为\n",
    "$ p(w|\\alpha) = \\mathcal{N}(w|0,\\alpha^{-1}I) \\tag{10.165} $\n",
    "我们的分析可以推广到更一般的高斯先验分布中，例如，如果我们希望为参数$ w_j $的不同子集关联一个不同的超参数，那么我们就可以将我们的分析进行推广。与之前一样，我们考虑$ \\alpha $上的共轭超先验，这是一个Gamma分布\n",
    "$ p(\\alpha) = Gam(\\alpha|\\alpha_0,b_0) \\tag{10.166} $\n",
    "它由常数$ a_0 $和$ b_0 $控制。\n",
    "这个模型的边缘似然函数现在的形式为\n",
    "$ p(t) = \\int\\int p(w,\\alpha,t)dwd\\alpha \\tag{10.167} $\n",
    "其中，联合概率分布为\n",
    "$ p(w,\\alpha,t) = p(t|w)p(w|\\alpha)p(\\alpha) \\tag{10.168} $\n",
    "我们现在无法直接计算关于$ w $和$ \\alpha $的积分。我们会在同一个模型中使用全局的变分方法和局部的变分方法来解决这个问题。\n",
    "首先，我们引入一个变分分布$ q(w, \\alpha) $，然后应用式（10.2）给出的分解方式。在这种情况\n",
    "$ \\ln p(t) = L(q) + KL(q \\Vert p) \\tag{10.169} $\n",
    "其中，下界$ L(q) $和Kullback-Leibler散度$ KL(q \\Vert p) $的定义为\n",
    "$ \\begin{eqnarray} L(q) &=& \\int\\int q(w,\\alpha)\\ln\\left{\\frac{p(w,\\alpha,t)}{q(w,\\alpha)}\\right}dwd\\alpha \\tag{10.170} \\ KL(q \\Vert p) &=& -\\int\\int q(w,\\alpha)\\ln\\left{\\frac{p(w,\\alpha|t)}{q(w,\\alpha)}\\right}dwd\\alpha \\tag{10.171} \\end{eqnarray} $\n",
    "现在，由于似然因子$ p(t|w) $的形式，下界$ L(q) $仍然无法求解。于是，与之前一样，我们对每个logistic sigmoid因子应用一个局部的变分界限。这使得我们可以使用不等式（10.152），得到$ L(q) $的下界，这个下界也是对数似然函数的一个下界。\n",
    "$ \\begin{eqnarray} \\ln p(t) &\\geq& L(q) \\geq \\tilde{L}(q,\\xi) \\ &=& \\int\\int q(w,\\alpha)\\ln\\left{\\frac{h(w,\\xi)p(w|\\alpha)p(\\alpha)}{q(w,\\alpha)}\\right}dwd\\alpha \\tag{10.172} \\end{eqnarray} $\n",
    "接下来我们假设变分分布可以在参数和超参数之间进行分解，即\n",
    "$ q(w,\\alpha) = q(w)q(\\alpha)\\tag{10.173} $\n",
    "有了这种分解，我们可以使用式（10.9）给出的一般结果，得到最优因子的表达式。首先考虑概率分布$ q(w) $。丢弃与$ w $无关的项，我们有\n",
    "$ \\begin{eqnarray} \\ln q(w) &=& \\mathbb{E}\\alpha[\\ln{h(w,\\xi)p(w|\\alpha)p(\\alpha)}] + const \\ &=& \\ln h(w,\\xi) + \\mathbb{E}\\alpha[\\ln p(w|\\alpha)]+ const \\end{eqnarray} $\n",
    "我们现在使用式（10.153）消去$ \\ln h(w, \\xi) $，使用式（10.165）消去$ \\ln p(w|\\alpha) $，有\n",
    "$ \\ln q(w) = -\\frac{\\mathbb{E}[\\alpha]}{2}w^Tw + \\sum\\limits_{n=1}^N\\left{(t_n - \\frac{1}{2})w^T\\phi_n - \\lambda(\\xi_n)w^t\\phi_n\\phi_n^Tw\\right} + const $\n",
    "我们看到这是$ w $的一个二次函数，因此$ q(w) $的解是高斯分布。使用通常的配平方方法，我们有\n",
    "$ q(w) = \\mathcal{N}(w|\\mu_N,\\Sigma_N) \\tag{10.174} $\n",
    "其中我们定义了\n",
    "$ \\begin{eqnarray} \\Sigma_N^{-1}\\mu_N &=& \\sum\\limits_{n=1}^N\\left(t_n - \\frac{1}{2}\\right)\\phi_n \\tag{10.175} \\ \\Sigma_N^{-1} = \\mathbb{E}[\\alpha]I + 2\\sum\\limits_{n=1}^N\\lambda(\\xi_n)\\phi_n\\phi_n^T \\tag{10.176} \\end{eqnarray} $\n",
    "类似的，因子$ q(\\alpha) $的最优解为\n",
    "$ \\ln q(\\alpha) = \\mathbb{E}_w[\\ln p(w|\\alpha)] + \\ln p(\\alpha) + const $\n",
    "使用式（10.165）消去$ \\ln p(w|\\alpha) $，使用式（10.166）消去$ ln p(\\alpha) $，我们有\n",
    "$ \\ln q(\\alpha) = \\frac{M}{2}\\ln\\alpha - \\frac{\\alpha}{2}\\mathbb{E}[w^Tw] + (\\alpha_0 - 1)\\ln\\alpha - b_0\\alpha + const $\n",
    "我们看到这是一个Gamma分布的对数，因此我们有\n",
    "$ q(\\alpha) = Gam(\\alpha|a_N,b_N) = \\frac{1}{\\Gamma(a_N)}a_N^{b_N}\\alpha^{a_N -1}e^{-b_N\\alpha} \\tag{10.177} $\n",
    "其中\n",
    "$ \\begin{eqnarray} a_N &=& a_0 + \\frac{M}{2}\\tag{10.178} \\ b_N &=& b_0 + \\frac{1}{2}\\mathbb{E}_w[w^Tw] \\tag{10.179} \\end{eqnarray} $\n",
    "我们还需要最优化变分参数$ \\xi_n $，这也可以通过最大化下界$ \\tilde{L}(q, \\xi) $的方式得到。略去与$ \\xi $无关的项，对$ \\alpha $积分，我们有\n",
    "$ \\tilde{L}(q,\\xi) = \\int q(w)\\ln h(w,\\xi)dw + const \\tag{10.180} $\n",
    "注意，它的形式与式（10.160）的形式完全相同，因此我们可以使用我们之前的结果（10.163），它可以通过直接对边缘似然函数的最优化得到，从而重估计方程的形式为\n",
    "$ (\\xi^{new})^2 = \\phi_n^T(\\Sigma_N + \\mu_N\\mu_N^T)\\phi_n \\tag{10.181} $\n",
    "我们已经得到了三个量$ q(w) , q(\\alpha) $和$ \\xi $的重估计方程，因此在进行合适的最优化之后，我们可以在这些量之间进行循环，每次都对各个量进行更新。所要求解的各阶矩为\n",
    "$ \\begin{eqnarray} \\mathbb{E}[\\alpha] = \\frac{a_N}{b_N} \\tag{10.182} \\ \\mathbb{E}[ww^T] = \\Sigma_N + \\mu_N\\mu_N^T \\tag{10.183} \\end{eqnarray} $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 10.7 期望传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "在本章的最后一节，我们讨论确定性近似推断的另一种被称为期望传播（expectation propagation）或EP(Minka, 2001a; Minka, 2001b)的形式。与目前为止讨论的变分贝叶斯方法相同，这种方法也基于对Kullback-Leibler散度的最小化，但是现在形式相反，从而得到了性质相当不同的近似结果。\n",
    "先考虑关于$ q(z) $最小化$ KL(p \\Vert q) $的问题，其中$ p(z) $是一个固定的概率分布，$ q(z) $是指数族分布的一个成员，因此根据式（2.194），可以写成\n",
    "$ q(z) = h(z)g(\\eta)exp{\\eta^Tu(z)} \\tag{10.184} $\n",
    "作为$ \\eta $的一个函数，Kullback-Leibler散度变成了\n",
    "$ KL(p \\Vert q) = -\\ln g(\\eta) - \\eta^T\\mathbb{E}_{p(z)}[u(z)] + const \\tag{10.85} $\n",
    "其中常数项与自然参数$ \\eta $无关。我们可以通过令关于$ \\eta $的梯度等于零的方式，在这个概率分布族中最小化$ KL(p \\Vert q) $，结果为\n",
    "$ -\\nabla\\ln g(\\eta) = \\mathbb{E}_{p(z)}[u(z)] \\tag{10.186} $\n",
    "然而，我们已经看到，在式（2.226）中，$ ln g(\\eta) $的负梯度有概率分布$ q(z) $下$ u(z) $的期望给模型证据为定。令这两个结果相等，我们有\n",
    "$ \\mathbb{E}{q(z)}[u(z)] = \\mathbb{E}{p(z)}[u(z)] \\tag{10.87} $\n",
    "我们看到，最优解仅仅对应于将充分统计量的期望进行匹配。因此，例如，如果$ q(z) $是一个高斯分布$ \\mathcal{N}(z|\\mu,\\Sigma) $，那么我们通过令$ q(z) $的均值$ \\mu $等于分布$ p(z) $的均值并且令协方差$ \\Sigma $等于$ p(z) $的协方差，即可最小化Kullback-Leibler散度。这有时被称为矩匹配（moment matching)。图10.3(a)给出了这个的一个例子。\n",
    "现在，让我们利用这个结果，得到近似推断的一个实用的算法。对于许多概率模型来说，数据$ D $和隐含变量（包括参数）$ \\theta $的联合概率分布由一组因子的乘积组成，形式为\n",
    "$ p(D,\\theta) = \\prod\\limits_if_i(\\theta) \\tag{10.188} $\n",
    "这个结果可能由独立同分布的数据的模型产生，其中对于每个数据点$ x_n $，都有一个因子$ f_n(\\theta) = p(x_n|\\theta) $，且因子$ f_0(\\theta) = p(\\theta) $对应于先验概率分布。更一般地，它也适用于任何由有向图定义的模型，其中每个因子是一个条件概率分布，对应于一个结点。也适用于无向图，其中每个因子是一个团块势函数。我们感兴趣的是计算后验概率分布$ p(\\theta|D) $用于进行预测，以及计算模型证据$ p(D) $用于进行模型比较。根据公式(10.188)，后验概率分布为\n",
    "$ p(\\theta|D) = \\frac{1}{p(D)}\\prod\\limits_if_i(\\theta) \\tag{10.189} $\n",
    "模型证据为\n",
    "$ p(D) = \\int\\prod\\limits_if_i(\\theta)d\\theta \\tag{10.190} $\n",
    "这里，我们考虑连续变量，但是下面的讨论同样适用于离散变量，只需把积分替换为求和即可。我们假设$ \\theta $上的边缘概率分布以及关于用来进行预测的后验概率分布的边缘分布都是无法计算的，从而需要某种形式的近似。\n",
    "期望传播基于后验概率分布的近似，这个近似也由一组因子的乘积给出，即\n",
    "$ q(\\theta) = \\frac{1}{Z}\\prod\\limits_i\\tilde{f}_i(\\theta) \\tag{10.191} $\n",
    "其中，近似中的每个因子$ \\tilde{f}_i(\\theta) $对应于真实后验分布（10.189）中的一个因子$ f(\\theta) $，因子$ 1 / Z $是标准化常数，用来确保式（10.191）的左侧的积分等于1。为了得到一个实用的算法，我们需要对因子$ \\tilde{f}_i(\\theta) $进行一定的限制，特别的，我们会假定因子来自指数族分布。于是，因子的乘积也是指数族分布，因此可以用充分统计量的有限集合来描述。例如，如果每个$ \\tilde{f}_i(\\theta) $是一个高斯分布，那么整体的近似$ q(\\theta) $也是高斯分布。\n",
    "理想情况下，我们通过最小化真实后验概率分布与近似分布之间的Kullback-Leibler散度的方式来确定$ \\tilde{f}_i(\\theta) $，这个散度为\n",
    "$ KL(p \\Vert q) = KL\\left(\\frac{1}{p(D)}\\prod\\limits_if_i(\\theta)\\big\\Vert\\frac{1}{Z}\\prod\\limits_i\\tilde{f}_i(\\theta)\\right) \\tag{10.192} $\n",
    "注意与变分推断中使用的KL散度相比，这个KL的散度恰好相反。通常这个最小化是无法进行的，因为KL散度涉及到关于真实概率分布求平均。作为一个粗略的近似，我们反过来最小化对应的因子对$ f_i(\\theta) $和$ \\tilde{f}_i(\\theta) $之间的KL散度。这个问题容易得多，并且具有算法无需迭代的优点。然而，由于每个因子被各自独立地进行近似，因此因子的乘积的近似效果可能很差。\n",
    "期望传播通过在所有剩余因子的环境中对每个因子进行优化，从而取得了一个效果好得多的近似。首先，这种方法初始化因子$ \\tilde{f}_i(\\theta) $，然后在因子之间进行循环，每次优化一个因子。这种方法的思想类似于之前讨论的变分贝叶斯框架的因子更近过程。假设我们希望优化因子$ \\tilde{f}_j(\\theta) $。\n",
    "首先，我们将这个因子从乘积中移除，得到$ \\prod_{i \\neq j}\\tilde{f}_i(\\theta) $。从概念上讲，我们要确定因子$ \\tilde{f}_j(\\theta) $的一个修正形式，使得乘积\n",
    "$ q^{new}(\\theta) \\propto \\tilde{f}j(\\theta)\\prod{i \\neq j}\\tilde{f}_i(\\theta) \\tag{10.193} $\n",
    "尽可能地接近\n",
    "$ f_j(\\theta)\\prod_{i \\neq j}\\prod_{i \\neq j}\\tilde{f}_i(\\theta) \\tag{10.194} $\n",
    "其中我们保持所有$ i \\neq j $的因子$ \\tilde{f}_i(\\theta) $固定。这保证了近似在由剩余的因子定义的后验概率较高的区域最精确。后面，当我们将EP应用于“聚类问题”的时候，我们会看到这种效果的一个例子。为了达到这个目的，我们首先从当前的对后验概率的近似中移除因子$ \\tilde{f}_j＝(\\theta) $方法是定义下面的未标准化的分布\n",
    "$ q^{\\j}(\\theta) = \\frac{q(\\theta)}{\\tilde{f}_j＝(\\theta)} \\tag{10.195} $\n",
    "注意，我们反过来从$ i \\neq j $的因子的乘积中求出$ q^{\\j}(\\theta) $，虽然在实际应用中，除法通常更容易。它现在与因子$ f_j(\\theta) $结合，得到概率分布\n",
    "$ \\frac{1}{Z_j}f_j(\\theta)q^{\\j}(\\theta) \\tag{10.196} $\n",
    "其中$ Z_j $是标准化常数，形式为\n",
    "$ Z_j = \\int f_j(\\theta)q^{\\j}(\\theta)d\\theta \\tag{10.197} $\n",
    "我们现在通过最小化Kullback-Leibler散度\n",
    "$ KL\\left(\\frac{f_j(\\theta)q^{\\j}(\\theta)}{Z_j}\\Vert q^{new}(\\theta)\\right) \\tag{10.198} $\n",
    "来确定一个修正的因子$ \\tilde{f}_j＝(\\theta) $。这很容易求解，因为近似分布$ q^{new}(\\theta) $来自指数族分布，因此我们可以使用结果（10.187），这个公式告诉我们，参数$ q(\\theta) $可以通过匹配式（10.1960的对应矩的充分统计量的期望的方式获得。我们会假设这是一个可以计算的操作。例如，如果我们将$ q(\\theta) $选择为高斯概率分布$ \\mathcal{N}(\\theta|\\mu,\\Sigma) $，那么$ \\mu $被设置为（未标准化的）分布$ f_j(\\theta)q^{\\j}(\\theta) $的均值，$ \\Sigma $被设置为它的方差。更一般地，得到指数族分布的任意成员的所需的分布是很容易的，只要它能够被标准化即可，因为充分统计量的期望可以与标准化系数的导数相关联，正如式（2.226）所述。图10.14说明了EP近似的过程。\n",
    " \n",
    "图 10.14 用高斯分布进行期望传播近似的说明,使用了之前在图4.14和图10.1中讨论的例子。左图给出了原始的概率分布（黄色）以及拉普拉斯近似（红色）、全局变分近似（绿色）以及EP近似（蓝色），右 图给出了对应的概率分布的负对数。注意,EP分布比变分推断得到的分布更宽，这是由于不同形式的KL散度造成的结果。\n",
    "根据式(10.193），我们看到修正的因子$ \\tilde{f}_j(\\theta) $可以按照下面的方法得到：取$ q^{new}(\\theta) $，然后，除以剩余的因子，即\n",
    "$ \\tilde{f}_j(\\theta) = K\\frac{q^{new}(\\theta)}{q^{\\j}(\\theta)} \\tag{10.199} $\n",
    "其中我们使用了公式(10.195)。系数$ K $通过下面的方式确定：将等式（10.199）的两侧乘以$ q^{\\j}(\\theta) $，然后积分，可得\n",
    "$ K = \\int\\tilde{f}_j(\\theta)q^{\\j}(\\theta)d\\theta \\tag{10.200} $\n",
    "其中我们已经使用了$ q^{new}(\\theta) $已经标准化这一事实。于是，$ K $的值可以通过匹配零阶矩的方式得到\n",
    "$ \\int\\tilde{f}_j(\\theta)q^{\\j}(\\theta)d\\theta = \\int f_j(\\theta)q^{\\j}(\\theta)d\\theta \\tag{10.201} $\n",
    "将这个式子与式（10.197）结合，我们看到$ K = Z_j $，因此可以通过计算式（10.197）中的积分的方式得到。\n",
    "在实际应用中，在因子集合中会进行多次迭代，每次都修正所有的因子。之后，使用式（10.191）可以得到后验概率分布$ p(\\theta|D) $的近似，模型证据$ p(D) $可以使用式（10.190）来近似，其中因子$ f_i(\\theta) $被替换为它们的近似$ \\tilde{f}(\\theta) $。\n",
    "我们给定观测数据集$ D $和随机变量$ \\theta $上的联合概率分布，用因子的乘积的形式表示\n",
    "$ p(D,\\theta) = \\prod\\limits_if_i(\\theta) \\tag{10.202} $\n",
    "我们希望使用下面形式的分布\n",
    "$ q(\\theta) = \\frac{1}{Z}\\prod\\limits_i\\tilde{f}_i(\\theta) \\tag{10.203} $\n",
    "来近似后验概率分布$ p(\\theta|D) $。我们也希望近似模型证据$ p(D) $。\n",
    "1.\t初始化所有的近似因子$ \\tilde{f}_i(\\theta) $。\n",
    "2.\t通过设置 $ q(\\thea) \\propto \\prod\\limits_i\\tilde{f}_i(\\theta) \\tag{10.204} $ 初始化后验近似。\n",
    "3.\t直到收敛：\n",
    "•\t选择一个因子$ \\tilde{f}_j(\\theta) $进行优化。\n",
    "•\t通过下面的除法 $ q^{\\j}(\\theta) = \\frac{q(\\theta)}{\\tilde{f}_j(\\theta)} \\tag{10.205} $ 从后验概率分布中移除$ \\tilde{f}_j(\\theta) $。\n",
    "•\t计算新的后验概率分布，方法为：令$ q^{new}(\\theta) $的充分统计量（矩）等于$ q^{\\j}(\\theta)f_j(\\theta) $的充分统计量（矩），包括计算标准化系数\n",
    "$ Z_j = \\int q^{\\j}(\\theta)f_j(\\theta)d\\theta \\tag{10.206} $\n",
    "•\t计算和存储新的因子 $ \\tilde{f}_j(\\theta) = Z_j\\frac{q^{new}(\\theta)}{q^{\\j}(\\theta)} \\tag{10.207} $\n",
    "4.\t计算模型证据的近似 $ p(D) \\simeq \\int\\prod\\limits_i\\tilde{f}_i(\\theta)d\\theta \\tag{10.208} $\n",
    "EP的一个特别的情况，被称为假定密度过滤（assumed density filtering, ADF）或矩匹配（moment matching）（MayBeck, 1982; Lauritzen, 1992; Boyen and Koller, 1998; Opper and Winther, 1999），可以这样得到：对除了第一个因子以外的所有近似因子初始化为1，然后在所有因子之间进行一次迭代，每次更新因子中的每一个。假定密度过滤对于在线学习很适用，其中数据点顺序地到达，我们需要从每个数据点中进行学习，然后在考虑下一个数据点之间将其丢弃。然而，在批处理的设定中，我们有机会多次重新适用数据点来得到更高的精度，并且这正是期望传播所利用的思想。此外，如果我们将ADF应用于批量的数据，结果会依赖于数据点的处理顺序，这不是我们想要的，而EP可以克服这个缺点。\n",
    "期望传播的一个缺点是，它不保证迭代会收敛。然而，对于指数族分布的近似$ q(\\theta) $，如果迭代确实收敛，那么求得的解是特定的势函数的驻点（Minka, 2001a），虽然每轮EP迭代未必减小势函数的值。这与变分贝叶斯相反。变分贝叶斯中，每轮迭代保证不会减小界限。直接优化EP的代价函数是可能的，这种情况下，它保证收敛，虽然会导致算法更慢，实现起来更复杂。\n",
    "变分贝叶斯和EP的另一个区别是来自于两个算法所最小化的KL散度的形式，因为前者最小化$ KL(q \\Vert p) $，而后者最小化$ KL(p \\Vert q) $。正如我们在图10.3中看到的那样，对于多峰的概率分布$ p(\\theta) $，最小化$ KL(p \\Vert q) $会产生较差的近似。特别的，如果将EP应用于混合概率分布，那么得到的结果没有意义，因为得到的近似试图覆盖后验概率分布的所有峰值。相反，在logistic类型的模型中，EP通常要比局部变分方法和拉普拉斯近似方法的表现更好（Kuss and Rasmussen, 2006）。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 10.7.1 例子：聚类问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "遵从Minka(2001b)的做法，我们使用一个简单的例子来说明EP算法，其中我们的目标是在给定服从那个分布的一组观测的情况下，推断变量$ x $上的多元高斯分布的均值$ \\theta $。为了让问题更加有趣，观测位于一个背景杂波中，它本身也是一个高斯分布，如图10.15所示。\n",
    " \n",
    "图 10.15 维度为$ D = 1 $的数据空间中的聚类问题的说明。训练数据点（用叉号表示），从两个高斯分布混合而成的分布中抽出，高斯分量用红色和蓝色表示。我们的目标是从观测数据中推断绿色高斯分布的均值。\n",
    "于是，观测值$ x $的概率分布是一个混合高斯分布，形式为\n",
    "$ p(x|\\theta) = (1 - w)\\mathcal{N}(x|\\theta,I) + w\\mathcal{N}(x|0,aI) \\tag{10.209} $\n",
    "其中，$ w $是背景杂波的比重，假设是已知的。$ \\theta $上的先验概率分布是高斯分布，形式为\n",
    "$ p(\\theta) = \\mathcal{M}(\\theta|0,bI) \\tag{10.210} $\n",
    "Minka(2001a)选择参数的值为$ a = 10,b = 100,w = 0.5 $。$ N $次观测$ D = {x_1,...,x_N} $和$ \\theta $的联合概率分布为\n",
    "$ p(D,\\theta) = p(\\theta)\\prod\\limits_{n=1}^Np(x_n|\\theta) \\tag{10.211} $\n",
    "因此后验概率分布由$ 2^N $个高斯分布混合而成。从而精确解决这个问题的计算代价会随着数据集的规模指数增长，因此对于大的$ N $值，精确求解是不可行的。\n",
    "为了将EP应用于杂波问题，我们首先看出，因子$ f_0(\\theta) = p(\\theta) $且$ f_n(\\theta) = p(x_n|\\theta) $。接下来，我们从指数族分布中选择一个近似分布。对于这个例子，比较方便的做法是选择一个球形高斯分布\n",
    "$ q(\\theta) = \\mathcal{N}(\\theta|m,vI) \\tag{10.212} $\n",
    "于是，因子近似会取指数-二次函数的形式，即\n",
    "$ \\tilde{f}_n(\\theta) = s_n\\mathcal{N}(\\theta|m_n,v_nI) \\tag{10.213} $\n",
    "其中$ n = 1,...,N $，且令$ \\tilde{f}_0(\\theta) $等于先验概率分布$ p(\\theta) $。注意，使用$ \\mathcal{N}(\\theta|\\dot,\\dot) $不表示右手边是一个良好定义的高斯概率密度（事实上，正如我们将看到的那样，方差参数$ v_n $可以为负），而是仅仅是一个方便的简化记号。近似$ \\tilde{f}_n(\\theta),n = 1,...,N $可以被初始化为1，对应于$ s_n = (2\\pi v_n)^{D /2} , v \\to \\infty $以及$ m_n = 0 $，其中$ D $是$ x $的维度，因此也是$ \\theta $的维度。式（10.191）定义的初始的$ q(\\theta) $因此就等于先验概率分布。\n",
    "我们接下来迭代的优化因子，方法是每次取一个因子$ f_n(\\theta) $，然后使用式（10.205）、（10.206）和（10.207）。注意，我们不需要修改$ f_0(\\theta) $，因为EP更新会让这一项保持不变。这里，我们给出结果，让读者自己来填充细节。\n",
    "$ \\begin{eqnarray} m^{\\backslash n} = m + v^{\\backslash n}v_n^{−1}(m − m_n) \\tag{10.214} \\ (v^{\\backslash n})^{-1} = v^{-1} - v_n^{-1} \\tag{10.215}\n",
    "\\end{eqnarray} $\n",
    "接下来，我们使用式（10.206）计算标准化常数，结果为\n",
    "$ Z_n =(1−w)\\mathcal{N}(x_n|m^{\\backslash n},(v^{\\backslash n} +1)I) + w\\mathcal{N}(x_n|0,\\alpha I) \\tag{10.216} $\n",
    "类似的，我们通过寻找$ q^{\\backslash n}(\\theta)f_n(\\theta) $的均值，计算$ q^{new}(\\theta) $的均值和方差，结果为\n",
    "$ \\begin{eqnarray} m^{new} &=& m^{\\backslash n} + \\rho_n\\frac{v^{\\backslash n}}{v^{\\backslash n} + 1}(x_n - m^{\\backslash n}) \\tag{10.217} \\ v^{new} &=& v^{\\backslash n} - \\rho_n\\frac{(v^{\\backslash n})^2}{v^{\\backslash n} + 1} + \\rho_n(1-\\rho_n)\\frac{(v^{\\backslash n})^2\\Vert x_n -m^{\\backslash n}\\Vert^2}{D(v^{\\backslash n} +1)^2} \\tag{10.218} \\end{eqnarray} $\n",
    "其中\n",
    "$ \\rho_n = 1 - \\frac{w}{Z_n}\\mathcal{N}(x_n|0,\\alpha I) \\tag{10.219} $\n",
    "它可以简单的表示为点$ x_n $不在杂波中的概率。然后，我们使用式（10.207）计算优化因子$ \\tilde{f}(\\theta) $，它的参数为\n",
    "$ \\begin{eqnarray} v_n^{-1} &=& (v^{new})^{-1} - (v^{\\backslash n})^{-1} \\tag{10.220} \\ m_n &=& m^{\\backslash n} + (v_n + v^{\\backslash n})(v^{\\backslash n})^{-1}(m^{new} - m^{\\backslash n}) \\tag{10.221} \\ s_n &=& \\frac{Z_n}{(2\\pi v_n)^{D/2}\\mathcal{N}(m_n|m^{\\backslash n},(v_n + v^{\\backslash n})I)} \\tag{10.222} \\end{eqnarray} $\n",
    "优化过程不断重复，直到满足一个合适的终止准则，例如在对所有因子进行的一次优化迭代中，参数值的最大改变量小于一个阈值。最后，我们使用式（10.208）来计算模型证据的近似，结果为\n",
    "$ p(D) \\simeq (2\\pi v^{new})^{D/2}exp\\left(\\frac{D}{2}\\right)\\prod\\limits_{n=1}^N\\left{s_n(2\\pi v_n)^{-D/2}\\right} \\tag{10.223} $\n",
    "其中\n",
    "$ B = \\frac{(m^{new})^Tm^{new}}{v} - \\sum\\limits_{n=1}^N\\frac{m_n^Tm_n}{v_n} \\tag{10.224} $\n",
    "图10.16给出了对于一维参数空间$ \\theta $的杂波问题的因子近似的例子。\n",
    " \n",
    "图 10.16 对于杂波问题的一维版本，具体因子的近似的例子。图中用蓝色表示$ f_n(\\theta) $，用红色表示$ \\tilde{f}(\\theta) $，用绿色表示$ q^{\\backslash n}(\\theta) $。注意$ q^{\\backslash n}(\\theta) $现在的形式控制了$ \\theta $的取值范围，在这个范围上，$ \\tilde{f}(\\theta) $是$ f_n(\\theta) $的一个很好的近似。\n",
    "注意，因子近似可以有无穷大的或者负数的“方差”参数$ v_n $。这仅仅对应于曲线向上弯曲而不是向下弯曲的情形，并且只要所有的近似后验概率$ q(\\theta) $有正的方差，这种情形就未必有问题。图10.17对比了在杂波问题中，EP的表现、变分贝叶斯（平均场理论）的表现以及拉普拉斯近似的表现。\n",
    " \n",
    "图 10.17 期望传播、变分推断和拉普拉斯近似在聚类问题上的对比。左图给出了预测后验概率分布的均值与浮点运算的数量的关系，右图给出了对应的模型证据的结果。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 10.7.2 图的期望传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "目前为止在我们对于EP的一般的讨论中，我们让概率分布$ p(\\theta) $中的所有因子$ f_i(\\theta) $是$ \\theta $的全部分量的函数，类似的，对于近似分布$ q(\\theta) $的近似因子$ \\tilde{f}(\\theta) $的情形也相同。我们现在考虑下面的情形：因子只依赖于变量的一个子集。这种限制可以很方便地使用第8章讨论的概率图模型的框架来表示。这里，我们使用因子图表示，因为它同时包含了有向图和无向图。\n",
    "我们会把注意力集中于近似概率分布完全分解的情形，我们会证明，在这种情形下，期望传播会简化为循环置信传播（Minka, 2001a）。首先，我们在一个简单的例子中证明这一点，然后我们会研究一般的情形。\n",
    "首先，回忆一下，根据式（10.17），如果我们关于一个分解的概率分布$ q $来最小化Kullback-Leibler散度$ KL(p \\Vert q) $，那么对于每个因子，最优解为$ p $的对应的边缘概率分布。\n",
    "现在，考虑图10.18左手边给出的因子图。\n",
    " \n",
    "图 10.18 左图是来自图8.51的一张简单的因子图，为了方便，这里重新画出。右图是对应的分解近似。\n",
    "我们之前在加-乘算法中介绍过这张图。联合概率分布为\n",
    "$ p(x) = f_a(x_1, x_2)f_b(x_2, x_3)f_c(x_2, x_4) \\tag{10.225} $\n",
    "我们寻找具有相同分解方式的一个近似$ q(x) $，即\n",
    "$ q(x) \\propto \\tilde{f}_a(x_1, x_2)\\tilde{f}_b(x_2, x_3)\\tilde{f}_c(x_2, x_4) \\tag{10.226} $\n",
    "注意，标准化常数被省略，这些可以在计算的最后使用局部标准化的方法计算出来，正如我们在置信传播中经常做的那样。现在，假设我们将注意力集中于近似分布上，其中因子本身可以关于各个变量进行分解，即\n",
    "$ q(x) \\propto \\tilde{f}{a1}(x_1)\\tilde{f}{a2}(x_2)\\tilde{f}{b2}(x_2)\\tilde{f}{b3}(x_3)\\tilde{f}{c2}(x_2)\\tilde{f}{c4}(x_4) \\tag{10.227} $\n",
    "它对应于图10.18右手边的因子图。由于各个独立的因子是分解的，因此整体概率分布$ q(x) $本身是完全分解的。\n",
    "现在，我们使用这个完全分解的近似，应用EP算法。假设我们已经初始化了所有的因子，并且我们选择优化因子$ \\tilde{f}b(x_2, x_3) = \\tilde{f}{b2}(x_2)\\tilde{f}_{b3}(x_3) $。首先，我们将这个因子从近似分布中移除，得到\n",
    "$ q^{\\b}(x) \\propto \\tilde{f}{a1}(x_1)\\tilde{f}{a2}(x_2)\\tilde{f}{c2}(x_2)\\tilde{f}{c4}(x_4) \\tag{10.228} $\n",
    "然后我们乘以精确因子$ f_b(x_2, x_3) $，可得\n",
    "$ \\hat{p}(x) = q^{\\b}(x)f_b(x_2,x_3) = \\tilde{f}{a1}(x_1)\\tilde{f}{a2}(x_2)\\tilde{f}{c2}(x_2)\\tilde{f}{c4}(x_4)\\tilde{f}_b(x_2,x_3) \\tag{10.229} $\n",
    "我们现在通过最小化Kullback-Leibler散度$ KL(p|q^{new}) $来寻找$ q^{new}(x) $。这个结果，正如之前注意到的那样，是$ q^{new}(z) $组成了因子的乘积，每个变量$ x_i $对应一个因子，其中每个因子由$ \\hat{p}(x) $的对应的边缘概率分布组成。这四个边缘概率分布为\n",
    "$ \\begin{eqnarray} \\hat{p}(x_1) &\\propto& \\tilde{f}{a1}(x_1) \\tag{10.230} \\ \\hat{p}(x_2) &\\propto& \\tilde{f}{a1}(x_2)\\tilde{f}{c2}(x_2)\\sum\\limits{x_3}f_b(x_2,x_3) \\tag{10.231} \\ \\hat{p}(x_3) &\\propto& \\sum\\limits_{x_2}\\left{f_b(x_2, x_3)\\tilde{f}{a2}(x_2)\\tilde{f}{c2}(x_2)\\right} \\tag{10.232} \\ \\hat{p}(x_4) \\propto \\tilde{f}_{c4}(x_4) \\tag{10.233} \\end{eqnarray} $\n",
    "$ q^{new} $可以通过将这些边缘概率分布相乘的方式得到。我们看到，当更新$ \\tilde{f}b(x_2, x_3) $，$ q(x) $中唯一改变的因子是涉及到$ f_b $中的变量的因子，即涉及到$ x_2 $和$ x_3 $的因子。为了得到优化的因子$ \\tilde{f}(x_2, x_3) = \\tilde{f}{b2}(x_2)\\tilde{f}_{b3}(x_3) $我们将$ q^{new}(x) $除以$ q^{\\b}(x) $，结果为\n",
    "$ \\begin{eqnarray} \\tilde{f}{b2}(x_2) &\\propto& \\sum\\limits{x3}f_b(x_2,x_3) \\tag{10.234} \\ \\tilde{f}{b3}(x_3) &\\propto& \\sum\\limits{x2}\\left{f_b(x_2,x_3)\\tilde{f}{a2}(x_2)\\tilde{f}{c2}(x_2)\\right} \\tag{10.235} \\end{eqnarray} $\n",
    "这些与使用置信传播得到的信息完全相同，其中从变量结点到因子结点的信息已经被整合到从因子结点到变量结点的信息当中。特别的，$ \\tilde{f}{b2}(x_2) $对应于由因子结点$ f_b $向变量结点$ x_2 $发送的信息$ \\mu{f_b \\to x_2}(x2) $，由式（8.81）给出。类似的，如果我们将式（8.78）代入式（8.79），我们得到式（10.235），其中$ \\tilde{f}{a2}(x_2) $对应于$ \\mu{f_a \\to x_2}(x_2) $，且$ \\tilde{f}{c2}f(x_2) $对应于$ \\mu{f_c \\to x_2}(x_2) $，给出了信息$ \\tilde{f}{b3}(x_3) $，它对应于$ \\mu{b_b \\to x_3}(x_3) $。\n",
    "这个结果与标准的置信传播稍微有些不同,因为信息同时向两个方向传递。我们可以很容易地修改EP步骤，给出加-乘算法的标准形式，修改方法为：每次只更新一个因子，例如如果我们只优化$ \\tilde{f}{b3}(x_3) $，那么根据定义，$ \\tilde{f}{b2}(x_2) $不变，而$ \\tilde{f}_{b3}(x_3) $的优化版本再次由式（10.235）给出。如果我们每次只优化一项，那么我们可以选择我们所希望进行的优化的顺序。特别的，对于一个树结构的图，我们可以遵循两遍更新的框架，对应于标准的置信传播方法，它会产生对变量和因子的边缘概率分布的精确的推断。这种情况下，近似因子的初始化不再重要。\n",
    "现在，让我们考虑一个一般的因子图，它对应于下面的概率分布\n",
    "$ p(\\theta) = \\prod\\limits_if_i(\\theta_i) \\tag{10.236} $\n",
    "其中$ \\theta_i $表示与因子$ f_i $关联的变量的子集。我们使用一个完全分解的概率分布来近似它，形式为\n",
    "$ q(\\theta) \\propto \\prod\\limits_i\\prod\\limits_k\\tilde{f}_{ik}(\\theta_k) \\tag{10.237} $\n",
    "其中$ \\theta_k $对应于一个单独的变量结点。假设我们希望优化特定的项$ \\tilde{f}_{kl}(\\theta_l) $，保持其他所有的项不变。首先，我们从$ q(\\theta) $中移除项$ \\tilde{f}_j(\\theta_j) $，可得\n",
    "$ q^{\\j}(\\theta) \\propto \\prod\\limits_{i \\neq j}\\prod\\limits_k\\tilde{f}_{ik}(\\theta_k) \\tag{10.238} $\n",
    "然后乘以精确因子$ f_j(\\theta_j) $。为了确定优化项$ \\tilde{f}_{jl}(\\theta_l) $，我们只需考虑对$ \\theta_l $的函数依赖，因此我们只需寻找\n",
    "$ q^{\\j}(\\theta)f_j(\\theta_j) \\tag{10.239} $\n",
    "对应的边缘概率分布。忽略一个可以做乘法的常数，这涉及到对$ f_j(\\theta_j) $与任意来自$ q^{\\j}(\\theta) $中的属于$ \\theta_j $中任意变量的函数的项进行相乘得到的结果求边缘概率分布。当我们接下来除以$ q^{\\j}(\\theta) $时，对应于$ i \\neq j $的其它因子$ \\tilde{f}_i(\\theta_i) $的项会在分子和分母之间消去。因此我们有\n",
    "$ \\tilde{f}{jl}(\\theta_l) \\propto \\sum\\limits{\\theta_{m \\neq l} \\in \\theta_j}f_j(\\theta_j)\\prod\\limits_k\\prod\\limits_{m \\neq l}\\tilde{f}_{km}(\\theta_m) \\tag{10.240} $\n",
    "我们将这个式子看做是加-乘规则的形式，其中，从变量结点到因子结点的信息被消除，正如图8.50中给出的例子那样。$ \\tilde{f}{jm}(\\theta_m) $对应于信息$ \\mu{f_j \\to \\theta_m} (\\theta_m) $，其中因子结点$ j $向变量结点$ m $发送信息，且式（10.240）中的在$ k $上的乘积作用于所有依赖于与因子$ f_j(\\theta_j) $有相同变量（除了变量$ \\theta_l $）的变量$ \\theta_m $。也就是说，为了计算来自一个因子结点的输出信息，我们对所有来自其它结点的输入信息求乘积，乘以局部因子，然后求和或积分。\n",
    "因此，如果我们使用完全分解的近似概率分布，那么加和-乘积算法就可以作为期望传播的一个具体的例子。这表明，更加灵活的近似分布（对应于部分连接的图）可以得到更高的准确率。另一种推广是将因子$ f_i(\\theta_i) $分成若干组，在一次迭代过程中优化组内的全部因子。这两种方法都可以产生精度的提升（Minka, 2001b）。通常，选择最好的分组和断开连接的方式是一个开放的问题。\n",
    "我们已经看到了变分信息传递和期望传播方法对Kullback-Leibler散度的两种不同的形式进行了最优化。Minka（2005）证明，一大类信息传递方法可以从一个涉及到最小化散度的alpha家族的成员的通用框架中推导出来，其中，散度的alpha家族由公式（10.19）给出。这些信息传递方法包括变分信息传递、循环置信传播、期望传播，以及一大类其他的算法，例如树重加权信 息传递（tree-reweighted message passing）（Wainwright et al.， 2005）、分数置信传播（fractional belief propagation）（Wiegerinck and Heskes， 2003）以及强EP（power EP）（Minka， 2004），篇幅所限，我们不会在这里介绍这些算法。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
